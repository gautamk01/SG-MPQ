{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RFSn9Q67DC0U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1761923780051,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "RFSn9Q67DC0U",
    "outputId": "cb925786-86f8-4c89-bf4e-035afbc3d12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PR'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 197 (delta 98), reused 182 (delta 83), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (197/197), 164.81 KiB | 18.31 MiB/s, done.\n",
      "Resolving deltas: 100% (98/98), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/gautamk01/PR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "E2ZXJMhADFlJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1761924041960,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "E2ZXJMhADFlJ",
    "outputId": "3d17acbc-91be-481e-9735-10aab093c6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/PR\n"
     ]
    }
   ],
   "source": [
    "%cd PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb8de07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 176452,
     "status": "ok",
     "timestamp": 1761923961842,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "6cb8de07",
    "outputId": "77613dfb-c2e3-4f14-a4ec-8f2050deb997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.28.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.41.0 (from -r requirements.txt (line 2))\n",
      "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lm_eval==0.4.2 (from -r requirements.txt (line 4))\n",
      "  Downloading lm_eval-0.4.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 7))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.40.1 (from -r requirements.txt (line 8))\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0 (from -r requirements.txt (line 9))\n",
      "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (5.29.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (18.1.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Collecting evaluate (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonlines (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (2.14.1)\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.17.1)\n",
      "Collecting pybind11>=2.6.2 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pytablewriter (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (1.6.1)\n",
      "Collecting sqlitedict (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tqdm-multiprocess (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.25.0)\n",
      "Collecting word2number (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (10.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1->-r requirements.txt (line 8))\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2025.10.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (75.2.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (8.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=73d6a226fe35d8f44e89cab3a61da3067b166f15e7a9a66268836863e4f1bf4b\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=fb90937c07a14a2bddd3d8296f44df054bf1a427eb2835207039c7771fb88026\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=48e15a91036bf2bbb0fd47db63a303b97543874a5505b35338fc5b476e3ff9f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, bitsandbytes, triton, tqdm, tcolorpy, pybind11, pyarrow-hotfix, portalocker, pathvalidate, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mbstrdecoder, jsonlines, fsspec, colorama, typepy, tqdm-multiprocess, sacrebleu, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, rouge-score, transformers, datasets, DataProperty, accelerate, tabledata, evaluate, pytablewriter, lm_eval\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.64.1 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 accelerate-0.28.0 bitsandbytes-0.41.0 colorama-0.4.6 datasets-2.18.0 evaluate-0.4.6 fsspec-2024.2.0 jsonlines-4.0.0 lm_eval-0.4.2 mbstrdecoder-1.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pathvalidate-3.3.1 portalocker-3.2.0 pyarrow-hotfix-0.7 pybind11-3.0.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tokenizers-0.19.1 torch-2.2.2 tqdm-4.64.1 tqdm-multiprocess-0.0.11 transformers-4.40.1 triton-2.2.0 typepy-1.3.4 word2number-1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "894f3bf84cbb4bb08185294e08d42fbc",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08f687b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5071,
     "status": "ok",
     "timestamp": 1761923966918,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "a08f687b",
    "outputId": "62357eac-fa3b-4920-dfc1-8af9f21bd134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.28.0 in /usr/local/lib/python3.12/dist-packages (0.28.0)\n",
      "Requirement already satisfied: transformers==4.40.1 in /usr/local/lib/python3.12/dist-packages (4.40.1)\n",
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (1.2.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.28.0) (12.6.85)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "Successfully installed peft-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.28.0 transformers==4.40.1 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e65bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1761923967723,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "21e65bee",
    "outputId": "f97fb2a4-2a96-420d-d53c-51c1e338b3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `newProject` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `newProject`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token #token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13494a9f",
   "metadata": {
    "id": "13494a9f",
    "outputId": "956916a8-a084-4365-e547-2e61d85f7b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '4.1', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama3_optimized_4.5bit', '--save_quant_dir', './output/llama3_optimized_4.5bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.1\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_4.5bit', save_quant_dir='./output/llama3_optimized_4.5bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=4.1, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-14 12:45:14 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|█████████████████████████████| 654/654 [00:00<00:00, 8.21MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 50.6k/50.6k [00:00<00:00, 4.07MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 61.7MB/s]\n",
      "special_tokens_map.json: 100%|███████████████| 73.0/73.0 [00:00<00:00, 1.14MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors.index.json: 100%|█████████| 23.9k/23.9k [00:00<00:00, 210MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 48.2M/4.98G [00:01<02:06, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏     | 182M/4.98G [00:01<00:39, 121MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▏   | 1.19G/4.98G [00:02<00:05, 686MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.48G/4.98G [00:02<00:01, 1.63GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.97G/4.98G [00:02<00:01, 1.54GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.31G/4.98G [00:03<00:01, 1.48GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.60G/4.98G [00:03<00:01, 1.33GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.86G/4.98G [00:03<00:00, 1.28GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.07G/4.98G [00:03<00:00, 1.31GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.27G/4.98G [00:03<00:00, 1.36GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.47G/4.98G [00:04<00:00, 1.34GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.64G/4.98G [00:04<00:00, 1.30GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|████| 4.98G/4.98G [00:04<00:00, 1.11GB/s]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:04<00:13,  4.61s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 4.31M/5.00G [00:01<20:45, 4.01MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎     | 264M/5.00G [00:01<00:23, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|█    | 1.02G/5.00G [00:01<00:04, 913MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.38G/5.00G [00:01<00:03, 1.06GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.66G/5.00G [00:02<00:02, 1.22GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▉   | 1.90G/5.00G [00:02<00:03, 870MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.51G/5.00G [00:02<00:01, 1.46GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.80G/5.00G [00:02<00:01, 1.46GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.07G/5.00G [00:03<00:01, 1.55GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.31G/5.00G [00:03<00:01, 1.57GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.58G/5.00G [00:03<00:00, 1.69GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.88G/5.00G [00:03<00:00, 1.79GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.15G/5.00G [00:03<00:00, 1.49GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/5.00G [00:03<00:00, 1.41GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.55G/5.00G [00:04<00:00, 1.34GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.73G/5.00G [00:04<00:00, 1.38GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|████| 5.00G/5.00G [00:04<00:00, 1.15GB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:09<00:09,  4.50s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 795k/4.92G [00:01<2:04:17, 659kB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎     | 253M/4.92G [00:01<00:24, 192MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|█▏    | 934M/4.92G [00:01<00:04, 805MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.24G/4.92G [00:01<00:03, 1.04GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▌   | 1.52G/4.92G [00:02<00:04, 785MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.90G/4.92G [00:02<00:02, 1.09GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.15G/4.92G [00:02<00:02, 1.11GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.35G/4.92G [00:03<00:02, 1.07GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.52G/4.92G [00:03<00:02, 1.01GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.66G/4.92G [00:03<00:02, 1.06GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.86G/4.92G [00:03<00:01, 1.23GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.07G/4.92G [00:03<00:01, 1.22GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.27G/4.92G [00:03<00:01, 1.28GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.47G/4.92G [00:03<00:01, 1.43GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.68G/4.92G [00:04<00:00, 1.42GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.86G/4.92G [00:04<00:00, 1.35GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.02G/4.92G [00:04<00:00, 1.33GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.20G/4.92G [00:04<00:00, 1.37GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.46G/4.92G [00:04<00:00, 1.63GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.71G/4.92G [00:04<00:00, 1.82GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.92G/4.92G [00:04<00:00, 1.01GB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:13<00:04,  4.71s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|    | 30.1M/1.17G [00:00<00:32, 35.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|▊     | 164M/1.17G [00:00<00:04, 217MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|█▎    | 250M/1.17G [00:01<00:03, 283MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  43%|██▌   | 508M/1.17G [00:01<00:01, 652MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  71%|███▌ | 824M/1.17G [00:01<00:00, 1.09GB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:01<00:00, 746MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:15<00:00,  3.91s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.24s/it]\n",
      "generation_config.json: 100%|██████████████████| 177/177 [00:00<00:00, 2.20MB/s]\n",
      "\u001b[32m[2025-10-14 12:45:36 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "\u001b[32m[2025-10-14 12:45:46 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-14 12:45:47 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 529)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 4.1 bits\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 530)\u001b[0m: INFO   Bit-widths: [6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 4, 6]\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 531)\u001b[0m: INFO   Actual Avg: 4.12 bits\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 532)\u001b[0m: INFO   Compression: 3.88x vs FP16\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-14 12:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 12:46:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:46:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-14 12:46:41 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-14 12:46:41 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-14 12:47:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000063 | Val Loss: 0.000050 | Grad Norm: 0.09 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:47:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000075 | Val Loss: 0.000018 | Grad Norm: 0.08 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:47:34 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-14 12:47:34 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 12:47:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000027 | Val Loss: 0.000022 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:48:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000026 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-14 12:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 12:48:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000033 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:49:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000031 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:49:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-14 12:49:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 12:49:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000044 | Val Loss: 0.000034 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:50:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000037 | Val Loss: 0.000033 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 12:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-14 12:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 12:50:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000050 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 12:51:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000042 | Val Loss: 0.000040 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 12:51:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-14 12:51:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-14 12:51:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000060 | Val Loss: 0.000055 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:51:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000055 | Val Loss: 0.000053 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:52:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-14 12:52:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 12:52:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000076 | Val Loss: 0.000074 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:52:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000071 | Val Loss: 0.000071 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-14 12:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 12:53:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000092 | Val Loss: 0.000093 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000087 | Val Loss: 0.000090 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:53:54 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-14 12:53:54 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 12:54:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000111 | Val Loss: 0.000118 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:54:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000104 | Val Loss: 0.000112 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:54:48 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-14 12:54:48 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 12:55:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000177 | Val Loss: 0.000184 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:55:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000161 | Val Loss: 0.000176 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:55:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-14 12:55:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 12:56:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000232 | Val Loss: 0.000245 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:56:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000214 | Val Loss: 0.000236 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:56:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-14 12:56:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 12:57:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000274 | Val Loss: 0.000293 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 12:57:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000255 | Val Loss: 0.000283 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 12:57:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-14 12:57:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 12:57:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000273 | Val Loss: 0.000304 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:58:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000256 | Val Loss: 0.000296 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:58:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-14 12:58:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 12:58:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000355 | Val Loss: 0.000391 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:59:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000329 | Val Loss: 0.000379 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 12:59:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-14 12:59:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 12:59:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000345 | Val Loss: 0.000395 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:00:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000326 | Val Loss: 0.000387 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:00:12 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-14 13:00:12 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 13:00:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000445 | Val Loss: 0.000504 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:00:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000415 | Val Loss: 0.000490 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:01:06 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-14 13:01:06 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 13:01:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000553 | Val Loss: 0.000633 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:01:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000519 | Val Loss: 0.000616 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:02:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-14 13:02:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 13:02:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000658 | Val Loss: 0.000765 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:02:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000621 | Val Loss: 0.000747 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-14 13:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 13:03:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.000778 | Val Loss: 0.000914 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:03:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000739 | Val Loss: 0.000898 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:03:47 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-14 13:03:47 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 13:04:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001357 | Val Loss: 0.001449 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:04:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001224 | Val Loss: 0.001418 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:04:41 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-14 13:04:41 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 13:05:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.002047 | Val Loss: 0.002174 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:05:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.001853 | Val Loss: 0.002131 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:05:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-14 13:05:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 13:05:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002723 | Val Loss: 0.002934 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002506 | Val Loss: 0.002891 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:06:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-14 13:06:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:06:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.008006 | Val Loss: 0.007256 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:07:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.006372 | Val Loss: 0.006869 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:07:23 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-14 13:07:23 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:07:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.012652 | Val Loss: 0.012009 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:08:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.010783 | Val Loss: 0.011564 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:08:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-14 13:08:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:08:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.018423 | Val Loss: 0.018004 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:09:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.016299 | Val Loss: 0.017489 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:09:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-14 13:09:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:09:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.025912 | Val Loss: 0.025746 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:09:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.023391 | Val Loss: 0.025139 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.2s\n",
      "\u001b[32m[2025-10-14 13:10:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-14 13:10:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:10:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.036020 | Val Loss: 0.036108 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:10:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.032801 | Val Loss: 0.035333 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:10:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-14 13:10:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 13:11:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.049668 | Val Loss: 0.050150 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 20.3s\n",
      "\u001b[32m[2025-10-14 13:11:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.045618 | Val Loss: 0.049187 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.2s\n",
      "\u001b[32m[2025-10-14 13:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-14 13:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 13:12:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.056445 | Val Loss: 0.059986 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.054143 | Val Loss: 0.059434 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:12:47 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-14 13:12:47 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 13:13:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.068548 | Val Loss: 0.074932 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:13:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.066329 | Val Loss: 0.074385 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:13:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-14 13:13:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 13:14:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.111157 | Val Loss: 0.129004 | Grad Norm: 0.85 | LR: 5.14e-05 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:14:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.106156 | Val Loss: 0.126530 | Grad Norm: 0.39 | LR: 5.00e-06 | Time: 20.4s\n",
      "\u001b[32m[2025-10-14 13:14:36 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_4.5bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 13:14:36 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_4.5bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 13:14:36 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 1740.10s (29.00min)\n",
      "\u001b[32m[2025-10-14 13:14:36 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-14 13:14:41 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_4.5bit/model\n",
      "\u001b[32m[2025-10-14 13:14:41 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 141/141 [01:49<00:00,  1.29it/s]\n",
      "wikitext2:8.385723114013672\n",
      "\u001b[32m[2025-10-14 13:16:34 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 8.39\n",
      "\u001b[32m[2025-10-14 13:16:35 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-14 13:16:36 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-14 13:16:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 170064.75it/s]\n",
      "\u001b[32m[2025-10-14 13:16:47 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5442.76it/s]\n",
      "\u001b[32m[2025-10-14 13:16:50 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2337.04it/s]\n",
      "\u001b[32m[2025-10-14 13:16:51 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1834.99it/s]\n",
      "\u001b[32m[2025-10-14 13:16:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:45<00:00, 33.55it/s]\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7348|±  |0.0124|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5773|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7589|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7677|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7525|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7758|±  |0.0097|\n",
      "|          |       |none  |     0|acc_norm|0.7851|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 71.39%\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_4.5bit/results.json\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-14 13:44:54 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy adaptive \\\n",
    "         --target_avg_bits 4.1 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_4.5bit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_4.5bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93544f9c",
   "metadata": {},
   "source": [
    "# F16 base scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N6FvvVBcy2gT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100052,
     "status": "ok",
     "timestamp": 1761822468221,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "N6FvvVBcy2gT",
    "outputId": "5b74c1c3-c5ec-4c0d-9191-7a5c2b622e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--eval_tasks', 'arc_challenge', '--output_dir', './output/llama3_f16_baseline1', '--wbits', '16']\n",
      "\u001b[32m[2025-10-30 11:06:12 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_f16_baseline1', save_quant_dir=None, real_quant=False, resume_quant=None, calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=False, eval_tasks='arc_challenge', eval_batch_size=16, wbits=16, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-30 11:06:12 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.03it/s]\n",
      "2025-10-30 11:06:22.383299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761822382.403115   53195 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761822382.409082   53195 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761822382.426620   53195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761822382.426644   53195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761822382.426647   53195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761822382.426650   53195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 11:06:27 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 11:06:28 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 11:06:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "Downloading data: 100% 190k/190k [00:00<00:00, 728kB/s]\n",
      "Downloading data: 100% 204k/204k [00:00<00:00, 846kB/s]\n",
      "Downloading data: 100% 55.7k/55.7k [00:00<00:00, 228kB/s]\n",
      "Generating train split: 100% 1119/1119 [00:00<00:00, 24993.35 examples/s]\n",
      "Generating test split: 100% 1172/1172 [00:00<00:00, 321058.34 examples/s]\n",
      "Generating validation split: 100% 299/299 [00:00<00:00, 130512.74 examples/s]\n",
      "\u001b[32m[2025-10-30 11:06:36 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "\u001b[32m[2025-10-30 11:06:36 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_challenge on rank 0...\n",
      "100% 1172/1172 [00:01<00:00, 1053.82it/s]\n",
      "\u001b[32m[2025-10-30 11:06:38 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 4687/4687 [01:02<00:00, 74.93it/s] \n",
      "\u001b[32m[2025-10-30 11:07:46 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     0|acc     |0.5034|±  |0.0146|\n",
      "|             |       |none  |     0|acc_norm|0.5350|±  |0.0146|\n",
      "\n",
      "\u001b[32m[2025-10-30 11:07:46 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 50.34%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "    --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "    --eval_tasks \"arc_challenge\" \\\n",
    "    --output_dir ./output/llama3_f16_baseline1 \\\n",
    "    --wbits 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hUF3vx7h522i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81757,
     "status": "ok",
     "timestamp": 1761927516867,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "hUF3vx7h522i",
    "outputId": "d6b01276-ef9d-4f83-b9db-ffc6554692e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande', '--output_dir', './output/llama2_f16_baseline', '--wbits', '16']\n",
      "\u001b[32m[2025-10-31 15:51:38 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/llama2_f16_baseline', save_quant_dir=None, real_quant=False, resume_quant=None, calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=16, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-31 15:51:38 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 596/596 [00:00<00:00, 5.45MB/s]\n",
      "tokenizer_config.json: 2.10kB [00:00, 12.9MB/s]\n",
      "tokenizer.model: 100% 493k/493k [00:01<00:00, 390kB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 4.63MB/s]\n",
      "tokenizer.json: 1.80MB [00:00, 182MB/s]\n",
      "model.safetensors.index.json: 25.1kB [00:00, 114MB/s]\n",
      "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
      "model-00001-of-00003.safetensors:   0% 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   0% 711k/4.94G [00:01<2:48:15, 490kB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   7% 361M/4.94G [00:01<00:19, 235MB/s]  \u001b[A\n",
      "model-00001-of-00003.safetensors:  19% 941M/4.94G [00:02<00:08, 494MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  27% 1.34G/4.94G [00:02<00:04, 764MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  31% 1.55G/4.94G [00:03<00:05, 611MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  34% 1.68G/4.94G [00:03<00:05, 606MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  37% 1.81G/4.94G [00:03<00:06, 494MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  39% 1.95G/4.94G [00:04<00:05, 507MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  42% 2.08G/4.94G [00:04<00:06, 462MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  43% 2.15G/4.94G [00:04<00:06, 441MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  45% 2.22G/4.94G [00:04<00:06, 445MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  46% 2.28G/4.94G [00:05<00:06, 431MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48% 2.35G/4.94G [00:07<00:23, 112MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  52% 2.55G/4.94G [00:07<00:11, 207MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  56% 2.75G/4.94G [00:07<00:06, 322MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  58% 2.89G/4.94G [00:07<00:05, 407MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  61% 3.02G/4.94G [00:07<00:04, 470MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  63% 3.14G/4.94G [00:07<00:03, 475MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  66% 3.27G/4.94G [00:08<00:03, 551MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  70% 3.47G/4.94G [00:08<00:02, 733MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73% 3.61G/4.94G [00:08<00:02, 583MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  76% 3.74G/4.94G [00:09<00:02, 485MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  78% 3.87G/4.94G [00:09<00:02, 361MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  80% 3.94G/4.94G [00:10<00:05, 173MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  81% 4.01G/4.94G [00:11<00:05, 177MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85% 4.21G/4.94G [00:11<00:02, 303MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  89% 4.41G/4.94G [00:11<00:01, 444MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  92% 4.54G/4.94G [00:11<00:00, 514MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  95% 4.67G/4.94G [00:11<00:00, 578MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  97% 4.81G/4.94G [00:11<00:00, 688MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors: 100% 4.94G/4.94G [00:12<00:00, 407MB/s]\n",
      "Downloading shards:  33% 1/3 [00:12<00:25, 12.66s/it]\n",
      "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   0% 754k/5.00G [00:01<2:35:14, 537kB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  10% 497M/5.00G [00:02<00:16, 267MB/s]  \u001b[A\n",
      "model-00002-of-00003.safetensors:  21% 1.05G/5.00G [00:02<00:06, 574MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  24% 1.19G/5.00G [00:02<00:06, 599MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  26% 1.32G/5.00G [00:03<00:06, 570MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  29% 1.46G/5.00G [00:03<00:07, 488MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  31% 1.54G/5.00G [00:03<00:07, 487MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  32% 1.61G/5.00G [00:03<00:07, 454MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  33% 1.67G/5.00G [00:04<00:10, 311MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  34% 1.71G/5.00G [00:04<00:11, 292MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  35% 1.75G/5.00G [00:04<00:12, 270MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  36% 1.80G/5.00G [00:04<00:11, 289MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  39% 1.96G/5.00G [00:04<00:06, 477MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  42% 2.08G/5.00G [00:05<00:10, 272MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  48% 2.38G/5.00G [00:05<00:04, 530MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  51% 2.55G/5.00G [00:06<00:05, 468MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  54% 2.69G/5.00G [00:06<00:05, 440MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  56% 2.78G/5.00G [00:06<00:05, 429MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  57% 2.87G/5.00G [00:07<00:05, 413MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  59% 2.93G/5.00G [00:07<00:05, 359MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  61% 3.04G/5.00G [00:07<00:04, 411MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  62% 3.11G/5.00G [00:07<00:04, 405MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64% 3.22G/5.00G [00:08<00:04, 389MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  67% 3.35G/5.00G [00:08<00:04, 394MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  68% 3.42G/5.00G [00:08<00:03, 401MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  70% 3.48G/5.00G [00:08<00:03, 400MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  71% 3.53G/5.00G [00:08<00:03, 390MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  72% 3.58G/5.00G [00:09<00:04, 292MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73% 3.63G/5.00G [00:09<00:05, 246MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73% 3.67G/5.00G [00:09<00:06, 206MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  75% 3.74G/5.00G [00:10<00:07, 172MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  76% 3.81G/5.00G [00:10<00:08, 146MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  79% 3.97G/5.00G [00:11<00:03, 277MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  82% 4.11G/5.00G [00:11<00:02, 379MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  84% 4.18G/5.00G [00:13<00:06, 128MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87% 4.34G/5.00G [00:13<00:03, 205MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92% 4.58G/5.00G [00:13<00:01, 357MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  95% 4.76G/5.00G [00:13<00:00, 486MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:13<00:00, 366MB/s]\n",
      "Downloading shards:  67% 2/3 [00:26<00:13, 13.54s/it]\n",
      "model-00003-of-00003.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   0% 816k/4.54G [00:01<2:00:51, 626kB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   6% 269M/4.54G [00:02<00:26, 164MB/s]  \u001b[A\n",
      "model-00003-of-00003.safetensors:  22% 1.01G/4.54G [00:02<00:07, 503MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  31% 1.41G/4.54G [00:03<00:05, 590MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34% 1.55G/4.54G [00:03<00:05, 510MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36% 1.61G/4.54G [00:03<00:06, 467MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  37% 1.68G/4.54G [00:04<00:06, 428MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  38% 1.75G/4.54G [00:04<00:07, 392MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40% 1.81G/4.54G [00:04<00:07, 382MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  43% 1.95G/4.54G [00:04<00:05, 483MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  46% 2.08G/4.54G [00:04<00:04, 592MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  49% 2.22G/4.54G [00:05<00:03, 655MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52% 2.35G/4.54G [00:05<00:04, 527MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53% 2.42G/4.54G [00:05<00:04, 496MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55% 2.48G/4.54G [00:05<00:04, 460MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56% 2.55G/4.54G [00:05<00:04, 439MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  58% 2.62G/4.54G [00:06<00:04, 420MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59% 2.68G/4.54G [00:06<00:04, 410MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  61% 2.75G/4.54G [00:06<00:04, 378MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62% 2.82G/4.54G [00:06<00:04, 392MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64% 2.88G/4.54G [00:06<00:04, 393MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  65% 2.95G/4.54G [00:07<00:04, 390MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  66% 3.00G/4.54G [00:07<00:03, 397MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68% 3.06G/4.54G [00:07<00:03, 380MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69% 3.13G/4.54G [00:07<00:03, 385MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70% 3.20G/4.54G [00:07<00:03, 376MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  72% 3.27G/4.54G [00:07<00:03, 376MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73% 3.33G/4.54G [00:08<00:03, 376MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75% 3.40G/4.54G [00:08<00:03, 379MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76% 3.47G/4.54G [00:08<00:03, 285MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  78% 3.53G/4.54G [00:08<00:03, 325MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79% 3.60G/4.54G [00:08<00:02, 375MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  81% 3.67G/4.54G [00:08<00:02, 417MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82% 3.74G/4.54G [00:09<00:01, 445MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84% 3.80G/4.54G [00:09<00:01, 394MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85% 3.87G/4.54G [00:09<00:01, 381MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  87% 3.94G/4.54G [00:09<00:01, 378MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88% 4.00G/4.54G [00:09<00:01, 349MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  90% 4.07G/4.54G [00:10<00:01, 335MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91% 4.14G/4.54G [00:10<00:01, 331MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93% 4.21G/4.54G [00:10<00:01, 335MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94% 4.27G/4.54G [00:10<00:00, 323MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96% 4.34G/4.54G [00:10<00:00, 311MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100% 4.54G/4.54G [00:11<00:00, 403MB/s]\n",
      "Downloading shards: 100% 3/3 [00:41<00:00, 13.72s/it]\n",
      "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
      "generation_config.json: 100% 111/111 [00:00<00:00, 1.35MB/s]\n",
      "get_wikitext2\n",
      "100% 163/163 [01:53<00:00,  1.43it/s]\n",
      "wikitext2:5.9415435791015625\n",
      "\u001b[32m[2025-10-31 15:54:38 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 5.94\n",
      "2025-10-31 15:54:38.515455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761926078.533574   10217 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761926078.539000   10217 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761926078.552994   10217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761926078.553075   10217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761926078.553078   10217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761926078.553081   10217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-31 15:54:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 15:54:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 15:54:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 124813.47it/s]\n",
      "\u001b[32m[2025-10-31 15:55:17 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2240.58it/s]\n",
      "\u001b[32m[2025-10-31 15:55:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1058.30it/s]\n",
      "\u001b[32m[2025-10-31 15:55:25 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1023.99it/s]\n",
      "\u001b[32m[2025-10-31 15:55:27 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [22:26<00:00, 41.51it/s] \n",
      "\u001b[32m[2025-10-31 16:18:33 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7395|±  |0.0123|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6601|±  |0.0047|\n",
      "|          |       |none  |     0|acc_norm|0.8364|±  |0.0037|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8131|±  |0.0080|\n",
      "|          |       |none  |     0|acc_norm|0.7677|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.8009|±  |0.0093|\n",
      "|          |       |none  |     0|acc_norm|0.8058|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-31 16:18:33 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 75.34%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "    --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\" \\\n",
    "    --output_dir ./output/llama2_f16_baseline \\\n",
    "    --wbits 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uuL053Re1sch",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1699427,
     "status": "ok",
     "timestamp": 1761925750794,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "uuL053Re1sch",
    "outputId": "1832e013-e2b2-413c-b0e6-91afbc2d116e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Llama-2-7b-hf', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande', '--output_dir', './output/llama2_f16_baseline', '--wbits', '16']\n",
      "\u001b[32m[2025-10-31 15:20:58 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/llama2_f16_baseline', save_quant_dir=None, real_quant=False, resume_quant=None, calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=16, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-31 15:20:58 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 609/609 [00:00<00:00, 4.10MB/s]\n",
      "tokenizer_config.json: 100% 776/776 [00:00<00:00, 6.30MB/s]\n",
      "tokenizer.model: 100% 500k/500k [00:01<00:00, 385kB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.29MB/s]\n",
      "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 4.31MB/s]\n",
      "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 105MB/s]\n",
      "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 8.96k/9.98G [00:01<536:56:19, 5.16kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 25.0M/9.98G [00:03<20:08, 8.23MB/s]    \u001b[A\n",
      "model-00001-of-00002.safetensors:  11% 1.11G/9.98G [00:03<00:17, 496MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  16% 1.63G/9.98G [00:04<00:13, 603MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 2.01G/9.98G [00:05<00:15, 531MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22% 2.24G/9.98G [00:05<00:15, 511MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24% 2.43G/9.98G [00:06<00:15, 485MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26% 2.60G/9.98G [00:06<00:15, 464MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27% 2.71G/9.98G [00:07<00:19, 377MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28% 2.82G/9.98G [00:07<00:18, 393MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 2.88G/9.98G [00:07<00:18, 381MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30% 3.04G/9.98G [00:07<00:15, 435MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31% 3.11G/9.98G [00:10<00:50, 135MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 4.00G/9.98G [00:10<00:11, 502MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42% 4.23G/9.98G [00:12<00:23, 247MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44% 4.36G/9.98G [00:13<00:20, 276MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45% 4.48G/9.98G [00:13<00:18, 304MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46% 4.58G/9.98G [00:13<00:15, 338MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48% 4.75G/9.98G [00:13<00:12, 434MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49% 4.89G/9.98G [00:13<00:09, 524MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 5.05G/9.98G [00:13<00:08, 583MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52% 5.16G/9.98G [00:14<00:08, 592MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53% 5.26G/9.98G [00:14<00:08, 540MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54% 5.37G/9.98G [00:14<00:09, 493MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.47G/9.98G [00:14<00:07, 568MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56% 5.58G/9.98G [00:14<00:06, 650MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57% 5.67G/9.98G [00:14<00:06, 673MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58% 5.76G/9.98G [00:15<00:12, 339MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 5.88G/9.98G [00:17<00:26, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61% 6.05G/9.98G [00:17<00:16, 243MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 6.17G/9.98G [00:17<00:12, 313MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.27G/9.98G [00:17<00:10, 356MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64% 6.38G/9.98G [00:17<00:08, 427MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.49G/9.98G [00:17<00:06, 504MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66% 6.60G/9.98G [00:18<00:09, 372MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.68G/9.98G [00:18<00:10, 325MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.79G/9.98G [00:18<00:10, 309MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.83G/9.98G [00:19<00:12, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.88G/9.98G [00:19<00:15, 203MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.90G/9.98G [00:19<00:16, 191MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70% 6.98G/9.98G [00:20<00:12, 244MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 7.04G/9.98G [00:20<00:10, 281MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 7.10G/9.98G [00:20<00:11, 252MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72% 7.17G/9.98G [00:20<00:08, 312MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.24G/9.98G [00:20<00:08, 326MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.28G/9.98G [00:21<00:09, 289MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74% 7.35G/9.98G [00:21<00:08, 326MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74% 7.41G/9.98G [00:21<00:10, 237MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.46G/9.98G [00:21<00:09, 264MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.49G/9.98G [00:22<00:15, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.52G/9.98G [00:22<00:16, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76% 7.59G/9.98G [00:22<00:10, 217MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.64G/9.98G [00:23<00:13, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.67G/9.98G [00:23<00:18, 123MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 7.74G/9.98G [00:23<00:12, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 7.80G/9.98G [00:23<00:09, 240MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:24<00:10, 195MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.91G/9.98G [00:24<00:08, 242MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80% 7.95G/9.98G [00:24<00:14, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.04G/9.98G [00:25<00:09, 211MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.08G/9.98G [00:25<00:09, 196MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82% 8.16G/9.98G [00:25<00:06, 261MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.26G/9.98G [00:25<00:04, 377MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.33G/9.98G [00:25<00:04, 360MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84% 8.40G/9.98G [00:26<00:05, 289MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 8.46G/9.98G [00:26<00:05, 290MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 8.50G/9.98G [00:26<00:06, 215MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.54G/9.98G [00:27<00:11, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.58G/9.98G [00:27<00:09, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.62G/9.98G [00:27<00:09, 140MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 8.67G/9.98G [00:28<00:10, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 8.71G/9.98G [00:29<00:13, 94.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88% 8.74G/9.98G [00:29<00:12, 98.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88% 8.79G/9.98G [00:29<00:10, 109MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.85G/9.98G [00:29<00:07, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.91G/9.98G [00:30<00:07, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.04G/9.98G [00:30<00:03, 251MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:30<00:03, 227MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 9.20G/9.98G [00:31<00:02, 311MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93% 9.31G/9.98G [00:31<00:01, 367MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 9.36G/9.98G [00:31<00:02, 211MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 9.39G/9.98G [00:32<00:03, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.46G/9.98G [00:32<00:02, 210MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.52G/9.98G [00:32<00:02, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96% 9.59G/9.98G [00:33<00:01, 224MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 9.66G/9.98G [00:33<00:01, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 9.71G/9.98G [00:33<00:01, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 9.78G/9.98G [00:34<00:01, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99% 9.91G/9.98G [00:35<00:00, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:35<00:00, 278MB/s]\n",
      "Downloading shards:  50% 1/2 [00:36<00:36, 36.39s/it]\n",
      "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 8.66k/3.50G [00:02<268:30:09, 3.62kB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 1.30M/3.50G [00:02<1:19:55, 730kB/s]   \u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 4.35M/3.50G [00:02<21:17, 2.74MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 7.70M/3.50G [00:04<26:46, 2.17MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4% 128M/3.50G [00:04<00:56, 60.1MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:   5% 173M/3.50G [00:05<00:52, 63.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6% 208M/3.50G [00:05<00:41, 79.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7% 237M/3.50G [00:05<00:46, 70.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 263M/3.50G [00:06<00:46, 69.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 289M/3.50G [00:06<00:48, 66.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9% 314M/3.50G [00:06<00:40, 79.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10% 345M/3.50G [00:07<00:40, 78.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11% 373M/3.50G [00:07<00:36, 85.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12% 412M/3.50G [00:07<00:29, 104MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  13% 445M/3.50G [00:08<00:27, 112MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13% 460M/3.50G [00:08<00:34, 87.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14% 481M/3.50G [00:08<00:30, 99.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15% 518M/3.50G [00:08<00:22, 132MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  17% 585M/3.50G [00:08<00:14, 201MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18% 624M/3.50G [00:09<00:14, 198MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19% 662M/3.50G [00:09<00:16, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20% 687M/3.50G [00:09<00:15, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21% 735M/3.50G [00:09<00:20, 137MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22% 772M/3.50G [00:12<01:07, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34% 1.19G/3.50G [00:12<00:10, 214MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37% 1.30G/3.50G [00:13<00:10, 212MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41% 1.42G/3.50G [00:13<00:09, 210MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43% 1.49G/3.50G [00:13<00:08, 225MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45% 1.56G/3.50G [00:14<00:08, 238MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46% 1.61G/3.50G [00:14<00:09, 196MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49% 1.71G/3.50G [00:14<00:06, 266MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52% 1.82G/3.50G [00:15<00:05, 283MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56% 1.96G/3.50G [00:15<00:04, 386MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60% 2.10G/3.50G [00:15<00:02, 503MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:16<00:06, 211MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66% 2.30G/3.50G [00:17<00:06, 201MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68% 2.39G/3.50G [00:17<00:04, 251MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73% 2.54G/3.50G [00:17<00:02, 330MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74% 2.61G/3.50G [00:17<00:02, 362MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76% 2.67G/3.50G [00:19<00:05, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82% 2.88G/3.50G [00:19<00:02, 265MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85% 2.97G/3.50G [00:19<00:02, 209MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87% 3.05G/3.50G [00:20<00:02, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90% 3.15G/3.50G [00:21<00:02, 143MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91% 3.20G/3.50G [00:22<00:02, 115MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93% 3.25G/3.50G [00:22<00:01, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94% 3.31G/3.50G [00:24<00:02, 78.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:24<00:01, 91.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98% 3.43G/3.50G [00:25<00:00, 93.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:26<00:00, 134MB/s] \n",
      "Downloading shards: 100% 2/2 [01:02<00:00, 31.50s/it]\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  7.21it/s]\n",
      "generation_config.json: 100% 188/188 [00:00<00:00, 1.72MB/s]\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 29.3MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 1.11MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:01<00:00, 3.79MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 1.05MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 80460.15 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 723612.53 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 611499.92 examples/s]\n",
      "100% 166/166 [01:47<00:00,  1.54it/s]\n",
      "wikitext2:5.472263336181641\n",
      "\u001b[32m[2025-10-31 15:24:18 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 5.47\n",
      "2025-10-31 15:24:20.159795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761924260.181195    2262 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761924260.187692    2262 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761924260.208824    2262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761924260.208853    2262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761924260.208856    2262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761924260.208858    2262 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 18.3MB/s]\n",
      "\u001b[32m[2025-10-31 15:24:27 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 15:24:27 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 15:24:31 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 22.8MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 30.5MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:00<00:00, 3.80MB/s]\n",
      "Downloading data: 815kB [00:00, 78.2MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 35090.88 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37928.90 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 35951.57 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 31.6MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.05MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.12MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 325kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 370642.57 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 473495.81 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 260600.97 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 23.4MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 38.0MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 12.3MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 12.2MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 257476.61 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 265998.15 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 245785.05 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 6545.13 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6758.61 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 48.1MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 2.99MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 238kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 176kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1197009.57 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 679876.63 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 549496.76 examples/s]\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 128458.10it/s]\n",
      "\u001b[32m[2025-10-31 15:25:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2426.28it/s]\n",
      "\u001b[32m[2025-10-31 15:25:29 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1047.39it/s]\n",
      "\u001b[32m[2025-10-31 15:25:31 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1018.20it/s]\n",
      "\u001b[32m[2025-10-31 15:25:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [22:53<00:00, 40.69it/s] \n",
      "\u001b[32m[2025-10-31 15:49:08 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6914|±  |0.0130|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5713|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7602|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7630|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7454|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7807|±  |0.0097|\n",
      "|          |       |none  |     0|acc_norm|0.7911|±  |0.0095|\n",
      "\n",
      "\u001b[32m[2025-10-31 15:49:08 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 70.16%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "    --model \"meta-llama/Llama-2-7b-hf\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\" \\\n",
    "    --output_dir ./output/llama2_f16_baseline \\\n",
    "    --wbits 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r3fLNN3MVUc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1551571,
     "status": "ok",
     "timestamp": 1761822126391,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "r3fLNN3MVUc2",
    "outputId": "893b7396-36a7-4c89-e1cc-822c75c7d23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande', '--output_dir', './output/llama3_f16_baseline', '--wbits', '16']\n",
      "\u001b[32m[2025-10-30 10:36:19 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_f16_baseline', save_quant_dir=None, real_quant=False, resume_quant=None, calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=16, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-30 10:36:19 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.02it/s]\n",
      "get_wikitext2\n",
      "100% 141/141 [01:38<00:00,  1.43it/s]\n",
      "wikitext2:6.135848045349121\n",
      "\u001b[32m[2025-10-30 10:38:17 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 6.14\n",
      "2025-10-30 10:38:17.687330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761820697.707953   45593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761820697.714061   45593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761820697.731843   45593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761820697.731868   45593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761820697.731871   45593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761820697.731874   45593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 10:38:22 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 10:38:23 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 10:38:27 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126600.51it/s]\n",
      "\u001b[32m[2025-10-30 10:38:54 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2401.34it/s]\n",
      "\u001b[32m[2025-10-30 10:39:00 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1047.51it/s]\n",
      "\u001b[32m[2025-10-30 10:39:02 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1012.21it/s]\n",
      "\u001b[32m[2025-10-30 10:39:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [22:17<00:00, 41.78it/s] \n",
      "\u001b[32m[2025-10-30 11:02:03 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7269|±  |0.0125|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6018|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7915|±  |0.0041|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8009|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7769|±  |0.0085|\n",
      "|piqa      |      1|none  |     0|acc     |0.7965|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8079|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-30 11:02:03 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 73.15%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "    --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\" \\\n",
    "    --output_dir ./output/llama3_f16_baseline \\\n",
    "    --wbits 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ce0c9",
   "metadata": {},
   "source": [
    "# 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffcf53",
   "metadata": {
    "id": "b3ffcf53",
    "outputId": "e7051d84-087a-4b16-cc44-1c5d52cfbf25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_adaptive_training', '--target_avg_bits', '4.0', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama_adaptive/llama3_optimized_4.0bit', '--save_quant_dir', './output/llama_adaptive/llama3_optimized_4.0bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama_adaptive/llama3_optimized_4.0bit', save_quant_dir='./output/llama_adaptive/llama3_optimized_4.0bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-14 14:04:33 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.24s/it]\n",
      "\u001b[32m[2025-10-14 14:04:43 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-14 14:04:43 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_train.cache\n",
      "\u001b[32m[2025-10-14 14:04:43 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_val.cache\n",
      "\u001b[32m[2025-10-14 14:04:43 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-14 14:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:04:46 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-14 14:05:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 5.57e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 14:05:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 2.07e-05 | Time: 20.6s\n",
      "\u001b[32m[2025-10-14 14:05:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000001 | Val Loss: 0.000001 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 20.6s\n",
      "\u001b[32m[2025-10-14 14:06:02 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-14 14:06:02 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:06:02 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-14 14:06:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000038 | Val Loss: 0.000029 | Grad Norm: 0.01 | LR: 5.71e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 14:06:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000064 | Val Loss: 0.000028 | Grad Norm: 0.02 | LR: 3.46e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 14:07:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000031 | Val Loss: 0.000019 | Grad Norm: 0.01 | LR: 1.23e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:07:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000016 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:07:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-14 14:07:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:07:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-14 14:08:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000026 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 5.67e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:08:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000024 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 2.11e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:08:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000024 | Val Loss: 0.000026 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 14:08:51 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-14 14:08:51 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:08:51 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:09:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000044 | Val Loss: 0.000047 | Grad Norm: 0.00 | LR: 5.75e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:09:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000042 | Val Loss: 0.000046 | Grad Norm: 0.00 | LR: 2.14e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:09:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000041 | Val Loss: 0.000045 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:10:06 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-14 14:10:06 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:10:06 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:10:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000069 | Val Loss: 0.000073 | Grad Norm: 0.00 | LR: 5.88e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:10:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000065 | Val Loss: 0.000070 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:11:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000063 | Val Loss: 0.000069 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:11:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-14 14:11:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:11:22 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:11:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000099 | Val Loss: 0.000105 | Grad Norm: 0.00 | LR: 5.93e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:12:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000094 | Val Loss: 0.000102 | Grad Norm: 0.00 | LR: 2.20e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:12:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000090 | Val Loss: 0.000101 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-14 14:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000132 | Val Loss: 0.000143 | Grad Norm: 0.00 | LR: 6.02e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:13:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000125 | Val Loss: 0.000138 | Grad Norm: 0.00 | LR: 2.24e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:13:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000120 | Val Loss: 0.000136 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-14 14:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:14:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000172 | Val Loss: 0.000189 | Grad Norm: 0.00 | LR: 6.16e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:14:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000163 | Val Loss: 0.000183 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:15:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000157 | Val Loss: 0.000181 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-14 14:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:15:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000209 | Val Loss: 0.000234 | Grad Norm: 0.00 | LR: 6.25e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:15:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000198 | Val Loss: 0.000227 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000190 | Val Loss: 0.000224 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:16:27 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-14 14:16:27 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:16:27 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:16:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000251 | Val Loss: 0.000290 | Grad Norm: 0.00 | LR: 6.31e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:17:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000239 | Val Loss: 0.000281 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000229 | Val Loss: 0.000276 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:17:43 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-14 14:17:43 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:17:43 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:18:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.000288 | Val Loss: 0.000339 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:18:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000274 | Val Loss: 0.000330 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:18:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000263 | Val Loss: 0.000326 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:18:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-14 14:18:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:18:59 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:19:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.000327 | Val Loss: 0.000392 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:19:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.000313 | Val Loss: 0.000382 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:20:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.000301 | Val Loss: 0.000377 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:20:16 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-14 14:20:16 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:20:16 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:20:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.000352 | Val Loss: 0.000424 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:21:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.000335 | Val Loss: 0.000415 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:21:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.000324 | Val Loss: 0.000410 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-14 14:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.000406 | Val Loss: 0.000496 | Grad Norm: 0.00 | LR: 6.39e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:22:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.000385 | Val Loss: 0.000484 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:22:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.000370 | Val Loss: 0.000477 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:22:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-14 14:22:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:22:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:23:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.000453 | Val Loss: 0.000563 | Grad Norm: 0.00 | LR: 6.49e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:23:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.000430 | Val Loss: 0.000548 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:23:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.000414 | Val Loss: 0.000541 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:24:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-14 14:24:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:24:05 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.000506 | Val Loss: 0.000638 | Grad Norm: 0.00 | LR: 6.42e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:24:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.000481 | Val Loss: 0.000623 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:25:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.000463 | Val Loss: 0.000615 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:25:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-14 14:25:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:25:22 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:25:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.000565 | Val Loss: 0.000727 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:26:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.000538 | Val Loss: 0.000713 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:26:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.000519 | Val Loss: 0.000702 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:26:38 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-14 14:26:38 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:26:38 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:27:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.000655 | Val Loss: 0.000858 | Grad Norm: 0.00 | LR: 6.62e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:27:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.000626 | Val Loss: 0.000843 | Grad Norm: 0.00 | LR: 2.46e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:27:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.000604 | Val Loss: 0.000832 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:27:54 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-14 14:27:54 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:27:54 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:28:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000749 | Val Loss: 0.001004 | Grad Norm: 0.00 | LR: 4.67e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:28:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000715 | Val Loss: 0.000984 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:28:51 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-14 14:28:51 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:28:51 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:29:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.000874 | Val Loss: 0.001180 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:29:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000838 | Val Loss: 0.001163 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:29:46 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-14 14:29:46 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:29:46 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:30:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001029 | Val Loss: 0.001399 | Grad Norm: 0.00 | LR: 4.86e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:30:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.000991 | Val Loss: 0.001384 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:30:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-14 14:30:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:30:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:31:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.001287 | Val Loss: 0.001777 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:31:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.001238 | Val Loss: 0.001752 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:31:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-14 14:31:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:31:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:32:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.001549 | Val Loss: 0.002164 | Grad Norm: 0.00 | LR: 4.79e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:32:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.001493 | Val Loss: 0.002141 | Grad Norm: 0.00 | LR: 4.67e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:32:32 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-14 14:32:32 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:32:32 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:32:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.001864 | Val Loss: 0.002647 | Grad Norm: 0.00 | LR: 5.00e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:33:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.001803 | Val Loss: 0.002621 | Grad Norm: 0.00 | LR: 4.87e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:33:27 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-14 14:33:27 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:33:27 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:33:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.002249 | Val Loss: 0.003220 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:34:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.002178 | Val Loss: 0.003196 | Grad Norm: 0.00 | LR: 4.92e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:34:23 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-14 14:34:23 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:34:23 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:34:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.002752 | Val Loss: 0.003979 | Grad Norm: 0.00 | LR: 5.08e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:35:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.002666 | Val Loss: 0.003947 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:35:18 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-14 14:35:18 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:35:18 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:35:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.003401 | Val Loss: 0.004967 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:36:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.003294 | Val Loss: 0.004925 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:36:14 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-14 14:36:14 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:36:14 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:36:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.004267 | Val Loss: 0.006255 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:36:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.004125 | Val Loss: 0.006206 | Grad Norm: 0.00 | LR: 4.96e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:37:09 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-14 14:37:09 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:37:09 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:37:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.005427 | Val Loss: 0.008011 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.005246 | Val Loss: 0.007954 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:38:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-14 14:38:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:38:05 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:38:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.007076 | Val Loss: 0.010492 | Grad Norm: 0.01 | LR: 4.95e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.006822 | Val Loss: 0.010400 | Grad Norm: 0.01 | LR: 4.82e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 14:39:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-14 14:39:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:39:00 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 14:39:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.009918 | Val Loss: 0.014692 | Grad Norm: 0.02 | LR: 4.61e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:39:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.009485 | Val Loss: 0.014511 | Grad Norm: 0.01 | LR: 4.49e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:39:56 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-14 14:39:56 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-14 14:39:56 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 14:40:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.021027 | Val Loss: 0.030522 | Grad Norm: 0.10 | LR: 5.67e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:40:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.019412 | Val Loss: 0.029617 | Grad Norm: 0.09 | LR: 2.11e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:41:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.018543 | Val Loss: 0.029106 | Grad Norm: 0.05 | LR: 3.75e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 14:41:12 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama_adaptive/llama3_optimized_4.0bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 14:41:12 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama_adaptive/llama3_optimized_4.0bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 14:41:12 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2189.57s (36.49min)\n",
      "\u001b[32m[2025-10-14 14:41:12 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-14 14:41:18 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama_adaptive/llama3_optimized_4.0bit/model\n",
      "\u001b[32m[2025-10-14 14:41:18 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 141/141 [01:48<00:00,  1.30it/s]\n",
      "wikitext2:6.422391414642334\n",
      "\u001b[32m[2025-10-14 14:43:10 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.42\n",
      "\u001b[32m[2025-10-14 14:43:11 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-14 14:43:12 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-14 14:43:15 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 167793.35it/s]\n",
      "\u001b[32m[2025-10-14 14:43:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5421.62it/s]\n",
      "\u001b[32m[2025-10-14 14:43:25 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2330.53it/s]\n",
      "\u001b[32m[2025-10-14 14:43:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1813.65it/s]\n",
      "\u001b[32m[2025-10-14 14:43:27 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:52<00:00, 33.42it/s]\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7395|±  |0.0123|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5936|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7777|±  |0.0041|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7988|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7875|±  |0.0084|\n",
      "|piqa      |      1|none  |     0|acc     |0.7840|±  |0.0096|\n",
      "|          |       |none  |     0|acc_norm|0.8003|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 72.90%\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama_adaptive/llama3_optimized_4.0bit/results.json\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-14 15:11:36 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_adaptive_training \\\n",
    "         --target_avg_bits 4.0 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama_adaptive/llama3_optimized_4.0bit\" \\\n",
    "         --save_quant_dir \"./output/llama_adaptive/llama3_optimized_4.0bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d352738",
   "metadata": {
    "id": "9d352738",
    "outputId": "28170dc4-7ea4-4237-d406-0a5c2ae7568d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '4.0', '--use_adaptive_training', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama3_optimized_4.0bit', '--save_quant_dir', './output/llama3_optimized_4.0bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_4.0bit', save_quant_dir='./output/llama3_optimized_4.0bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-14 15:38:46 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:09<00:00,  2.33s/it]\n",
      "\u001b[32m[2025-10-14 15:38:56 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-14 15:38:56 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_train.cache\n",
      "\u001b[32m[2025-10-14 15:38:56 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_val.cache\n",
      "\u001b[32m[2025-10-14 15:38:56 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-14 15:38:58 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 529)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 530)\u001b[0m: INFO   Bit-widths: [6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 6]\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 531)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 532)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 15:38:59 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-14 15:39:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.57e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 15:39:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 2.07e-05 | Time: 20.6s\n",
      "\u001b[32m[2025-10-14 15:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 20.6s\n",
      "\u001b[32m[2025-10-14 15:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-14 15:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-14 15:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-14 15:40:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000047 | Val Loss: 0.000025 | Grad Norm: 0.08 | LR: 5.71e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:40:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000066 | Val Loss: 0.000017 | Grad Norm: 0.09 | LR: 3.46e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:41:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000013 | Val Loss: 0.000011 | Grad Norm: 0.03 | LR: 1.23e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:41:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000007 | Val Loss: 0.000010 | Grad Norm: 0.02 | LR: 3.33e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:41:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-14 15:41:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 15:41:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-14 15:42:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000010 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 5.67e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:42:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000009 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 2.11e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:42:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000009 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:43:03 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-14 15:43:03 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 15:43:03 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:43:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000014 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 5.75e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:43:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000013 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 2.14e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:44:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000013 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:44:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-14 15:44:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 15:44:17 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:44:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000021 | Val Loss: 0.000022 | Grad Norm: 0.00 | LR: 5.88e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:45:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000017 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:45:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000016 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-14 15:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 15:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:45:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000026 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 5.93e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:46:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000022 | Val Loss: 0.000026 | Grad Norm: 0.00 | LR: 2.20e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:46:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000020 | Val Loss: 0.000026 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:46:50 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-14 15:46:50 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-14 15:46:50 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000036 | Val Loss: 0.000040 | Grad Norm: 0.00 | LR: 6.02e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:47:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000033 | Val Loss: 0.000039 | Grad Norm: 0.00 | LR: 2.24e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:47:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000031 | Val Loss: 0.000038 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-14 15:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 15:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:48:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000050 | Val Loss: 0.000057 | Grad Norm: 0.00 | LR: 6.16e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:48:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000047 | Val Loss: 0.000055 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:49:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000044 | Val Loss: 0.000053 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-14 15:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 15:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:49:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000064 | Val Loss: 0.000074 | Grad Norm: 0.00 | LR: 6.25e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:50:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000061 | Val Loss: 0.000071 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:50:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000057 | Val Loss: 0.000069 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-14 15:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 15:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:51:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000081 | Val Loss: 0.000094 | Grad Norm: 0.00 | LR: 6.31e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:51:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000077 | Val Loss: 0.000091 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:51:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000071 | Val Loss: 0.000088 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:51:54 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-14 15:51:54 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 15:51:54 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:52:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.000143 | Val Loss: 0.000159 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:52:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000132 | Val Loss: 0.000151 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:53:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000123 | Val Loss: 0.000147 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-14 15:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 15:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:53:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.000197 | Val Loss: 0.000218 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:53:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.000182 | Val Loss: 0.000208 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:54:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.000172 | Val Loss: 0.000203 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:54:27 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-14 15:54:27 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 15:54:27 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:54:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.000236 | Val Loss: 0.000262 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:55:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.000219 | Val Loss: 0.000252 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:55:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.000209 | Val Loss: 0.000248 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:55:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-14 15:55:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 15:55:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:56:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.000228 | Val Loss: 0.000268 | Grad Norm: 0.00 | LR: 6.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:56:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.000216 | Val Loss: 0.000262 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:56:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.000207 | Val Loss: 0.000258 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:56:57 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-14 15:56:57 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 15:56:57 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 15:57:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.000309 | Val Loss: 0.000353 | Grad Norm: 0.00 | LR: 6.49e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.000287 | Val Loss: 0.000341 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:58:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.000274 | Val Loss: 0.000335 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:58:13 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-14 15:58:13 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-14 15:58:13 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:58:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.000291 | Val Loss: 0.000353 | Grad Norm: 0.00 | LR: 6.42e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:58:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.000279 | Val Loss: 0.000346 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.000265 | Val Loss: 0.000340 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 15:59:29 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-14 15:59:29 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 15:59:29 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 15:59:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.000387 | Val Loss: 0.000459 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:00:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.000363 | Val Loss: 0.000446 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:00:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.000347 | Val Loss: 0.000438 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:00:45 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-14 16:00:45 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 16:00:45 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:01:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.000485 | Val Loss: 0.000579 | Grad Norm: 0.00 | LR: 6.62e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:01:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.000457 | Val Loss: 0.000563 | Grad Norm: 0.00 | LR: 2.46e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:01:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.000437 | Val Loss: 0.000554 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:02:01 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-14 16:02:01 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 16:02:01 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:02:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000574 | Val Loss: 0.000695 | Grad Norm: 0.00 | LR: 4.67e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:02:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000542 | Val Loss: 0.000683 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:02:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-14 16:02:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 16:02:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:03:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001083 | Val Loss: 0.001160 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:03:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000964 | Val Loss: 0.001133 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:03:50 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-14 16:03:50 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:03:50 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:04:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.004370 | Val Loss: 0.003814 | Grad Norm: 0.00 | LR: 4.86e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:04:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.003394 | Val Loss: 0.003565 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-14 16:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 16:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:05:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.004200 | Val Loss: 0.004308 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:05:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.003879 | Val Loss: 0.004238 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-14 16:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-14 16:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:06:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.004865 | Val Loss: 0.005145 | Grad Norm: 0.00 | LR: 4.79e-05 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:06:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.004574 | Val Loss: 0.005086 | Grad Norm: 0.00 | LR: 4.67e-06 | Time: 20.8s\n",
      "\u001b[32m[2025-10-14 16:06:34 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-14 16:06:34 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:06:34 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:06:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.010219 | Val Loss: 0.009641 | Grad Norm: 0.01 | LR: 5.00e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:07:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.008613 | Val Loss: 0.009267 | Grad Norm: 0.00 | LR: 4.87e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:07:29 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-14 16:07:29 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:07:29 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:07:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.015076 | Val Loss: 0.014638 | Grad Norm: 0.01 | LR: 5.06e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:08:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.013233 | Val Loss: 0.014204 | Grad Norm: 0.00 | LR: 4.92e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:08:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-14 16:08:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:08:24 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:08:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.021158 | Val Loss: 0.020990 | Grad Norm: 0.01 | LR: 5.08e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:09:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.019039 | Val Loss: 0.020457 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-14 16:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:09:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.028976 | Val Loss: 0.029097 | Grad Norm: 0.01 | LR: 5.09e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:10:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.026458 | Val Loss: 0.028493 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:10:14 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-14 16:10:14 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:10:14 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:10:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.039424 | Val Loss: 0.039896 | Grad Norm: 0.02 | LR: 5.09e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:11:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.036228 | Val Loss: 0.039117 | Grad Norm: 0.01 | LR: 4.96e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:11:10 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-14 16:11:10 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:11:10 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:11:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.053555 | Val Loss: 0.054440 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:11:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.049491 | Val Loss: 0.053454 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-14 16:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-14 16:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:12:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.074384 | Val Loss: 0.075664 | Grad Norm: 0.03 | LR: 4.95e-05 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:12:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.068721 | Val Loss: 0.074317 | Grad Norm: 0.02 | LR: 4.82e-06 | Time: 20.7s\n",
      "\u001b[32m[2025-10-14 16:13:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-14 16:13:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-14 16:13:00 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-14 16:13:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.085693 | Val Loss: 0.092251 | Grad Norm: 0.04 | LR: 4.61e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 16:13:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.082773 | Val Loss: 0.091493 | Grad Norm: 0.03 | LR: 4.49e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 16:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-14 16:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-14 16:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-14 16:14:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.138012 | Val Loss: 0.159737 | Grad Norm: 0.76 | LR: 5.67e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 16:14:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.132172 | Val Loss: 0.156057 | Grad Norm: 0.51 | LR: 2.11e-05 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 16:15:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.129722 | Val Loss: 0.154784 | Grad Norm: 0.41 | LR: 3.75e-06 | Time: 20.9s\n",
      "\u001b[32m[2025-10-14 16:15:12 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_4.0bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 16:15:12 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_4.0bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-14 16:15:12 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2176.32s (36.27min)\n",
      "\u001b[32m[2025-10-14 16:15:12 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-14 16:15:17 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_4.0bit/model\n",
      "\u001b[32m[2025-10-14 16:15:17 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 141/141 [01:49<00:00,  1.29it/s]\n",
      "wikitext2:9.19449234008789\n",
      "\u001b[32m[2025-10-14 16:17:11 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 9.19\n",
      "\u001b[32m[2025-10-14 16:17:12 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-14 16:17:13 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-14 16:17:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 170386.46it/s]\n",
      "\u001b[32m[2025-10-14 16:17:24 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5292.69it/s]\n",
      "\u001b[32m[2025-10-14 16:17:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 1987.68it/s]\n",
      "\u001b[32m[2025-10-14 16:17:27 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:00<00:00, 2244.77it/s]\n",
      "\u001b[32m[2025-10-14 16:17:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:47<00:00, 33.51it/s]\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7348|±  |0.0124|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5711|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7534|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7635|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7588|±  |0.0088|\n",
      "|piqa      |      1|none  |     0|acc     |0.7671|±  |0.0099|\n",
      "|          |       |none  |     0|acc_norm|0.7753|±  |0.0097|\n",
      "\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 70.91%\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_4.0bit/results.json\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-14 16:45:33 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy adaptive \\\n",
    "         --target_avg_bits 4.0 \\\n",
    "         --use_adaptive_training \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_4.0bit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_4.0bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0xg9z38aW4D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4922260,
     "status": "ok",
     "timestamp": 1761827439801,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "h0xg9z38aW4D",
    "outputId": "71efe0b8-d6ff-4140-c041-c9e2c64f8a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '4', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--output_dir', './output/llama3_baseline4', '--save_quant_dir', './output/llama3_baseline4/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 11:08:41 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_baseline4', save_quant_dir='./output/llama3_baseline4/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-30 11:08:41 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.02it/s]\n",
      "\u001b[32m[2025-10-30 11:08:47 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-30 11:08:47 root]\u001b[0m\u001b[33m(main_block_ap.py 170)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-30 11:08:47 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-30 11:08:47 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-30 11:08:49 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-30 11:08:51 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:09:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:1.981732793865376e-06 val_loss:1.6575758081671665e-06 quant_lr:5.1362214303938806e-05 norm:0.00025109 max memory_allocated 7374.298828125 time 25.63761830329895 \n",
      "\u001b[32m[2025-10-30 11:09:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:1.5027377457954572e-06 val_loss:1.510487663836102e-06 quant_lr:5e-06 norm:0.00011500 max memory_allocated 7438.173828125 time 25.744585514068604 \n",
      "\u001b[32m[2025-10-30 11:09:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:09:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:09:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:09:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:09:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:10:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:10:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:10:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:10:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:4.587268995237537e-05 val_loss:3.0554299883078784e-05 quant_lr:5.1362214303938806e-05 norm:0.01196632 max memory_allocated 7438.173828125 time 25.57684850692749 \n",
      "\u001b[32m[2025-10-30 11:11:00 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:6.451818626374006e-05 val_loss:2.3416880139848217e-05 quant_lr:5e-06 norm:0.01760308 max memory_allocated 7438.173828125 time 25.670873641967773 \n",
      "\u001b[32m[2025-10-30 11:11:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:11:16 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:11:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:3.609288978623226e-05 val_loss:3.335358269396238e-05 quant_lr:5.1362214303938806e-05 norm:0.00015647 max memory_allocated 7438.173828125 time 25.67678213119507 \n",
      "\u001b[32m[2025-10-30 11:12:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:3.461520100245252e-05 val_loss:3.283370824647136e-05 quant_lr:5e-06 norm:0.00008969 max memory_allocated 7438.173828125 time 25.697978019714355 \n",
      "\u001b[32m[2025-10-30 11:12:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:12:29 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:12:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:5.535692980629392e-05 val_loss:5.3858304454479367e-05 quant_lr:5.1362214303938806e-05 norm:0.00028020 max memory_allocated 7438.173828125 time 25.696818113327026 \n",
      "\u001b[32m[2025-10-30 11:13:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:5.2982686611358076e-05 val_loss:5.2833318477496505e-05 quant_lr:5e-06 norm:0.00020211 max memory_allocated 7439.173828125 time 25.723067045211792 \n",
      "\u001b[32m[2025-10-30 11:13:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:13:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:14:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:8.056261867750436e-05 val_loss:8.057690865825862e-05 quant_lr:5.1362214303938806e-05 norm:0.00064286 max memory_allocated 7439.173828125 time 25.688210487365723 \n",
      "\u001b[32m[2025-10-30 11:14:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:7.598911906825379e-05 val_loss:7.841009210096672e-05 quant_lr:5e-06 norm:0.00043120 max memory_allocated 7439.173828125 time 25.759726524353027 \n",
      "\u001b[32m[2025-10-30 11:14:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:14:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:15:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.00011254732817178592 val_loss:0.00011439365334808826 quant_lr:5.1362214303938806e-05 norm:0.00057141 max memory_allocated 7439.173828125 time 25.69892454147339 \n",
      "\u001b[32m[2025-10-30 11:15:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.00010576875502010807 val_loss:0.00011119841656181961 quant_lr:5e-06 norm:0.00042814 max memory_allocated 7439.173828125 time 25.81067943572998 \n",
      "\u001b[32m[2025-10-30 11:15:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:15:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:15:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:15:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:16:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:16:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:16:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:16:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:16:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0001488212146796286 val_loss:0.00015398100367747247 quant_lr:5.1362214303938806e-05 norm:0.00114214 max memory_allocated 7439.173828125 time 25.67204713821411 \n",
      "\u001b[32m[2025-10-30 11:17:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0001389126555295661 val_loss:0.00014956199447624385 quant_lr:5e-06 norm:0.00070332 max memory_allocated 7439.173828125 time 25.741313457489014 \n",
      "\u001b[32m[2025-10-30 11:17:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:17:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:17:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0001900281204143539 val_loss:0.00020237371791154146 quant_lr:5.1362214303938806e-05 norm:0.00099231 max memory_allocated 7439.173828125 time 25.692681312561035 \n",
      "\u001b[32m[2025-10-30 11:18:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0001783428160706535 val_loss:0.00019659119425341487 quant_lr:5e-06 norm:0.00077237 max memory_allocated 7439.173828125 time 25.70386552810669 \n",
      "\u001b[32m[2025-10-30 11:18:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:18:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:19:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.00022956322936806828 val_loss:0.0002492298081051558 quant_lr:5.1362214303938806e-05 norm:0.00096643 max memory_allocated 7439.173828125 time 25.657220363616943 \n",
      "\u001b[32m[2025-10-30 11:19:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.00021574943093582988 val_loss:0.00024258326448034495 quant_lr:5e-06 norm:0.00079058 max memory_allocated 7439.173828125 time 25.668999195098877 \n",
      "\u001b[32m[2025-10-30 11:19:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:19:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:20:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.00027542252792045474 val_loss:0.0003070257371291518 quant_lr:5.1362214303938806e-05 norm:0.00123256 max memory_allocated 7439.173828125 time 25.70979642868042 \n",
      "\u001b[32m[2025-10-30 11:20:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.00025942703359760344 val_loss:0.00029859173810109496 quant_lr:5e-06 norm:0.00111406 max memory_allocated 7439.173828125 time 25.73723816871643 \n",
      "\u001b[32m[2025-10-30 11:20:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:20:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:20:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:20:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:21:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:21:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:21:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:21:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:21:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.000317634257953614 val_loss:0.00036179684684611857 quant_lr:5.1362214303938806e-05 norm:0.00148403 max memory_allocated 7439.173828125 time 25.673840045928955 \n",
      "\u001b[32m[2025-10-30 11:22:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.00029863216332159936 val_loss:0.0003506271168589592 quant_lr:5e-06 norm:0.00110583 max memory_allocated 7439.173828125 time 25.725643157958984 \n",
      "\u001b[32m[2025-10-30 11:22:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:22:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:22:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.00035803118953481317 val_loss:0.0004122760728932917 quant_lr:5.1362214303938806e-05 norm:0.00102718 max memory_allocated 7439.173828125 time 25.620840311050415 \n",
      "\u001b[32m[2025-10-30 11:23:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.00033870022161863744 val_loss:0.00040319646359421313 quant_lr:5e-06 norm:0.00086992 max memory_allocated 7439.173828125 time 25.708523511886597 \n",
      "\u001b[32m[2025-10-30 11:23:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:23:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:24:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.00038531635073013604 val_loss:0.0004496053734328598 quant_lr:5.1362214303938806e-05 norm:0.00143729 max memory_allocated 7439.173828125 time 25.665858507156372 \n",
      "\u001b[32m[2025-10-30 11:24:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0003643740783445537 val_loss:0.0004400070756673813 quant_lr:5e-06 norm:0.00103373 max memory_allocated 7439.173828125 time 25.692618131637573 \n",
      "\u001b[32m[2025-10-30 11:24:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:24:49 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:25:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.0004464127996470779 val_loss:0.0005253003910183907 quant_lr:5.1362214303938806e-05 norm:0.00156554 max memory_allocated 7439.173828125 time 25.76821231842041 \n",
      "\u001b[32m[2025-10-30 11:25:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.00041892126318998635 val_loss:0.0005116548272781074 quant_lr:5e-06 norm:0.00124223 max memory_allocated 7439.173828125 time 25.826372623443604 \n",
      "\u001b[32m[2025-10-30 11:25:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:25:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:25:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:25:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:25:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:26:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:26:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:26:04 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:26:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.0004968729917891324 val_loss:0.0005907212616875768 quant_lr:5.1362214303938806e-05 norm:0.00161837 max memory_allocated 7439.173828125 time 25.7078640460968 \n",
      "\u001b[32m[2025-10-30 11:27:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.0004661561397369951 val_loss:0.0005766434478573501 quant_lr:5e-06 norm:0.00108704 max memory_allocated 7439.173828125 time 25.797480821609497 \n",
      "\u001b[32m[2025-10-30 11:27:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:27:18 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:27:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.0005529032787308097 val_loss:0.0006690124282613397 quant_lr:5.1362214303938806e-05 norm:0.00141242 max memory_allocated 7439.173828125 time 25.734951496124268 \n",
      "\u001b[32m[2025-10-30 11:28:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0005237537552602589 val_loss:0.0006553782150149345 quant_lr:5e-06 norm:0.00137646 max memory_allocated 7439.173828125 time 25.795101404190063 \n",
      "\u001b[32m[2025-10-30 11:28:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:28:33 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:29:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.0006226545083336532 val_loss:0.0007688235491514206 quant_lr:5.1362214303938806e-05 norm:0.00210792 max memory_allocated 7439.173828125 time 25.729522705078125 \n",
      "\u001b[32m[2025-10-30 11:29:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.0005873119807802141 val_loss:0.0007518398342654109 quant_lr:5e-06 norm:0.00172122 max memory_allocated 7439.173828125 time 25.837454557418823 \n",
      "\u001b[32m[2025-10-30 11:29:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:29:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:30:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.0007214060169644654 val_loss:0.0009114836575463414 quant_lr:5.1362214303938806e-05 norm:0.00248834 max memory_allocated 7439.173828125 time 25.72218370437622 \n",
      "\u001b[32m[2025-10-30 11:30:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.0006863692542538047 val_loss:0.0008937814855016768 quant_lr:5e-06 norm:0.00258174 max memory_allocated 7439.673828125 time 25.775399684906006 \n",
      "\u001b[32m[2025-10-30 11:30:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:30:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:30:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:30:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:30:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:30:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:31:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:31:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:31:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.0008269018144346774 val_loss:0.0010716498363763094 quant_lr:5.1362214303938806e-05 norm:0.00194764 max memory_allocated 7439.673828125 time 25.694323778152466 \n",
      "\u001b[32m[2025-10-30 11:31:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.0007879778277128935 val_loss:0.0010565471602603793 quant_lr:5e-06 norm:0.00168189 max memory_allocated 7439.673828125 time 25.750279426574707 \n",
      "\u001b[32m[2025-10-30 11:32:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:32:16 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:32:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.0009524807101115584 val_loss:0.0012666585389524698 quant_lr:5.1362214303938806e-05 norm:0.00209985 max memory_allocated 7439.673828125 time 25.686856508255005 \n",
      "\u001b[32m[2025-10-30 11:33:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.000913651951123029 val_loss:0.0012511011445894837 quant_lr:5e-06 norm:0.00193309 max memory_allocated 7439.673828125 time 25.7358660697937 \n",
      "\u001b[32m[2025-10-30 11:33:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:33:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:34:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.0011156561085954309 val_loss:0.0015111304819583893 quant_lr:5.1362214303938806e-05 norm:0.00179916 max memory_allocated 7439.673828125 time 25.67287015914917 \n",
      "\u001b[32m[2025-10-30 11:34:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.0010738966520875692 val_loss:0.0014951354824006557 quant_lr:5e-06 norm:0.00172453 max memory_allocated 7439.673828125 time 25.730773448944092 \n",
      "\u001b[32m[2025-10-30 11:34:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:34:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:35:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.0013877471210435033 val_loss:0.0019201587419956923 quant_lr:5.1362214303938806e-05 norm:0.00228072 max memory_allocated 7439.673828125 time 25.642194271087646 \n",
      "\u001b[32m[2025-10-30 11:35:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.0013335845433175564 val_loss:0.0018982943147420883 quant_lr:5e-06 norm:0.00197253 max memory_allocated 7439.673828125 time 25.717517852783203 \n",
      "\u001b[32m[2025-10-30 11:35:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:35:59 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:36:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.0016604306874796748 val_loss:0.002360319485887885 quant_lr:5.1362214303938806e-05 norm:0.00209328 max memory_allocated 7439.673828125 time 25.685811519622803 \n",
      "\u001b[32m[2025-10-30 11:36:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.0016019432805478573 val_loss:0.0023373500443995 quant_lr:5e-06 norm:0.00181572 max memory_allocated 7439.673828125 time 25.70293092727661 \n",
      "\u001b[32m[2025-10-30 11:37:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:37:13 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:37:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.001989874290302396 val_loss:0.002917979611083865 quant_lr:5.1362214303938806e-05 norm:0.00183324 max memory_allocated 7439.673828125 time 25.654018878936768 \n",
      "\u001b[32m[2025-10-30 11:38:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.0019264094298705459 val_loss:0.0029000632930547 quant_lr:5e-06 norm:0.00165270 max memory_allocated 7439.673828125 time 25.698977947235107 \n",
      "\u001b[32m[2025-10-30 11:38:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:38:27 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:38:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.0023934757336974144 val_loss:0.0035944338887929916 quant_lr:5.1362214303938806e-05 norm:0.00178482 max memory_allocated 7439.673828125 time 25.636497735977173 \n",
      "\u001b[32m[2025-10-30 11:39:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.002321559702977538 val_loss:0.0035743527114391327 quant_lr:5e-06 norm:0.00161961 max memory_allocated 7439.673828125 time 25.71507501602173 \n",
      "\u001b[32m[2025-10-30 11:39:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:39:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:40:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.002920126309618354 val_loss:0.004495357628911734 quant_lr:5.1362214303938806e-05 norm:0.00303302 max memory_allocated 7439.673828125 time 25.663705110549927 \n",
      "\u001b[32m[2025-10-30 11:40:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.0028319519478827715 val_loss:0.004472633358091116 quant_lr:5e-06 norm:0.00238910 max memory_allocated 7439.673828125 time 25.736069679260254 \n",
      "\u001b[32m[2025-10-30 11:40:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:40:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:41:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.0035962285473942757 val_loss:0.005697925109416246 quant_lr:5.1362214303938806e-05 norm:0.00299627 max memory_allocated 7439.673828125 time 25.68839955329895 \n",
      "\u001b[32m[2025-10-30 11:41:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.003487988142296672 val_loss:0.005662913434207439 quant_lr:5e-06 norm:0.00256579 max memory_allocated 7439.673828125 time 25.679794311523438 \n",
      "\u001b[32m[2025-10-30 11:41:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:41:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:41:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:41:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:42:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:42:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:42:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:42:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:42:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.004500101320445538 val_loss:0.007265838328748941 quant_lr:5.1362214303938806e-05 norm:0.00420931 max memory_allocated 7439.673828125 time 25.702921152114868 \n",
      "\u001b[32m[2025-10-30 11:43:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.004356181714683771 val_loss:0.007217054255306721 quant_lr:5e-06 norm:0.00330868 max memory_allocated 7439.673828125 time 25.7183678150177 \n",
      "\u001b[32m[2025-10-30 11:43:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:43:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:43:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.005699719302356243 val_loss:0.009481893852353096 quant_lr:5.1362214303938806e-05 norm:0.00421414 max memory_allocated 7439.673828125 time 25.671162605285645 \n",
      "\u001b[32m[2025-10-30 11:44:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.005516873672604561 val_loss:0.00940869003534317 quant_lr:5e-06 norm:0.00362134 max memory_allocated 7439.673828125 time 25.75353217124939 \n",
      "\u001b[32m[2025-10-30 11:44:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:44:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:45:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.007410102523863316 val_loss:0.012523160316050053 quant_lr:5.1362214303938806e-05 norm:0.01065620 max memory_allocated 7439.673828125 time 25.69780993461609 \n",
      "\u001b[32m[2025-10-30 11:45:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.007149455603212118 val_loss:0.01243138499557972 quant_lr:5e-06 norm:0.00775211 max memory_allocated 7439.673828125 time 25.735238552093506 \n",
      "\u001b[32m[2025-10-30 11:45:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:45:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:46:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.010358988307416439 val_loss:0.017542198300361633 quant_lr:5.1362214303938806e-05 norm:0.01883901 max memory_allocated 7439.673828125 time 25.690040826797485 \n",
      "\u001b[32m[2025-10-30 11:46:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.009903131052851677 val_loss:0.01736512966454029 quant_lr:5e-06 norm:0.01392423 max memory_allocated 7439.673828125 time 25.755459547042847 \n",
      "\u001b[32m[2025-10-30 11:46:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:46:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:46:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:46:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:47:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:47:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:47:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:47:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 11:47:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.021895881742239 val_loss:0.03370179608464241 quant_lr:5.1362214303938806e-05 norm:0.11460203 max memory_allocated 7439.673828125 time 25.70626997947693 \n",
      "\u001b[32m[2025-10-30 11:48:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.020009571686387062 val_loss:0.03283139318227768 quant_lr:5e-06 norm:0.08893299 max memory_allocated 7439.673828125 time 25.773417472839355 \n",
      "\u001b[32m[2025-10-30 11:48:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 11:48:23 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2376.015201330185\n",
      "\u001b[32m[2025-10-30 11:48:23 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-30 11:48:40 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 141/141 [02:27<00:00,  1.04s/it]\n",
      "wikitext2:6.427642345428467\n",
      "\u001b[32m[2025-10-30 11:51:19 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 6.43\n",
      "2025-10-30 11:51:19.496756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761825079.518787   53935 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761825079.525123   53935 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761825079.543760   53935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761825079.543786   53935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761825079.543789   53935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761825079.543793   53935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 11:51:24 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 11:51:25 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 11:51:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 129013.21it/s]\n",
      "\u001b[32m[2025-10-30 11:51:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2390.38it/s]\n",
      "\u001b[32m[2025-10-30 11:52:02 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1038.46it/s]\n",
      "\u001b[32m[2025-10-30 11:52:04 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1009.98it/s]\n",
      "\u001b[32m[2025-10-30 11:52:06 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:50<00:00, 24.61it/s]\n",
      "\u001b[32m[2025-10-30 12:30:36 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7316|±  |0.0125|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5918|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7770|±  |0.0042|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8035|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7811|±  |0.0085|\n",
      "|piqa      |      1|none  |     0|acc     |0.7949|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8003|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-30 12:30:36 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 73.05%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "     --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 4 \\\n",
    "       --group_size 128 \\\n",
    "        --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "       --output_dir ./output/llama3_baseline4 \\\n",
    "       --save_quant_dir ./output/llama3_baseline4/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sz5juZ3JzihZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5184045,
     "status": "ok",
     "timestamp": 1761833143487,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "Sz5juZ3JzihZ",
    "outputId": "be24bc39-8674-4b85-dd89-ba42a9da0278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_mixed_precision', '--mpq_strategy', 'conservative', '--target_avg_bits', '4.0', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama3_optimized_mpq4bit', '--save_quant_dir', './output/llama3_optimized_mpq4bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande,arc_challenge']\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_mpq4bit', save_quant_dir='./output/llama3_optimized_mpq4bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande,arc_challenge', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=4.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 12:39:23 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.03it/s]\n",
      "\u001b[32m[2025-10-30 12:39:29 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-30 12:39:29 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_train.cache\n",
      "\u001b[32m[2025-10-30 12:39:29 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_val.cache\n",
      "\u001b[32m[2025-10-30 12:39:29 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 12:39:30 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [6, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5]\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 12:39:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-30 12:40:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.0s\n",
      "\u001b[32m[2025-10-30 12:40:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-30 12:40:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 12:40:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-30 12:41:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000062 | Val Loss: 0.000050 | Grad Norm: 0.10 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:41:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000080 | Val Loss: 0.000017 | Grad Norm: 0.09 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:42:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 12:42:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 12:42:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000029 | Val Loss: 0.000022 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:43:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000027 | Val Loss: 0.000022 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:43:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 12:43:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 12:43:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000037 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:44:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000035 | Val Loss: 0.000030 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:44:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 12:44:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 12:45:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000048 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:45:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000044 | Val Loss: 0.000039 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:45:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 12:45:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 12:46:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000059 | Val Loss: 0.000053 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:46:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000055 | Val Loss: 0.000051 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 12:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 12:47:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000072 | Val Loss: 0.000067 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:48:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000066 | Val Loss: 0.000065 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:48:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 12:48:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-30 12:49:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000087 | Val Loss: 0.000086 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:49:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000082 | Val Loss: 0.000083 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:49:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 12:49:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-30 12:50:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000104 | Val Loss: 0.000106 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:50:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000098 | Val Loss: 0.000103 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:51:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 12:51:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:51:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000173 | Val Loss: 0.000176 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000158 | Val Loss: 0.000168 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:52:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 12:52:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000224 | Val Loss: 0.000235 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:53:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000206 | Val Loss: 0.000226 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 12:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:54:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000275 | Val Loss: 0.000293 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:54:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000256 | Val Loss: 0.000284 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:55:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 12:55:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:55:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000312 | Val Loss: 0.000336 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:56:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000292 | Val Loss: 0.000327 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:56:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 12:56:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:56:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000380 | Val Loss: 0.000412 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:57:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000353 | Val Loss: 0.000400 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:57:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 12:57:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:58:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000439 | Val Loss: 0.000482 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:58:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000408 | Val Loss: 0.000469 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 12:58:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 12:58:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 12:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000502 | Val Loss: 0.000561 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 12:59:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000470 | Val Loss: 0.000548 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:00:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 13:00:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 13:00:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000577 | Val Loss: 0.000658 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 13:01:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000540 | Val Loss: 0.000638 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:01:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 13:01:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 13:02:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000676 | Val Loss: 0.000780 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 13:02:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000637 | Val Loss: 0.000764 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:02:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 13:02:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 13:03:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000780 | Val Loss: 0.000918 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000741 | Val Loss: 0.000902 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:04:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 13:04:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:04:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001291 | Val Loss: 0.001395 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:05:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001169 | Val Loss: 0.001367 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:05:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 13:05:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:06:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001788 | Val Loss: 0.001918 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:06:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001635 | Val Loss: 0.001886 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:06:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 13:06:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:07:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.002492 | Val Loss: 0.002688 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:07:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.002284 | Val Loss: 0.002641 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 13:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:08:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.003197 | Val Loss: 0.003497 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:09:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002970 | Val Loss: 0.003453 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:09:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 13:09:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:09:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.004055 | Val Loss: 0.004504 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:10:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.003801 | Val Loss: 0.004457 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:10:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 13:10:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:11:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.005112 | Val Loss: 0.005742 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:11:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.004821 | Val Loss: 0.005686 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 13:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:12:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.006512 | Val Loss: 0.007372 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.006154 | Val Loss: 0.007303 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:13:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 13:13:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.008341 | Val Loss: 0.009508 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.007890 | Val Loss: 0.009422 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:14:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 13:14:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.010804 | Val Loss: 0.012395 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.010217 | Val Loss: 0.012273 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:16:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 13:16:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:16:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.014182 | Val Loss: 0.016368 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 13:17:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.013432 | Val Loss: 0.016198 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:17:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 13:17:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 13:17:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.019152 | Val Loss: 0.022158 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 13:18:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.018061 | Val Loss: 0.021887 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:18:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 13:18:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-30 13:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.024251 | Val Loss: 0.029338 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:19:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.023365 | Val Loss: 0.029059 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:19:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 13:19:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-30 13:20:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.041354 | Val Loss: 0.053539 | Grad Norm: 0.21 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 13:20:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.039592 | Val Loss: 0.052211 | Grad Norm: 0.15 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 13:21:14 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_mpq4bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 13:21:14 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_mpq4bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 13:21:14 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2505.37s (41.76min)\n",
      "\u001b[32m[2025-10-30 13:21:14 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 13:21:33 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_mpq4bit/model\n",
      "\u001b[32m[2025-10-30 13:21:33 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:28<00:00,  1.05s/it]\n",
      "wikitext2:6.737231254577637\n",
      "\u001b[32m[2025-10-30 13:24:13 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.74\n",
      "2025-10-30 13:24:13.813209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761830653.835863   76337 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761830653.842376   76337 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761830653.861156   76337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761830653.861182   76337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761830653.861187   76337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761830653.861190   76337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 13:24:18 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 13:24:20 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 13:24:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 13:24:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_challenge on rank 0...\n",
      "100% 1172/1172 [00:01<00:00, 1044.46it/s]\n",
      "\u001b[32m[2025-10-30 13:24:53 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 3355.00it/s]\n",
      "\u001b[32m[2025-10-30 13:24:53 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2359.01it/s]\n",
      "\u001b[32m[2025-10-30 13:24:59 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1036.97it/s]\n",
      "\u001b[32m[2025-10-30 13:25:01 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 994.26it/s]\n",
      "\u001b[32m[2025-10-30 13:25:03 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 60566/60566 [39:53<00:00, 25.31it/s]\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     0|acc     |0.4650|±  |0.0146|\n",
      "|             |       |none  |     0|acc_norm|0.5119|±  |0.0146|\n",
      "|winogrande   |      1|none  |     0|acc     |0.7435|±  |0.0123|\n",
      "|hellaswag    |      1|none  |     0|acc     |0.5887|±  |0.0049|\n",
      "|             |       |none  |     0|acc_norm|0.7723|±  |0.0042|\n",
      "|arc_easy     |      1|none  |     0|acc     |0.7753|±  |0.0086|\n",
      "|             |       |none  |     0|acc_norm|0.7500|±  |0.0089|\n",
      "|piqa         |      1|none  |     0|acc     |0.7878|±  |0.0095|\n",
      "|             |       |none  |     0|acc_norm|0.8003|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 67.21%\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_mpq4bit/results.json\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 14:05:40 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy conservative \\\n",
    "         --target_avg_bits 4.0 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_mpq4bit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_mpq4bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande,arc_challenge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a40913",
   "metadata": {},
   "source": [
    "73.3825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5a1d",
   "metadata": {},
   "source": [
    "# 3 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b068e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 598106,
     "status": "ok",
     "timestamp": 1761815449501,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "e6b068e4",
    "outputId": "2c93c26e-a2dd-4bd8-9c51-c86067e39e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '3', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama3_optimized_4.5bit', '--save_quant_dir', './output/llama3_optimized_4.5bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_4.5bit', save_quant_dir='./output/llama3_optimized_4.5bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=3.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 07:45:36 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 654/654 [00:00<00:00, 4.42MB/s]\n",
      "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 23.8MB/s]\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:01<00:00, 6.02MB/s]\n",
      "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 582kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 99.6MB/s]\n",
      "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1% 44.8M/4.98G [00:02<03:51, 21.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6% 288M/4.98G [00:02<00:36, 127MB/s]  \u001b[A\n",
      "model-00001-of-00004.safetensors:  25% 1.23G/4.98G [00:02<00:05, 700MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [00:05<00:10, 327MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [00:05<00:08, 375MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44% 2.18G/4.98G [00:06<00:06, 429MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [00:07<00:07, 349MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50% 2.49G/4.98G [00:07<00:07, 355MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [00:09<00:12, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54% 2.71G/4.98G [00:09<00:11, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62% 3.11G/4.98G [00:09<00:04, 382MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66% 3.30G/4.98G [00:10<00:03, 450MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [00:10<00:03, 488MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73% 3.61G/4.98G [00:10<00:02, 481MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76% 3.76G/4.98G [00:10<00:02, 507MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78% 3.86G/4.98G [00:11<00:02, 475MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79% 3.95G/4.98G [00:11<00:02, 451MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81% 4.02G/4.98G [00:13<00:07, 124MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85% 4.25G/4.98G [00:13<00:03, 216MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89% 4.43G/4.98G [00:14<00:01, 299MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93% 4.63G/4.98G [00:14<00:00, 425MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:14<00:00, 345MB/s]\n",
      "Downloading shards:  25% 1/4 [00:14<00:44, 14.96s/it]\n",
      "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0% 31.4k/5.00G [00:01<61:51:19, 22.5kB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9% 452M/5.00G [00:02<00:16, 273MB/s]     \u001b[A\n",
      "model-00002-of-00004.safetensors:  23% 1.15G/5.00G [00:02<00:04, 773MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28% 1.38G/5.00G [00:02<00:06, 573MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31% 1.56G/5.00G [00:05<00:13, 247MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:05<00:11, 283MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [00:06<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46% 2.29G/5.00G [00:06<00:05, 498MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50% 2.52G/5.00G [00:06<00:04, 508MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55% 2.73G/5.00G [00:06<00:04, 554MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57% 2.86G/5.00G [00:07<00:03, 566MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60% 3.01G/5.00G [00:07<00:03, 604MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62% 3.11G/5.00G [00:07<00:03, 509MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64% 3.19G/5.00G [00:07<00:03, 490MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65% 3.26G/5.00G [00:07<00:03, 476MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66% 3.32G/5.00G [00:08<00:03, 453MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69% 3.42G/5.00G [00:08<00:04, 391MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70% 3.49G/5.00G [00:08<00:03, 391MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71% 3.56G/5.00G [00:08<00:03, 384MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73% 3.63G/5.00G [00:08<00:03, 385MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74% 3.71G/5.00G [00:09<00:03, 381MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75% 3.76G/5.00G [00:11<00:13, 93.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78% 3.89G/5.00G [00:11<00:07, 158MB/s] \u001b[A\n",
      "model-00002-of-00004.safetensors:  82% 4.08G/5.00G [00:11<00:03, 272MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86% 4.30G/5.00G [00:11<00:01, 426MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88% 4.41G/5.00G [00:11<00:01, 471MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93% 4.66G/5.00G [00:11<00:00, 728MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [00:12<00:00, 662MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:12<00:00, 401MB/s]\n",
      "Downloading shards:  50% 2/4 [00:27<00:27, 13.79s/it]\n",
      "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0% 795k/4.92G [00:01<2:27:49, 554kB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5% 236M/4.92G [00:02<00:39, 118MB/s]  \u001b[A\n",
      "model-00003-of-00004.safetensors:   9% 437M/4.92G [00:02<00:18, 242MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12% 611M/4.92G [00:02<00:12, 356MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16% 770M/4.92G [00:02<00:08, 475MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18% 908M/4.92G [00:02<00:07, 540MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21% 1.01G/4.92G [00:03<00:06, 595MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23% 1.14G/4.92G [00:03<00:05, 666MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25% 1.24G/4.92G [00:03<00:09, 386MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31% 1.55G/4.92G [00:03<00:04, 690MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34% 1.68G/4.92G [00:04<00:04, 699MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38% 1.86G/4.92G [00:06<00:16, 183MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42% 2.04G/4.92G [00:06<00:11, 259MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45% 2.21G/4.92G [00:06<00:08, 335MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48% 2.35G/4.92G [00:06<00:06, 419MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51% 2.52G/4.92G [00:07<00:05, 453MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58% 2.83G/4.92G [00:07<00:03, 690MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60% 2.96G/4.92G [00:07<00:02, 676MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63% 3.10G/4.92G [00:08<00:03, 561MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65% 3.20G/4.92G [00:08<00:02, 580MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67% 3.29G/4.92G [00:08<00:03, 528MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69% 3.40G/4.92G [00:08<00:03, 457MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70% 3.47G/4.92G [00:08<00:03, 463MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72% 3.53G/4.92G [00:09<00:03, 437MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73% 3.60G/4.92G [00:09<00:03, 422MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75% 3.67G/4.92G [00:09<00:02, 418MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76% 3.73G/4.92G [00:09<00:03, 381MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77% 3.80G/4.92G [00:09<00:03, 355MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79% 3.86G/4.92G [00:10<00:03, 326MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80% 3.93G/4.92G [00:10<00:03, 318MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82% 4.03G/4.92G [00:10<00:03, 286MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85% 4.16G/4.92G [00:10<00:01, 422MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86% 4.23G/4.92G [00:10<00:01, 421MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87% 4.30G/4.92G [00:11<00:01, 405MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89% 4.36G/4.92G [00:11<00:01, 380MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90% 4.42G/4.92G [00:11<00:01, 381MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93% 4.55G/4.92G [00:11<00:00, 422MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94% 4.62G/4.92G [00:11<00:00, 411MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95% 4.68G/4.92G [00:12<00:00, 406MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97% 4.75G/4.92G [00:12<00:00, 400MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:12<00:00, 389MB/s]\n",
      "Downloading shards:  75% 3/4 [00:41<00:13, 13.47s/it]\n",
      "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3% 30.1M/1.17G [00:01<00:49, 23.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8% 97.1M/1.17G [00:01<00:13, 76.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37% 432M/1.17G [00:01<00:01, 437MB/s]  \u001b[A\n",
      "model-00004-of-00004.safetensors:  54% 634M/1.17G [00:01<00:00, 615MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72% 843M/1.17G [00:01<00:00, 782MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85% 993M/1.17G [00:02<00:00, 754MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:02<00:00, 443MB/s]\n",
      "Downloading shards: 100% 4/4 [00:44<00:00, 11.04s/it]\n",
      "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n",
      "generation_config.json: 100% 177/177 [00:00<00:00, 1.64MB/s]\n",
      "\u001b[32m[2025-10-30 07:46:32 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 29.7MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 767kB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:00<00:00, 12.8MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 1.35MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 87343.34 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 707356.91 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 582929.81 examples/s]\n",
      "\u001b[32m[2025-10-30 07:46:57 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 07:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 07:47:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:47:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000008 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 26.9s\n",
      "\u001b[32m[2025-10-30 07:48:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-30 07:48:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 07:48:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 07:48:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000057 | Val Loss: 0.000056 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:49:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000103 | Val Loss: 0.000033 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 27.4s\n",
      "\u001b[32m[2025-10-30 07:49:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 07:49:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:50:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000072 | Val Loss: 0.000063 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:50:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000066 | Val Loss: 0.000061 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 07:50:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 07:50:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:51:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000134 | Val Loss: 0.000125 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:51:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000123 | Val Loss: 0.000122 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 07:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:52:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000217 | Val Loss: 0.000208 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:53:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000198 | Val Loss: 0.000203 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 07:53:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 07:53:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:54:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000326 | Val Loss: 0.000319 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:54:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000297 | Val Loss: 0.000312 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 07:54:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 07:54:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 07:55:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000443 | Val Loss: 0.000445 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:55:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000406 | Val Loss: 0.000435 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:56:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 07:56:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 07:56:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000616 | Val Loss: 0.000618 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:57:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000559 | Val Loss: 0.000604 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:57:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 07:57:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 07:57:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000771 | Val Loss: 0.000780 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:58:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000700 | Val Loss: 0.000764 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:58:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 07:58:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 07:59:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000946 | Val Loss: 0.000973 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:59:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000860 | Val Loss: 0.000953 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 07:59:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 07:59:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:00:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.001094 | Val Loss: 0.001148 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 08:00:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.001005 | Val Loss: 0.001127 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:01:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 08:01:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:01:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.001255 | Val Loss: 0.001325 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:02:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.001156 | Val Loss: 0.001302 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:02:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 08:02:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:03:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.001365 | Val Loss: 0.001449 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 08:03:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.001260 | Val Loss: 0.001426 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:03:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 08:03:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:04:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.001601 | Val Loss: 0.001704 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.1s\n",
      "\u001b[32m[2025-10-30 08:04:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.001466 | Val Loss: 0.001671 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:05:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 08:05:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:05:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.001811 | Val Loss: 0.001935 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:06:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.001660 | Val Loss: 0.001900 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:06:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 08:06:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:07:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.002043 | Val Loss: 0.002214 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:07:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.001884 | Val Loss: 0.002175 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:07:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 08:07:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:08:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.002313 | Val Loss: 0.002538 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:08:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.002144 | Val Loss: 0.002498 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:09:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 08:09:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:09:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.002705 | Val Loss: 0.003016 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:10:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.002523 | Val Loss: 0.002974 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:10:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 08:10:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:11:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.003125 | Val Loss: 0.003538 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:11:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.002949 | Val Loss: 0.003502 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:11:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 08:11:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:12:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.003632 | Val Loss: 0.004158 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:12:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.003447 | Val Loss: 0.004120 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 08:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:13:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.004298 | Val Loss: 0.004942 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:14:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.004094 | Val Loss: 0.004905 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:14:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 08:14:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:14:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.005372 | Val Loss: 0.006255 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:15:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.005112 | Val Loss: 0.006203 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:15:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 08:15:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:16:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.006455 | Val Loss: 0.007621 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:16:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.006181 | Val Loss: 0.007571 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:16:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 08:16:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:17:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.007783 | Val Loss: 0.009314 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:17:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.007479 | Val Loss: 0.009265 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:18:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 08:18:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:18:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.009368 | Val Loss: 0.011302 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:19:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.009027 | Val Loss: 0.011244 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:19:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 08:19:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:20:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.011435 | Val Loss: 0.013881 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:20:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.011021 | Val Loss: 0.013800 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:20:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 08:20:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:21:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.014057 | Val Loss: 0.017165 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:21:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.013543 | Val Loss: 0.017061 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:22:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 08:22:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:22:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.017493 | Val Loss: 0.021416 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:23:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.016841 | Val Loss: 0.021278 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:23:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 08:23:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:24:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.022116 | Val Loss: 0.027167 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.021279 | Val Loss: 0.026984 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:24:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 08:24:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:25:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.028680 | Val Loss: 0.035156 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:25:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.027493 | Val Loss: 0.034877 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:26:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 08:26:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 08:26:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.040415 | Val Loss: 0.049042 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:27:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.038192 | Val Loss: 0.048475 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:27:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 08:27:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 08:27:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.083914 | Val Loss: 0.101757 | Grad Norm: 0.13 | LR: 5.14e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-30 08:28:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.075889 | Val Loss: 0.099125 | Grad Norm: 0.09 | LR: 5.00e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-30 08:28:44 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_4.5bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 08:28:44 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_4.5bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 08:28:44 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2532.50s (42.21min)\n",
      "\u001b[32m[2025-10-30 08:28:44 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 08:29:00 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_4.5bit/model\n",
      "\u001b[32m[2025-10-30 08:29:00 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:32<00:00,  1.08s/it]\n",
      "wikitext2:7.585897445678711\n",
      "\u001b[32m[2025-10-30 08:31:42 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.59\n",
      "2025-10-30 08:31:44.155206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761813104.442235    2830 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761813104.525527    2830 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761813105.108219    2830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761813105.108256    2830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761813105.108261    2830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761813105.108265    2830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 17.2MB/s]\n",
      "\u001b[32m[2025-10-30 08:31:51 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 08:31:52 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 08:31:57 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 20.9MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 28.5MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:01<00:00, 1.53MB/s]\n",
      "Downloading data: 815kB [00:00, 76.1MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 34929.66 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37282.38 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 35905.52 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 32.5MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.26MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.42MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 343kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 373886.36 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 458951.20 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 261455.96 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 25.7MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 34.4MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 11.9MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 10.9MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 251247.35 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 256184.69 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 254889.41 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 6174.43 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 5399.12 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 31.9MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 4.20MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 241kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 181kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1176538.14 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 631235.43 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 516692.58 examples/s]\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127042.39it/s]\n",
      "\u001b[32m[2025-10-30 08:32:48 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2415.86it/s]\n",
      "\u001b[32m[2025-10-30 08:32:54 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1062.17it/s]\n",
      "\u001b[32m[2025-10-30 08:32:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1015.57it/s]\n",
      "\u001b[32m[2025-10-30 08:32:58 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:06<00:00, 25.10it/s]\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7040|±  |0.0128|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5634|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7436|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7643|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7458|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7726|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7835|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 70.11%\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_4.5bit/results.json\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 09:10:45 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy adaptive \\\n",
    "         --target_avg_bits 3 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_4.5bit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_4.5bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eg2GhJ3m0uQ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81832,
     "status": "ok",
     "timestamp": 1761838753263,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "eg2GhJ3m0uQ8",
    "outputId": "b5433315-3c7c-4a0d-a83f-5223ea41f332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_adaptive_training', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '3.0', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama_adaptive/llama3_optimized_4.0bit', '--save_quant_dir', './output/llama_adaptive/llama3_optimized_4.0bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama_adaptive/llama3_optimized_4.0bit', save_quant_dir='./output/llama_adaptive/llama3_optimized_4.0bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=3.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 14:06:10 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.05s/it]\n",
      "\u001b[32m[2025-10-30 14:06:16 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-30 14:06:16 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_train.cache\n",
      "\u001b[32m[2025-10-30 14:06:16 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_val.cache\n",
      "\u001b[32m[2025-10-30 14:06:16 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 14:06:19 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:06:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 14:06:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000008 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 5.57e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-30 14:07:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 2.07e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-30 14:07:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 14:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 14:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-30 14:08:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000060 | Val Loss: 0.000062 | Grad Norm: 0.02 | LR: 5.71e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:09:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000069 | Val Loss: 0.000041 | Grad Norm: 0.02 | LR: 3.46e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:09:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000054 | Val Loss: 0.000037 | Grad Norm: 0.02 | LR: 1.23e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:10:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000026 | Val Loss: 0.000033 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:10:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 14:10:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:10:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 14:10:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000056 | Val Loss: 0.000063 | Grad Norm: 0.00 | LR: 5.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:11:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000050 | Val Loss: 0.000061 | Grad Norm: 0.00 | LR: 2.11e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:11:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000049 | Val Loss: 0.000060 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 14:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:12:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000117 | Val Loss: 0.000124 | Grad Norm: 0.00 | LR: 5.75e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:13:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000106 | Val Loss: 0.000120 | Grad Norm: 0.00 | LR: 2.14e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:13:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000103 | Val Loss: 0.000119 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:13:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 14:13:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:13:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000199 | Val Loss: 0.000205 | Grad Norm: 0.00 | LR: 5.88e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:14:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000179 | Val Loss: 0.000199 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:15:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000173 | Val Loss: 0.000197 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:15:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 14:15:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:15:35 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:16:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000305 | Val Loss: 0.000314 | Grad Norm: 0.00 | LR: 5.93e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:16:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000275 | Val Loss: 0.000305 | Grad Norm: 0.00 | LR: 2.20e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:17:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000267 | Val Loss: 0.000302 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:17:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 14:17:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:17:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:17:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000418 | Val Loss: 0.000436 | Grad Norm: 0.00 | LR: 6.02e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:18:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000380 | Val Loss: 0.000424 | Grad Norm: 0.00 | LR: 2.24e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:18:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000369 | Val Loss: 0.000420 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 14:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:19:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000588 | Val Loss: 0.000606 | Grad Norm: 0.00 | LR: 6.16e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:20:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000527 | Val Loss: 0.000587 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:20:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000512 | Val Loss: 0.000582 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:20:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 14:20:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:20:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:21:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000736 | Val Loss: 0.000763 | Grad Norm: 0.00 | LR: 6.25e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000661 | Val Loss: 0.000740 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000642 | Val Loss: 0.000734 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:22:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 14:22:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:22:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:23:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000901 | Val Loss: 0.000950 | Grad Norm: 0.00 | LR: 6.31e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:23:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000811 | Val Loss: 0.000919 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:24:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000787 | Val Loss: 0.000912 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 14:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:25:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.001038 | Val Loss: 0.001114 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:25:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000945 | Val Loss: 0.001084 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:25:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000921 | Val Loss: 0.001077 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 14:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:26:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.001188 | Val Loss: 0.001285 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:27:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.001085 | Val Loss: 0.001250 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:27:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.001059 | Val Loss: 0.001242 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:28:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 14:28:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:28:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:28:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.001291 | Val Loss: 0.001402 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:29:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.001181 | Val Loss: 0.001367 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:29:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.001154 | Val Loss: 0.001358 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:29:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 14:29:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:29:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:30:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.001516 | Val Loss: 0.001646 | Grad Norm: 0.00 | LR: 6.39e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:30:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.001375 | Val Loss: 0.001599 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:31:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.001338 | Val Loss: 0.001587 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:31:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 14:31:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:31:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:32:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.001712 | Val Loss: 0.001866 | Grad Norm: 0.00 | LR: 6.49e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:32:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.001555 | Val Loss: 0.001815 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:33:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.001514 | Val Loss: 0.001802 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:33:24 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 14:33:24 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:33:24 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:33:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.001927 | Val Loss: 0.002133 | Grad Norm: 0.00 | LR: 6.42e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:34:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.001762 | Val Loss: 0.002079 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.001716 | Val Loss: 0.002065 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:35:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 14:35:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:35:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:35:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.002174 | Val Loss: 0.002440 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:36:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.002001 | Val Loss: 0.002383 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.001952 | Val Loss: 0.002368 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:36:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 14:36:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:36:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:37:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.002530 | Val Loss: 0.002890 | Grad Norm: 0.00 | LR: 6.62e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:37:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.002346 | Val Loss: 0.002835 | Grad Norm: 0.00 | LR: 2.46e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:38:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.002290 | Val Loss: 0.002816 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:38:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 14:38:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:38:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:39:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.002904 | Val Loss: 0.003386 | Grad Norm: 0.00 | LR: 4.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:39:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.002739 | Val Loss: 0.003351 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:40:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 14:40:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:40:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:40:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.003412 | Val Loss: 0.004001 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:41:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.003233 | Val Loss: 0.003966 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:41:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 14:41:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:41:23 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:41:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.004064 | Val Loss: 0.004774 | Grad Norm: 0.00 | LR: 4.86e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:42:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.003864 | Val Loss: 0.004736 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:42:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 14:42:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:42:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.005103 | Val Loss: 0.006054 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:43:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.004850 | Val Loss: 0.006001 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 14:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:44:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.006157 | Val Loss: 0.007389 | Grad Norm: 0.00 | LR: 4.79e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:45:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.005887 | Val Loss: 0.007338 | Grad Norm: 0.00 | LR: 4.67e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:45:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 14:45:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:45:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:45:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.007440 | Val Loss: 0.009034 | Grad Norm: 0.00 | LR: 5.00e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:46:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.007141 | Val Loss: 0.008984 | Grad Norm: 0.00 | LR: 4.87e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 14:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.008976 | Val Loss: 0.010980 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:47:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.008639 | Val Loss: 0.010922 | Grad Norm: 0.00 | LR: 4.92e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 14:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:48:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.010981 | Val Loss: 0.013505 | Grad Norm: 0.00 | LR: 5.08e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:49:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.010573 | Val Loss: 0.013427 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:49:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 14:49:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:49:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:49:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.013529 | Val Loss: 0.016716 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:50:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.013024 | Val Loss: 0.016627 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 14:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:51:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.016884 | Val Loss: 0.020913 | Grad Norm: 0.01 | LR: 5.09e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:51:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.016240 | Val Loss: 0.020770 | Grad Norm: 0.00 | LR: 4.96e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:51:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 14:51:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:51:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:52:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.021401 | Val Loss: 0.026552 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:52:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.020568 | Val Loss: 0.026374 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 14:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:53:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.027844 | Val Loss: 0.034464 | Grad Norm: 0.01 | LR: 4.95e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:54:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.026670 | Val Loss: 0.034202 | Grad Norm: 0.01 | LR: 4.82e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:54:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 14:54:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 14:54:35 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 14:55:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.039466 | Val Loss: 0.048342 | Grad Norm: 0.03 | LR: 4.61e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:55:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.037284 | Val Loss: 0.047756 | Grad Norm: 0.02 | LR: 4.49e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 14:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 14:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 14:56:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.083185 | Val Loss: 0.101875 | Grad Norm: 0.13 | LR: 5.67e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 14:56:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.074823 | Val Loss: 0.098553 | Grad Norm: 0.10 | LR: 2.11e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 14:57:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.072470 | Val Loss: 0.097650 | Grad Norm: 0.07 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 14:57:41 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama_adaptive/llama3_optimized_4.0bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 14:57:41 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama_adaptive/llama3_optimized_4.0bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 14:57:41 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3085.43s (51.42min)\n",
      "\u001b[32m[2025-10-30 14:57:41 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 14:57:57 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama_adaptive/llama3_optimized_4.0bit/model\n",
      "\u001b[32m[2025-10-30 14:57:57 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:27<00:00,  1.05s/it]\n",
      "wikitext2:7.527123928070068\n",
      "\u001b[32m[2025-10-30 15:00:36 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.53\n",
      "2025-10-30 15:00:37.063685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761836437.086033   97985 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761836437.092514   97985 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761836437.111255   97985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761836437.111287   97985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761836437.111291   97985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761836437.111293   97985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 15:00:42 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 15:00:43 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 15:00:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 15:01:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 15:01:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 15:01:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 15:01:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 15:01:13 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 128061.86it/s]\n",
      "\u001b[32m[2025-10-30 15:01:14 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2439.78it/s]\n",
      "\u001b[32m[2025-10-30 15:01:19 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1048.52it/s]\n",
      "\u001b[32m[2025-10-30 15:01:22 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1020.43it/s]\n",
      "\u001b[32m[2025-10-30 15:01:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:06<00:00, 25.10it/s]\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7119|±  |0.0127|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5645|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7479|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7622|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7449|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7840|±  |0.0096|\n",
      "|          |       |none  |     0|acc_norm|0.7862|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 70.57%\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama_adaptive/llama3_optimized_4.0bit/results.json\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 15:39:10 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_adaptive_training \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy adaptive \\\n",
    "         --target_avg_bits 3.0 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama_adaptive/llama3_optimized_4.0bit\" \\\n",
    "         --save_quant_dir \"./output/llama_adaptive/llama3_optimized_4.0bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tRr5aEQcE7GT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4919920,
     "status": "ok",
     "timestamp": 1761820369647,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "tRr5aEQcE7GT",
    "outputId": "359762f1-51be-434d-8ff9-c6c7bc152faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '3', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--output_dir', './output/llama3_baseline', '--save_quant_dir', './output/llama3_baseline/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 09:10:54 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_baseline', save_quant_dir='./output/llama3_baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-30 09:10:54 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:13<00:00,  3.45s/it]\n",
      "\u001b[32m[2025-10-30 09:11:09 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "get_wikitext2\n",
      "\u001b[32m[2025-10-30 09:11:31 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-30 09:11:33 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-30 09:11:35 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:12:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:7.74296040617628e-06 val_loss:6.528880931000458e-06 quant_lr:5.1362214303938806e-05 norm:0.00040603 max memory_allocated 7374.298828125 time 25.963460206985474 \n",
      "\u001b[32m[2025-10-30 09:12:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:5.979690740787191e-06 val_loss:6.114275493018795e-06 quant_lr:5e-06 norm:0.00018375 max memory_allocated 7438.173828125 time 25.668950080871582 \n",
      "\u001b[32m[2025-10-30 09:12:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:12:50 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:13:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.00020978422253392637 val_loss:0.0001671618956606835 quant_lr:5.1362214303938806e-05 norm:0.01713497 max memory_allocated 7438.173828125 time 25.59941601753235 \n",
      "\u001b[32m[2025-10-30 09:13:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.00014038926747161895 val_loss:0.00012100214371457696 quant_lr:5e-06 norm:0.01084709 max memory_allocated 7438.173828125 time 25.631559133529663 \n",
      "\u001b[32m[2025-10-30 09:13:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:13:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:13:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:13:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:13:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:13:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:14:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:14:01 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:14:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.00017148558981716633 val_loss:0.00017677263531368226 quant_lr:5.1362214303938806e-05 norm:0.00033236 max memory_allocated 7438.173828125 time 25.63560175895691 \n",
      "\u001b[32m[2025-10-30 09:14:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.0001615878427401185 val_loss:0.00017468328587710857 quant_lr:5e-06 norm:0.00013340 max memory_allocated 7438.673828125 time 25.685910940170288 \n",
      "\u001b[32m[2025-10-30 09:15:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:15:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:15:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.00024121378373820335 val_loss:0.0002487809688318521 quant_lr:5.1362214303938806e-05 norm:0.00044223 max memory_allocated 7438.673828125 time 25.654875993728638 \n",
      "\u001b[32m[2025-10-30 09:16:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.00022623484255746007 val_loss:0.00024571752874180675 quant_lr:5e-06 norm:0.00023780 max memory_allocated 7438.673828125 time 25.706000804901123 \n",
      "\u001b[32m[2025-10-30 09:16:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:16:29 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:16:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0003284595732111484 val_loss:0.00033992258249782026 quant_lr:5.1362214303938806e-05 norm:0.00055508 max memory_allocated 7438.673828125 time 25.667361736297607 \n",
      "\u001b[32m[2025-10-30 09:17:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0003052632964681834 val_loss:0.00033481919672340155 quant_lr:5e-06 norm:0.00036767 max memory_allocated 7438.673828125 time 25.749892234802246 \n",
      "\u001b[32m[2025-10-30 09:17:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:17:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:18:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0004452975408639759 val_loss:0.000458348571555689 quant_lr:5.1362214303938806e-05 norm:0.00082238 max memory_allocated 7438.673828125 time 25.686947107315063 \n",
      "\u001b[32m[2025-10-30 09:18:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.00041074774344451725 val_loss:0.00045151804806664586 quant_lr:5e-06 norm:0.00044572 max memory_allocated 7438.673828125 time 25.76625370979309 \n",
      "\u001b[32m[2025-10-30 09:18:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:18:57 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:19:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0005663607735186815 val_loss:0.0005928013706579804 quant_lr:5.1362214303938806e-05 norm:0.00094296 max memory_allocated 7438.673828125 time 25.6489417552948 \n",
      "\u001b[32m[2025-10-30 09:19:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0005238839075900614 val_loss:0.0005831424496136606 quant_lr:5e-06 norm:0.00057543 max memory_allocated 7438.673828125 time 25.696159839630127 \n",
      "\u001b[32m[2025-10-30 09:19:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:19:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:19:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:20:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:20:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:20:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:20:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:20:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:20:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0007153967162594199 val_loss:0.0007560623926110566 quant_lr:5.1362214303938806e-05 norm:0.00103101 max memory_allocated 7438.673828125 time 25.651745557785034 \n",
      "\u001b[32m[2025-10-30 09:21:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0006613536388613284 val_loss:0.0007433931459672749 quant_lr:5e-06 norm:0.00067876 max memory_allocated 7438.673828125 time 25.7244234085083 \n",
      "\u001b[32m[2025-10-30 09:21:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:21:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:21:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.0008470009197480977 val_loss:0.0009056275594048202 quant_lr:5.1362214303938806e-05 norm:0.00107432 max memory_allocated 7438.673828125 time 25.640902280807495 \n",
      "\u001b[32m[2025-10-30 09:22:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.0007852016715332866 val_loss:0.0008922741981223226 quant_lr:5e-06 norm:0.00076829 max memory_allocated 7438.673828125 time 25.69853663444519 \n",
      "\u001b[32m[2025-10-30 09:22:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:22:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:23:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.0010005709482356906 val_loss:0.0010883987415581942 quant_lr:5.1362214303938806e-05 norm:0.00142668 max memory_allocated 7438.673828125 time 25.747117042541504 \n",
      "\u001b[32m[2025-10-30 09:23:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.000926177657674998 val_loss:0.0010714380769059062 quant_lr:5e-06 norm:0.00104084 max memory_allocated 7438.673828125 time 25.809218168258667 \n",
      "\u001b[32m[2025-10-30 09:23:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:23:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:24:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.0011345224920660257 val_loss:0.0012495719129219651 quant_lr:5.1362214303938806e-05 norm:0.00129079 max memory_allocated 7438.673828125 time 25.649721384048462 \n",
      "\u001b[32m[2025-10-30 09:24:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.0010572230676189065 val_loss:0.00123326457105577 quant_lr:5e-06 norm:0.00097703 max memory_allocated 7438.923828125 time 25.820483446121216 \n",
      "\u001b[32m[2025-10-30 09:24:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:24:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:24:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:24:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:24:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:25:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:25:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:25:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:25:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0012738327495753765 val_loss:0.0014143303269520402 quant_lr:5.1362214303938806e-05 norm:0.00123845 max memory_allocated 7438.923828125 time 25.73343324661255 \n",
      "\u001b[32m[2025-10-30 09:26:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.0011903783306479454 val_loss:0.0013953002635389566 quant_lr:5e-06 norm:0.00091556 max memory_allocated 7438.923828125 time 25.8088116645813 \n",
      "\u001b[32m[2025-10-30 09:26:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:26:20 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:26:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.0013613620540127158 val_loss:0.0015267006820067763 quant_lr:5.1362214303938806e-05 norm:0.00133860 max memory_allocated 7438.923828125 time 25.647783994674683 \n",
      "\u001b[32m[2025-10-30 09:27:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.001275456277653575 val_loss:0.0015072182286530733 quant_lr:5e-06 norm:0.00098798 max memory_allocated 7438.923828125 time 25.750356435775757 \n",
      "\u001b[32m[2025-10-30 09:27:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:27:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:28:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.0015658094780519605 val_loss:0.0017611508956179023 quant_lr:5.1362214303938806e-05 norm:0.00161889 max memory_allocated 7438.923828125 time 25.669832706451416 \n",
      "\u001b[32m[2025-10-30 09:28:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.0014529076870530844 val_loss:0.0017338731558993459 quant_lr:5e-06 norm:0.00117791 max memory_allocated 7438.923828125 time 25.71169924736023 \n",
      "\u001b[32m[2025-10-30 09:28:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:28:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:29:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.0017436887137591839 val_loss:0.0019663458224385977 quant_lr:5.1362214303938806e-05 norm:0.00147376 max memory_allocated 7438.923828125 time 25.669440269470215 \n",
      "\u001b[32m[2025-10-30 09:29:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.00161978998221457 val_loss:0.0019388905493542552 quant_lr:5e-06 norm:0.00114350 max memory_allocated 7438.923828125 time 25.72238063812256 \n",
      "\u001b[32m[2025-10-30 09:29:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:29:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:29:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:29:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:29:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:29:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:30:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:30:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:30:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.00195022311527282 val_loss:0.0022270355839282274 quant_lr:5.1362214303938806e-05 norm:0.00167641 max memory_allocated 7438.923828125 time 25.67796039581299 \n",
      "\u001b[32m[2025-10-30 09:30:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0018167626112699509 val_loss:0.0021978833246976137 quant_lr:5e-06 norm:0.00132202 max memory_allocated 7438.923828125 time 25.770951986312866 \n",
      "\u001b[32m[2025-10-30 09:31:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:31:16 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:31:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.0021925007458776236 val_loss:0.0025376789271831512 quant_lr:5.1362214303938806e-05 norm:0.00196907 max memory_allocated 7438.923828125 time 25.684181690216064 \n",
      "\u001b[32m[2025-10-30 09:32:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.0020516503136605024 val_loss:0.0025086046662181616 quant_lr:5e-06 norm:0.00161747 max memory_allocated 7438.923828125 time 25.796786546707153 \n",
      "\u001b[32m[2025-10-30 09:32:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:32:31 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:33:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.00254573835991323 val_loss:0.003003336489200592 quant_lr:5.1362214303938806e-05 norm:0.00213512 max memory_allocated 7438.923828125 time 25.68282198905945 \n",
      "\u001b[32m[2025-10-30 09:33:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.0023922258988022804 val_loss:0.002972705289721489 quant_lr:5e-06 norm:0.00177229 max memory_allocated 7438.923828125 time 25.81136202812195 \n",
      "\u001b[32m[2025-10-30 09:33:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:33:46 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:34:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.00293662934564054 val_loss:0.0035386481322348118 quant_lr:5.1362214303938806e-05 norm:0.00236425 max memory_allocated 7438.923828125 time 25.693630933761597 \n",
      "\u001b[32m[2025-10-30 09:34:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.002786296419799328 val_loss:0.0035078725777566433 quant_lr:5e-06 norm:0.00187781 max memory_allocated 7438.923828125 time 25.79851484298706 \n",
      "\u001b[32m[2025-10-30 09:34:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:34:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:34:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:34:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:34:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:35:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:35:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:35:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.0034006224013864994 val_loss:0.004172794986516237 quant_lr:5.1362214303938806e-05 norm:0.00247003 max memory_allocated 7438.923828125 time 25.71062207221985 \n",
      "\u001b[32m[2025-10-30 09:35:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.003243999555706978 val_loss:0.004143220838159323 quant_lr:5e-06 norm:0.00213578 max memory_allocated 7438.923828125 time 25.819798231124878 \n",
      "\u001b[32m[2025-10-30 09:36:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:36:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:36:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.004008165095001459 val_loss:0.004963844083249569 quant_lr:5.1362214303938806e-05 norm:0.00318818 max memory_allocated 7438.923828125 time 25.7186336517334 \n",
      "\u001b[32m[2025-10-30 09:37:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.0038348345551639795 val_loss:0.004933910444378853 quant_lr:5e-06 norm:0.00216954 max memory_allocated 7438.923828125 time 25.758604526519775 \n",
      "\u001b[32m[2025-10-30 09:37:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:37:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:38:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.0049795848317444324 val_loss:0.006266764365136623 quant_lr:5.1362214303938806e-05 norm:0.00254474 max memory_allocated 7438.923828125 time 25.72169256210327 \n",
      "\u001b[32m[2025-10-30 09:38:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.0047608669847249985 val_loss:0.006229327525943518 quant_lr:5e-06 norm:0.00223784 max memory_allocated 7438.923828125 time 25.773953437805176 \n",
      "\u001b[32m[2025-10-30 09:38:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:38:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:39:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.005976864602416754 val_loss:0.007694876752793789 quant_lr:5.1362214303938806e-05 norm:0.00284132 max memory_allocated 7438.923828125 time 25.698518991470337 \n",
      "\u001b[32m[2025-10-30 09:39:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.005740937311202288 val_loss:0.007658088114112616 quant_lr:5e-06 norm:0.00218270 max memory_allocated 7438.923828125 time 25.80026078224182 \n",
      "\u001b[32m[2025-10-30 09:39:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:39:59 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:40:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.007188320159912109 val_loss:0.009485695511102676 quant_lr:5.1362214303938806e-05 norm:0.00295647 max memory_allocated 7438.923828125 time 25.70168685913086 \n",
      "\u001b[32m[2025-10-30 09:40:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.006926099769771099 val_loss:0.009444310329854488 quant_lr:5e-06 norm:0.00231717 max memory_allocated 7438.923828125 time 25.80013418197632 \n",
      "\u001b[32m[2025-10-30 09:41:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:41:14 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:41:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.008630628697574139 val_loss:0.011594719253480434 quant_lr:5.1362214303938806e-05 norm:0.00263297 max memory_allocated 7438.923828125 time 25.727148056030273 \n",
      "\u001b[32m[2025-10-30 09:42:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.00833530630916357 val_loss:0.011552304029464722 quant_lr:5e-06 norm:0.00211045 max memory_allocated 7438.923828125 time 25.78998064994812 \n",
      "\u001b[32m[2025-10-30 09:42:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:42:28 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:42:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.010501581244170666 val_loss:0.014343766495585442 quant_lr:5.1362214303938806e-05 norm:0.00373778 max memory_allocated 7438.923828125 time 25.71798825263977 \n",
      "\u001b[32m[2025-10-30 09:43:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.010147259570658207 val_loss:0.01429472491145134 quant_lr:5e-06 norm:0.00289368 max memory_allocated 7438.923828125 time 25.776275396347046 \n",
      "\u001b[32m[2025-10-30 09:43:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:43:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:44:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.012875479646027088 val_loss:0.017911411821842194 quant_lr:5.1362214303938806e-05 norm:0.00404560 max memory_allocated 7438.923828125 time 25.72040557861328 \n",
      "\u001b[32m[2025-10-30 09:44:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.012436331249773502 val_loss:0.017852023243904114 quant_lr:5e-06 norm:0.00327324 max memory_allocated 7438.923828125 time 25.796578645706177 \n",
      "\u001b[32m[2025-10-30 09:44:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:44:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:45:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.015996865928173065 val_loss:0.022541655227541924 quant_lr:5.1362214303938806e-05 norm:0.00578065 max memory_allocated 7438.923828125 time 25.722275018692017 \n",
      "\u001b[32m[2025-10-30 09:45:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.015428418293595314 val_loss:0.022408785298466682 quant_lr:5e-06 norm:0.00441335 max memory_allocated 7438.923828125 time 25.82421898841858 \n",
      "\u001b[32m[2025-10-30 09:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:46:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:46:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:46:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:46:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:46:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:46:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.020169464871287346 val_loss:0.028801321983337402 quant_lr:5.1362214303938806e-05 norm:0.00654027 max memory_allocated 7438.923828125 time 25.731239557266235 \n",
      "\u001b[32m[2025-10-30 09:47:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.019450150430202484 val_loss:0.028669016435742378 quant_lr:5e-06 norm:0.00500319 max memory_allocated 7438.923828125 time 25.80191922187805 \n",
      "\u001b[32m[2025-10-30 09:47:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:47:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:47:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.026037583127617836 val_loss:0.03729015588760376 quant_lr:5.1362214303938806e-05 norm:0.01155221 max memory_allocated 7438.923828125 time 25.77052140235901 \n",
      "\u001b[32m[2025-10-30 09:48:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.025032799690961838 val_loss:0.037046562880277634 quant_lr:5e-06 norm:0.00845256 max memory_allocated 7438.923828125 time 25.789140224456787 \n",
      "\u001b[32m[2025-10-30 09:48:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:48:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:49:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.03639481961727142 val_loss:0.05147469788789749 quant_lr:5.1362214303938806e-05 norm:0.02519370 max memory_allocated 7438.923828125 time 25.731422424316406 \n",
      "\u001b[32m[2025-10-30 09:49:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.03454495221376419 val_loss:0.05109017342329025 quant_lr:5e-06 norm:0.01814307 max memory_allocated 7438.923828125 time 25.821248054504395 \n",
      "\u001b[32m[2025-10-30 09:49:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:49:53 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-30 09:50:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.07797471433877945 val_loss:0.10074885189533234 quant_lr:5.1362214303938806e-05 norm:0.12834255 max memory_allocated 7438.923828125 time 25.699772834777832 \n",
      "\u001b[32m[2025-10-30 09:50:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.07017478346824646 val_loss:0.09849090129137039 quant_lr:5e-06 norm:0.08589544 max memory_allocated 7438.923828125 time 25.840728759765625 \n",
      "\u001b[32m[2025-10-30 09:50:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-30 09:50:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-30 09:50:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-30 09:50:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-30 09:51:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-30 09:51:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-30 09:51:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-30 09:51:09 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2399.625407218933\n",
      "\u001b[32m[2025-10-30 09:51:09 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-30 09:51:27 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 141/141 [02:28<00:00,  1.05s/it]\n",
      "wikitext2:7.470151424407959\n",
      "\u001b[32m[2025-10-30 09:54:04 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 7.47\n",
      "2025-10-30 09:54:05.002018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761818045.023361   24553 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761818045.029663   24553 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761818045.048257   24553 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761818045.048294   24553 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761818045.048298   24553 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761818045.048302   24553 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 09:54:09 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 09:54:10 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 09:54:15 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 125313.82it/s]\n",
      "\u001b[32m[2025-10-30 09:54:45 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2460.78it/s]\n",
      "\u001b[32m[2025-10-30 09:54:51 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1067.56it/s]\n",
      "\u001b[32m[2025-10-30 09:54:53 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1033.43it/s]\n",
      "\u001b[32m[2025-10-30 09:54:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:11<00:00, 25.04it/s]\n",
      "\u001b[32m[2025-10-30 10:32:46 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7103|±  |0.0127|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5639|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7429|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7647|±  |0.0087|\n",
      "|          |       |none  |     0|acc_norm|0.7483|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7813|±  |0.0096|\n",
      "|          |       |none  |     0|acc_norm|0.7889|±  |0.0095|\n",
      "\n",
      "\u001b[32m[2025-10-30 10:32:46 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 70.51%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "     --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 3 \\\n",
    "       --group_size 128 \\\n",
    "        --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "       --output_dir ./output/llama3_baseline \\\n",
    "       --save_quant_dir ./output/llama3_baseline/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7R3lglGNoWUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5595668,
     "status": "ok",
     "timestamp": 1761847673688,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "7R3lglGNoWUo",
    "outputId": "a904083f-1404-49eb-d4cc-cab188865f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_adaptive_training', '--wbits', '3', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--real_quant', '--output_dir', './output/llama3_optimized_3bitsgra', '--save_quant_dir', './output/llama3_optimized_3bitsgra/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_3bitsgra', save_quant_dir='./output/llama3_optimized_3bitsgra/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 16:34:42 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n",
      "\u001b[32m[2025-10-30 16:34:48 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-30 16:34:48 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_train.cache\n",
      "\u001b[32m[2025-10-30 16:34:48 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_128_32_2048_val.cache\n",
      "\u001b[32m[2025-10-30 16:34:48 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 16:34:51 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:34:52 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 16:35:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000008 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 5.57e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:35:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 2.07e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:36:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 16:36:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:36:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-30 16:37:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000228 | Val Loss: 0.000149 | Grad Norm: 0.02 | LR: 5.71e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:37:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000138 | Val Loss: 0.000120 | Grad Norm: 0.01 | LR: 3.46e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:38:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000106 | Val Loss: 0.000108 | Grad Norm: 0.01 | LR: 1.23e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:38:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000084 | Val Loss: 0.000088 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 16:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 16:39:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000118 | Val Loss: 0.000126 | Grad Norm: 0.00 | LR: 5.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:39:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000109 | Val Loss: 0.000122 | Grad Norm: 0.00 | LR: 2.11e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:40:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000107 | Val Loss: 0.000122 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:40:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 16:40:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:40:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:41:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000185 | Val Loss: 0.000194 | Grad Norm: 0.00 | LR: 5.75e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:41:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000172 | Val Loss: 0.000189 | Grad Norm: 0.00 | LR: 2.14e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:42:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000168 | Val Loss: 0.000188 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:42:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 16:42:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:42:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:42:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000271 | Val Loss: 0.000281 | Grad Norm: 0.00 | LR: 5.88e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:43:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000248 | Val Loss: 0.000273 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:43:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000243 | Val Loss: 0.000272 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:44:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 16:44:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:44:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:44:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000383 | Val Loss: 0.000395 | Grad Norm: 0.00 | LR: 5.93e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:45:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000350 | Val Loss: 0.000385 | Grad Norm: 0.00 | LR: 2.20e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:45:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000341 | Val Loss: 0.000383 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:45:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 16:45:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:45:51 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:46:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000498 | Val Loss: 0.000522 | Grad Norm: 0.00 | LR: 6.02e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:46:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000457 | Val Loss: 0.000509 | Grad Norm: 0.00 | LR: 2.24e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000446 | Val Loss: 0.000505 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:47:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 16:47:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:47:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:48:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000639 | Val Loss: 0.000677 | Grad Norm: 0.00 | LR: 6.16e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:48:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000588 | Val Loss: 0.000659 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:49:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000573 | Val Loss: 0.000655 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:49:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 16:49:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:49:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:49:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000764 | Val Loss: 0.000818 | Grad Norm: 0.00 | LR: 6.25e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:50:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000703 | Val Loss: 0.000798 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:50:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000685 | Val Loss: 0.000793 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:51:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 16:51:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:51:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:51:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000907 | Val Loss: 0.000989 | Grad Norm: 0.00 | LR: 6.31e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:52:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000835 | Val Loss: 0.000964 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:52:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000813 | Val Loss: 0.000958 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 16:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:53:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.001029 | Val Loss: 0.001142 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:54:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000954 | Val Loss: 0.001119 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:54:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000931 | Val Loss: 0.001112 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:54:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 16:54:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:54:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:55:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.001161 | Val Loss: 0.001302 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:55:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.001078 | Val Loss: 0.001273 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:56:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.001055 | Val Loss: 0.001265 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:56:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 16:56:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:56:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:57:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.001244 | Val Loss: 0.001404 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:57:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.001158 | Val Loss: 0.001376 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 16:58:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.001133 | Val Loss: 0.001369 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 16:58:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 16:58:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 16:58:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 16:58:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.001439 | Val Loss: 0.001631 | Grad Norm: 0.00 | LR: 6.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:59:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.001326 | Val Loss: 0.001592 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 16:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.001293 | Val Loss: 0.001582 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:00:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 17:00:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:00:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:00:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.001604 | Val Loss: 0.001832 | Grad Norm: 0.00 | LR: 6.49e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:01:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.001481 | Val Loss: 0.001792 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:01:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.001445 | Val Loss: 0.001779 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:01:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 17:01:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:01:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 17:02:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.001797 | Val Loss: 0.002082 | Grad Norm: 0.00 | LR: 6.42e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:02:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.001665 | Val Loss: 0.002040 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:03:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.001623 | Val Loss: 0.002027 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:03:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 17:03:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:03:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 17:04:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.002016 | Val Loss: 0.002374 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:04:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.001880 | Val Loss: 0.002332 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:05:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.001835 | Val Loss: 0.002317 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:05:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 17:05:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:05:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:06:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.002336 | Val Loss: 0.002808 | Grad Norm: 0.00 | LR: 6.62e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:06:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.002187 | Val Loss: 0.002761 | Grad Norm: 0.00 | LR: 2.46e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:06:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.002137 | Val Loss: 0.002747 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:07:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 17:07:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:07:15 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:07:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.002681 | Val Loss: 0.003288 | Grad Norm: 0.00 | LR: 4.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:08:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.002544 | Val Loss: 0.003259 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:08:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 17:08:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:08:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:09:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.003144 | Val Loss: 0.003883 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.002994 | Val Loss: 0.003856 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:09:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 17:09:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:09:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:10:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.003735 | Val Loss: 0.004623 | Grad Norm: 0.00 | LR: 4.86e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:10:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.003568 | Val Loss: 0.004594 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:11:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 17:11:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:11:13 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:11:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.004666 | Val Loss: 0.005862 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:12:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.004455 | Val Loss: 0.005823 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:12:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 17:12:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:12:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:13:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.005624 | Val Loss: 0.007159 | Grad Norm: 0.00 | LR: 4.79e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:13:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.005397 | Val Loss: 0.007120 | Grad Norm: 0.00 | LR: 4.67e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:13:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 17:13:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:13:52 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:14:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.006789 | Val Loss: 0.008764 | Grad Norm: 0.00 | LR: 5.00e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:14:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.006534 | Val Loss: 0.008726 | Grad Norm: 0.00 | LR: 4.87e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 17:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:15:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:15:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.008178 | Val Loss: 0.010639 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:16:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.007890 | Val Loss: 0.010595 | Grad Norm: 0.00 | LR: 4.92e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:16:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 17:16:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:16:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:17:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.009981 | Val Loss: 0.013058 | Grad Norm: 0.00 | LR: 5.08e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:17:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.009633 | Val Loss: 0.013002 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:17:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 17:17:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:17:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:18:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.012270 | Val Loss: 0.016170 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:18:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.011842 | Val Loss: 0.016094 | Grad Norm: 0.00 | LR: 4.95e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 17:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:19:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.015290 | Val Loss: 0.020156 | Grad Norm: 0.01 | LR: 5.09e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:20:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.014734 | Val Loss: 0.020044 | Grad Norm: 0.00 | LR: 4.96e-06 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:20:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 17:20:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:20:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:21:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.019332 | Val Loss: 0.025523 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:21:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.018625 | Val Loss: 0.025394 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:21:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 17:21:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:21:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:22:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.025053 | Val Loss: 0.032972 | Grad Norm: 0.01 | LR: 4.95e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:22:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.024066 | Val Loss: 0.032717 | Grad Norm: 0.01 | LR: 4.82e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:23:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 17:23:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:23:09 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 17:23:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.035238 | Val Loss: 0.045774 | Grad Norm: 0.02 | LR: 4.61e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-30 17:24:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.033436 | Val Loss: 0.045352 | Grad Norm: 0.02 | LR: 4.49e-06 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 17:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 17:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 17:25:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.076643 | Val Loss: 0.097810 | Grad Norm: 0.12 | LR: 5.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:25:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.068720 | Val Loss: 0.094470 | Grad Norm: 0.09 | LR: 2.11e-05 | Time: 27.8s\n",
      "\u001b[32m[2025-10-30 17:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.066465 | Val Loss: 0.093661 | Grad Norm: 0.07 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-30 17:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_3bitsgra/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 17:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_3bitsgra/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 17:26:16 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3088.73s (51.48min)\n",
      "\u001b[32m[2025-10-30 17:26:16 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 17:26:31 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_3bitsgra/model\n",
      "\u001b[32m[2025-10-30 17:26:31 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:26<00:00,  1.04s/it]\n",
      "wikitext2:7.4117112159729\n",
      "\u001b[32m[2025-10-30 17:29:09 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.41\n",
      "2025-10-30 17:29:10.351814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761845350.373585  135146 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761845350.379926  135146 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761845350.398648  135146 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761845350.398685  135146 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761845350.398693  135146 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761845350.398696  135146 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 17:29:15 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-30 17:29:16 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 17:29:20 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127420.11it/s]\n",
      "\u001b[32m[2025-10-30 17:29:46 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2382.49it/s]\n",
      "\u001b[32m[2025-10-30 17:29:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1045.46it/s]\n",
      "\u001b[32m[2025-10-30 17:29:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1015.84it/s]\n",
      "\u001b[32m[2025-10-30 17:29:57 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:14<00:00, 25.01it/s]\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6969|±  |0.0129|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5633|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7461|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7786|±  |0.0085|\n",
      "|          |       |none  |     0|acc_norm|0.7677|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.7862|±  |0.0096|\n",
      "|          |       |none  |     0|acc_norm|0.7927|±  |0.0095|\n",
      "\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 70.63%\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_3bitsgra/results.json\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 18:07:50 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_adaptive_training \\\n",
    "          --wbits 3 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 1e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_3bitsgra\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_3bitsgra/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59136fc9",
   "metadata": {},
   "source": [
    "# 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dnp3L26ucKV9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4922155,
     "status": "ok",
     "timestamp": 1761884429177,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "dnp3L26ucKV9",
    "outputId": "8e8cb86c-2715-4995-f228-6d3d158f2b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '2', '--group_size', '64', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--output_dir', './output/llama3_baseline4', '--save_quant_dir', './output/llama3_baseline4/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 02:58:34 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_baseline4', save_quant_dir='./output/llama3_baseline4/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=64, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-31 02:58:34 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 654/654 [00:00<00:00, 4.33MB/s]\n",
      "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 7.45MB/s]\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 19.4MB/s]\n",
      "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 560kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 103MB/s]\n",
      "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1% 41.7M/4.98G [00:02<04:16, 19.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7% 355M/4.98G [00:02<00:28, 160MB/s]  \u001b[A\n",
      "model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:03<00:06, 598MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28% 1.39G/4.98G [00:03<00:05, 608MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30% 1.51G/4.98G [00:04<00:07, 458MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32% 1.58G/4.98G [00:06<00:17, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36% 1.81G/4.98G [00:06<00:11, 280MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40% 1.98G/4.98G [00:06<00:08, 355MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43% 2.14G/4.98G [00:06<00:06, 442MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46% 2.30G/4.98G [00:06<00:05, 521MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49% 2.44G/4.98G [00:06<00:05, 506MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52% 2.60G/4.98G [00:06<00:03, 616MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54% 2.71G/4.98G [00:07<00:03, 578MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57% 2.86G/4.98G [00:10<00:14, 144MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [00:10<00:08, 229MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66% 3.26G/4.98G [00:10<00:05, 310MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69% 3.42G/4.98G [00:10<00:03, 396MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72% 3.60G/4.98G [00:10<00:02, 517MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76% 3.80G/4.98G [00:10<00:01, 657MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80% 3.98G/4.98G [00:10<00:01, 774MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84% 4.16G/4.98G [00:11<00:01, 586MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86% 4.28G/4.98G [00:11<00:01, 508MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89% 4.42G/4.98G [00:12<00:01, 424MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91% 4.55G/4.98G [00:12<00:00, 428MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93% 4.62G/4.98G [00:12<00:00, 434MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94% 4.68G/4.98G [00:12<00:00, 415MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96% 4.79G/4.98G [00:12<00:00, 399MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97% 4.85G/4.98G [00:13<00:00, 387MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:13<00:00, 375MB/s]\n",
      "Downloading shards:  25% 1/4 [00:13<00:41, 13.81s/it]\n",
      "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0% 34.5k/5.00G [00:01<53:59:25, 25.7kB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7% 373M/5.00G [00:02<00:20, 231MB/s]     \u001b[A\n",
      "model-00002-of-00004.safetensors:  19% 962M/5.00G [00:02<00:06, 662MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24% 1.18G/5.00G [00:04<00:16, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [00:04<00:11, 313MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31% 1.57G/5.00G [00:05<00:09, 367MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34% 1.70G/5.00G [00:05<00:07, 422MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37% 1.84G/5.00G [00:05<00:08, 359MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48% 2.38G/5.00G [00:06<00:04, 649MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [00:06<00:04, 607MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53% 2.65G/5.00G [00:06<00:04, 558MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55% 2.77G/5.00G [00:08<00:11, 200MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60% 3.00G/5.00G [00:08<00:06, 287MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64% 3.18G/5.00G [00:09<00:04, 370MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66% 3.31G/5.00G [00:09<00:03, 442MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70% 3.48G/5.00G [00:09<00:03, 499MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72% 3.60G/5.00G [00:09<00:02, 578MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75% 3.73G/5.00G [00:09<00:02, 516MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77% 3.84G/5.00G [00:10<00:02, 443MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80% 4.02G/5.00G [00:10<00:01, 563MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83% 4.16G/5.00G [00:10<00:01, 513MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86% 4.29G/5.00G [00:11<00:01, 359MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88% 4.41G/5.00G [00:12<00:02, 283MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89% 4.47G/5.00G [00:12<00:02, 258MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91% 4.54G/5.00G [00:12<00:01, 230MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93% 4.63G/5.00G [00:13<00:01, 267MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:13<00:00, 378MB/s]\n",
      "Downloading shards:  50% 2/4 [00:27<00:27, 13.76s/it]\n",
      "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0% 407k/4.92G [00:01<4:25:07, 309kB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3% 144M/4.92G [00:02<00:55, 86.5MB/s] \u001b[A\n",
      "model-00003-of-00004.safetensors:  23% 1.13G/4.92G [00:02<00:05, 720MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26% 1.28G/4.92G [00:02<00:05, 668MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29% 1.41G/4.92G [00:02<00:05, 648MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31% 1.51G/4.92G [00:05<00:17, 193MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37% 1.80G/4.92G [00:05<00:10, 306MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39% 1.94G/4.92G [00:05<00:08, 363MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43% 2.12G/4.92G [00:05<00:06, 458MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47% 2.29G/4.92G [00:05<00:04, 547MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50% 2.44G/4.92G [00:06<00:04, 515MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53% 2.62G/4.92G [00:06<00:03, 652MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56% 2.76G/4.92G [00:06<00:03, 650MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59% 2.92G/4.92G [00:06<00:03, 574MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62% 3.05G/4.92G [00:07<00:03, 542MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63% 3.12G/4.92G [00:07<00:03, 489MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65% 3.19G/4.92G [00:09<00:12, 136MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69% 3.37G/4.92G [00:09<00:06, 221MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73% 3.58G/4.92G [00:09<00:04, 328MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76% 3.71G/4.92G [00:09<00:02, 412MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78% 3.85G/4.92G [00:10<00:02, 472MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81% 3.98G/4.92G [00:10<00:01, 525MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86% 4.22G/4.92G [00:10<00:01, 640MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89% 4.35G/4.92G [00:10<00:01, 538MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91% 4.46G/4.92G [00:11<00:00, 562MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93% 4.59G/4.92G [00:11<00:00, 440MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95% 4.66G/4.92G [00:11<00:00, 373MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96% 4.73G/4.92G [00:12<00:00, 324MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:12<00:00, 399MB/s]\n",
      "Downloading shards:  75% 3/4 [00:40<00:13, 13.32s/it]\n",
      "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3% 30.1M/1.17G [00:01<00:42, 26.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8% 97.1M/1.17G [00:01<00:12, 86.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  49% 567M/1.17G [00:01<00:00, 606MB/s]  \u001b[A\n",
      "model-00004-of-00004.safetensors:  66% 776M/1.17G [00:01<00:00, 665MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78% 910M/1.17G [00:01<00:00, 669MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87% 1.01G/1.17G [00:02<00:00, 672MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:02<00:00, 473MB/s]\n",
      "Downloading shards: 100% 4/4 [00:43<00:00, 10.82s/it]\n",
      "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n",
      "generation_config.json: 100% 177/177 [00:00<00:00, 1.28MB/s]\n",
      "\u001b[32m[2025-10-31 02:59:27 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 27.5MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 1.38MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:00<00:00, 12.5MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 1.38MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 86326.11 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 745285.08 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 610293.06 examples/s]\n",
      "\u001b[32m[2025-10-31 02:59:53 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-31 02:59:56 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-31 02:59:57 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:00:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:2.845898416126147e-05 val_loss:2.4917029804782942e-05 quant_lr:5.1362214303938806e-05 norm:0.00036141 max memory_allocated 7436.798828125 time 25.575700521469116 \n",
      "\u001b[32m[2025-10-31 03:00:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:2.2875427021062933e-05 val_loss:2.3614118617842905e-05 quant_lr:5e-06 norm:0.00014203 max memory_allocated 7498.173828125 time 25.285247564315796 \n",
      "\u001b[32m[2025-10-31 03:00:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:01:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:01:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0009824951412156224 val_loss:0.00035505532287061214 quant_lr:5.1362214303938806e-05 norm:0.04588989 max memory_allocated 7498.173828125 time 25.333343267440796 \n",
      "\u001b[32m[2025-10-31 03:02:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.0002925452427007258 val_loss:0.000277395942248404 quant_lr:5e-06 norm:0.01568115 max memory_allocated 7499.798828125 time 25.46074342727661 \n",
      "\u001b[32m[2025-10-31 03:02:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:02:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:02:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.0004193433269392699 val_loss:0.0004203072458039969 quant_lr:5.1362214303938806e-05 norm:0.00054672 max memory_allocated 7499.798828125 time 25.60516929626465 \n",
      "\u001b[32m[2025-10-31 03:03:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.00038851535646244884 val_loss:0.00041323318146169186 quant_lr:5e-06 norm:0.00028395 max memory_allocated 7499.798828125 time 25.68065118789673 \n",
      "\u001b[32m[2025-10-31 03:03:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:03:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:04:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0006813718355260789 val_loss:0.0006743461126461625 quant_lr:5.1362214303938806e-05 norm:0.00095630 max memory_allocated 7499.798828125 time 25.767324686050415 \n",
      "\u001b[32m[2025-10-31 03:04:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0006182068027555943 val_loss:0.0006621041684411466 quant_lr:5e-06 norm:0.00048611 max memory_allocated 7499.798828125 time 25.7754647731781 \n",
      "\u001b[32m[2025-10-31 03:04:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:04:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:05:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0009949469240382314 val_loss:0.0009912598179653287 quant_lr:5.1362214303938806e-05 norm:0.00123340 max memory_allocated 7499.798828125 time 25.716077089309692 \n",
      "\u001b[32m[2025-10-31 03:05:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0008905598078854382 val_loss:0.0009703407995402813 quant_lr:5e-06 norm:0.00063696 max memory_allocated 7499.798828125 time 25.770795822143555 \n",
      "\u001b[32m[2025-10-31 03:05:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:05:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:05:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:05:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:05:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:06:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:06:01 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:06:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.001393969519995153 val_loss:0.0013837623409926891 quant_lr:5.1362214303938806e-05 norm:0.00159790 max memory_allocated 7499.798828125 time 25.702831268310547 \n",
      "\u001b[32m[2025-10-31 03:06:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.0012419995618984103 val_loss:0.0013546198606491089 quant_lr:5e-06 norm:0.00080142 max memory_allocated 7501.923828125 time 25.805842399597168 \n",
      "\u001b[32m[2025-10-31 03:07:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:07:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:07:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0018066010670736432 val_loss:0.0018031334038823843 quant_lr:5.1362214303938806e-05 norm:0.00160821 max memory_allocated 7501.923828125 time 25.72635817527771 \n",
      "\u001b[32m[2025-10-31 03:08:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0016115441685542464 val_loss:0.0017678914591670036 quant_lr:5e-06 norm:0.00085958 max memory_allocated 7501.923828125 time 25.77882218360901 \n",
      "\u001b[32m[2025-10-31 03:08:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:08:29 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:09:00 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0022953147999942303 val_loss:0.002299176063388586 quant_lr:5.1362214303938806e-05 norm:0.00204393 max memory_allocated 7501.923828125 time 25.728017807006836 \n",
      "\u001b[32m[2025-10-31 03:09:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0020566440653055906 val_loss:0.0022552539594471455 quant_lr:5e-06 norm:0.00103597 max memory_allocated 7501.923828125 time 25.863240718841553 \n",
      "\u001b[32m[2025-10-31 03:09:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:09:43 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:10:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.002709395717829466 val_loss:0.0027189403772354126 quant_lr:5.1362214303938806e-05 norm:0.00267520 max memory_allocated 7501.923828125 time 25.73365831375122 \n",
      "\u001b[32m[2025-10-31 03:10:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.002435063011944294 val_loss:0.002668107394129038 quant_lr:5e-06 norm:0.00127626 max memory_allocated 7501.923828125 time 25.84978461265564 \n",
      "\u001b[32m[2025-10-31 03:10:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:10:58 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:11:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.0032182331196963787 val_loss:0.003226436208933592 quant_lr:5.1362214303938806e-05 norm:0.00343303 max memory_allocated 7501.923828125 time 25.779863595962524 \n",
      "\u001b[32m[2025-10-31 03:11:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.002871050965040922 val_loss:0.0031625786796212196 quant_lr:5e-06 norm:0.00161786 max memory_allocated 7501.923828125 time 25.884674072265625 \n",
      "\u001b[32m[2025-10-31 03:12:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:12:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:12:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.0035955822095274925 val_loss:0.003669625148177147 quant_lr:5.1362214303938806e-05 norm:0.00293669 max memory_allocated 7501.923828125 time 25.73261284828186 \n",
      "\u001b[32m[2025-10-31 03:13:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.0032818021718412638 val_loss:0.003610644955188036 quant_lr:5e-06 norm:0.00150511 max memory_allocated 7501.923828125 time 25.877389907836914 \n",
      "\u001b[32m[2025-10-31 03:13:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:13:27 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:13:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0040246653370559216 val_loss:0.004069684073328972 quant_lr:5.1362214303938806e-05 norm:0.00375809 max memory_allocated 7501.923828125 time 25.758409023284912 \n",
      "\u001b[32m[2025-10-31 03:14:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.00366168562322855 val_loss:0.0040048942901194096 quant_lr:5e-06 norm:0.00170095 max memory_allocated 7501.923828125 time 25.85262894630432 \n",
      "\u001b[32m[2025-10-31 03:14:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:14:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:15:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.004352112300693989 val_loss:0.004433959256857634 quant_lr:5.1362214303938806e-05 norm:0.00500891 max memory_allocated 7501.923828125 time 25.783652544021606 \n",
      "\u001b[32m[2025-10-31 03:15:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.003981764428317547 val_loss:0.004369712900370359 quant_lr:5e-06 norm:0.00223491 max memory_allocated 7501.923828125 time 25.864962339401245 \n",
      "\u001b[32m[2025-10-31 03:15:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:15:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:16:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.005047103390097618 val_loss:0.005140943452715874 quant_lr:5.1362214303938806e-05 norm:0.00433462 max memory_allocated 7501.923828125 time 25.748870849609375 \n",
      "\u001b[32m[2025-10-31 03:16:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.00458266818895936 val_loss:0.00505510438233614 quant_lr:5e-06 norm:0.00244704 max memory_allocated 7501.923828125 time 25.891934156417847 \n",
      "\u001b[32m[2025-10-31 03:16:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:16:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:17:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:17:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.005684700328856707 val_loss:0.005773893557488918 quant_lr:5.1362214303938806e-05 norm:0.00442353 max memory_allocated 7501.923828125 time 25.742393493652344 \n",
      "\u001b[32m[2025-10-31 03:18:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.005164687521755695 val_loss:0.00567664485424757 quant_lr:5e-06 norm:0.00211942 max memory_allocated 7501.923828125 time 25.88651943206787 \n",
      "\u001b[32m[2025-10-31 03:18:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:18:25 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:18:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.006614168174564838 val_loss:0.006802392192184925 quant_lr:5.1362214303938806e-05 norm:0.00469761 max memory_allocated 7501.923828125 time 25.754955291748047 \n",
      "\u001b[32m[2025-10-31 03:19:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.006047793664038181 val_loss:0.006692684255540371 quant_lr:5e-06 norm:0.00293195 max memory_allocated 7501.923828125 time 25.80758833885193 \n",
      "\u001b[32m[2025-10-31 03:19:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:19:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:20:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.007636878173798323 val_loss:0.007991999387741089 quant_lr:5.1362214303938806e-05 norm:0.00564724 max memory_allocated 7501.923828125 time 25.802799940109253 \n",
      "\u001b[32m[2025-10-31 03:20:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.007050248794257641 val_loss:0.007880971767008305 quant_lr:5e-06 norm:0.00388206 max memory_allocated 7501.923828125 time 25.881609439849854 \n",
      "\u001b[32m[2025-10-31 03:20:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:20:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:21:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.009263778105378151 val_loss:0.009856880642473698 quant_lr:5.1362214303938806e-05 norm:0.00529367 max memory_allocated 7501.923828125 time 25.787083387374878 \n",
      "\u001b[32m[2025-10-31 03:21:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.008560500107705593 val_loss:0.009724198840558529 quant_lr:5e-06 norm:0.00362507 max memory_allocated 7501.923828125 time 25.888667345046997 \n",
      "\u001b[32m[2025-10-31 03:21:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:21:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:21:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:21:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:22:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:22:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:22:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:22:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:22:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.010924430564045906 val_loss:0.011936618015170097 quant_lr:5.1362214303938806e-05 norm:0.00462960 max memory_allocated 7501.923828125 time 25.755110263824463 \n",
      "\u001b[32m[2025-10-31 03:23:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.010232822969555855 val_loss:0.011777954176068306 quant_lr:5e-06 norm:0.00365031 max memory_allocated 7501.923828125 time 25.81856608390808 \n",
      "\u001b[32m[2025-10-31 03:23:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:23:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:23:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.012844053097069263 val_loss:0.014218674041330814 quant_lr:5.1362214303938806e-05 norm:0.00503763 max memory_allocated 7501.923828125 time 25.77582573890686 \n",
      "\u001b[32m[2025-10-31 03:24:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.012087397277355194 val_loss:0.014048541896045208 quant_lr:5e-06 norm:0.00371835 max memory_allocated 7501.923828125 time 25.8405282497406 \n",
      "\u001b[32m[2025-10-31 03:24:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:24:37 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:25:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.015203975141048431 val_loss:0.016975320875644684 quant_lr:5.1362214303938806e-05 norm:0.00469359 max memory_allocated 7501.923828125 time 25.757056713104248 \n",
      "\u001b[32m[2025-10-31 03:25:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.014398080296814442 val_loss:0.01680648699402809 quant_lr:5e-06 norm:0.00364883 max memory_allocated 7501.923828125 time 25.834885120391846 \n",
      "\u001b[32m[2025-10-31 03:25:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:25:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:26:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.019333234056830406 val_loss:0.021865956485271454 quant_lr:5.1362214303938806e-05 norm:0.00589245 max memory_allocated 7501.923828125 time 25.77283000946045 \n",
      "\u001b[32m[2025-10-31 03:26:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.018276432529091835 val_loss:0.02164747565984726 quant_lr:5e-06 norm:0.00445775 max memory_allocated 7501.923828125 time 25.814095735549927 \n",
      "\u001b[32m[2025-10-31 03:26:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:26:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:26:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:26:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:26:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:27:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:27:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:27:06 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:27:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.023403378203511238 val_loss:0.02700473554432392 quant_lr:5.1362214303938806e-05 norm:0.00556671 max memory_allocated 7501.923828125 time 25.78343677520752 \n",
      "\u001b[32m[2025-10-31 03:28:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.022313302382826805 val_loss:0.02679704688489437 quant_lr:5e-06 norm:0.00411013 max memory_allocated 7501.923828125 time 25.826528072357178 \n",
      "\u001b[32m[2025-10-31 03:28:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:28:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:28:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.028430070728063583 val_loss:0.03350990638136864 quant_lr:5.1362214303938806e-05 norm:0.00632740 max memory_allocated 7501.923828125 time 25.802712202072144 \n",
      "\u001b[32m[2025-10-31 03:29:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.027199847623705864 val_loss:0.03328055143356323 quant_lr:5e-06 norm:0.00500141 max memory_allocated 7501.923828125 time 25.874273777008057 \n",
      "\u001b[32m[2025-10-31 03:29:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:29:35 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:30:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.034142084419727325 val_loss:0.04065361246466637 quant_lr:5.1362214303938806e-05 norm:0.00606557 max memory_allocated 7501.923828125 time 25.774269342422485 \n",
      "\u001b[32m[2025-10-31 03:30:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.032775625586509705 val_loss:0.04043741524219513 quant_lr:5e-06 norm:0.00453617 max memory_allocated 7501.923828125 time 25.833078145980835 \n",
      "\u001b[32m[2025-10-31 03:30:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:30:50 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:31:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.04123493283987045 val_loss:0.04944649338722229 quant_lr:5.1362214303938806e-05 norm:0.00786318 max memory_allocated 7501.923828125 time 25.792983770370483 \n",
      "\u001b[32m[2025-10-31 03:31:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.03964084014296532 val_loss:0.04917227104306221 quant_lr:5e-06 norm:0.00595249 max memory_allocated 7501.923828125 time 25.8474543094635 \n",
      "\u001b[32m[2025-10-31 03:31:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:31:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:31:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:31:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:31:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:32:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:32:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:32:04 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:32:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.05005817860364914 val_loss:0.06065821275115013 quant_lr:5.1362214303938806e-05 norm:0.01040773 max memory_allocated 7501.923828125 time 25.746867656707764 \n",
      "\u001b[32m[2025-10-31 03:33:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.04808010533452034 val_loss:0.06023551523685455 quant_lr:5e-06 norm:0.00749573 max memory_allocated 7501.923828125 time 25.88052225112915 \n",
      "\u001b[32m[2025-10-31 03:33:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:33:19 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:33:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.06070992723107338 val_loss:0.07375132292509079 quant_lr:5.1362214303938806e-05 norm:0.01240434 max memory_allocated 7501.923828125 time 25.75022840499878 \n",
      "\u001b[32m[2025-10-31 03:34:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.05838590860366821 val_loss:0.07313644140958786 quant_lr:5e-06 norm:0.00997583 max memory_allocated 7501.923828125 time 25.87778115272522 \n",
      "\u001b[32m[2025-10-31 03:34:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:34:33 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:35:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.07522953301668167 val_loss:0.09120973944664001 quant_lr:5.1362214303938806e-05 norm:0.01754059 max memory_allocated 7501.923828125 time 25.746958255767822 \n",
      "\u001b[32m[2025-10-31 03:35:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.0722016841173172 val_loss:0.09052790701389313 quant_lr:5e-06 norm:0.01244665 max memory_allocated 7501.923828125 time 25.83553719520569 \n",
      "\u001b[32m[2025-10-31 03:35:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:35:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:36:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.09465202689170837 val_loss:0.11404528468847275 quant_lr:5.1362214303938806e-05 norm:0.02554888 max memory_allocated 7501.923828125 time 25.789560079574585 \n",
      "\u001b[32m[2025-10-31 03:36:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.09052693098783493 val_loss:0.11308709532022476 quant_lr:5e-06 norm:0.01896105 max memory_allocated 7501.923828125 time 25.926051139831543 \n",
      "\u001b[32m[2025-10-31 03:36:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:36:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:36:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:36:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:36:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:36:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:37:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:37:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:37:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.12787878513336182 val_loss:0.15212559700012207 quant_lr:5.1362214303938806e-05 norm:0.04687807 max memory_allocated 7501.923828125 time 25.760716199874878 \n",
      "\u001b[32m[2025-10-31 03:37:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.12105641514062881 val_loss:0.1504833698272705 quant_lr:5e-06 norm:0.03280710 max memory_allocated 7501.923828125 time 25.848350763320923 \n",
      "\u001b[32m[2025-10-31 03:38:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:38:17 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 03:38:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.27095580101013184 val_loss:0.3023039996623993 quant_lr:5.1362214303938806e-05 norm:0.22721006 max memory_allocated 7501.923828125 time 25.750131845474243 \n",
      "\u001b[32m[2025-10-31 03:39:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.24568206071853638 val_loss:0.29600876569747925 quant_lr:5e-06 norm:0.15018418 max memory_allocated 7501.923828125 time 25.842077493667603 \n",
      "\u001b[32m[2025-10-31 03:39:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 03:39:32 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2404.3040528297424\n",
      "\u001b[32m[2025-10-31 03:39:32 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-31 03:39:44 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 141/141 [02:28<00:00,  1.05s/it]\n",
      "wikitext2:29.145849227905273\n",
      "\u001b[32m[2025-10-31 03:42:22 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 29.15\n",
      "2025-10-31 03:42:24.859384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761882145.162766    2820 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761882145.243453    2820 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761882145.862901    2820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761882145.862962    2820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761882145.862972    2820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761882145.862980    2820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 17.2MB/s]\n",
      "\u001b[32m[2025-10-31 03:42:34 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 03:42:35 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 03:42:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 19.0MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 29.4MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:01<00:00, 1.64MB/s]\n",
      "Downloading data: 815kB [00:00, 79.5MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 34731.52 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37199.61 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 34659.46 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 33.1MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.27MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.38MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 345kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 381423.60 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 479510.48 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 258767.54 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 22.8MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 39.2MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 11.2MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 11.4MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 242765.50 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 250696.86 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 254656.71 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 5798.47 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6371.10 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 6.43MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 3.72MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 192kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 135kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1162987.70 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 607088.40 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 534572.29 examples/s]\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 125803.30it/s]\n",
      "\u001b[32m[2025-10-31 03:43:32 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2204.83it/s]\n",
      "\u001b[32m[2025-10-31 03:43:38 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1024.34it/s]\n",
      "\u001b[32m[2025-10-31 03:43:41 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1005.01it/s]\n",
      "\u001b[32m[2025-10-31 03:43:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:01<00:00, 25.85it/s]\n",
      "\u001b[32m[2025-10-31 04:20:25 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5446|±  |0.0140|\n",
      "|hellaswag |      1|none  |     0|acc     |0.3896|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.4759|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.4339|±  |0.0102|\n",
      "|          |       |none  |     0|acc_norm|0.3948|±  |0.0100|\n",
      "|piqa      |      1|none  |     0|acc     |0.6164|±  |0.0113|\n",
      "|          |       |none  |     0|acc_norm|0.6208|±  |0.0113|\n",
      "\n",
      "\u001b[32m[2025-10-31 04:20:25 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 49.61%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "     --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 2 \\\n",
    "       --group_size 64 \\\n",
    "        --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "       --output_dir ./output/llama3_baseline4 \\\n",
    "       --save_quant_dir ./output/llama3_baseline4/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "PiH7gVjif68m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5600172,
     "status": "ok",
     "timestamp": 1761894161613,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "PiH7gVjif68m",
    "outputId": "15d0f8b8-0de3-4b8e-a0cb-dcab1568757f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '32', '--use_mixed_precision', '--mpq_strategy', 'aggressive', '--target_avg_bits', '2.5', '--use_adaptive_training', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/llama3_optimized_2.0bit', '--save_quant_dir', './output/llama3_optimized_2.0bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: aggressive\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.5\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_2.0bit', save_quant_dir='./output/llama3_optimized_2.0bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='aggressive', target_avg_bits=2.5, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-31 05:29:28 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 654/654 [00:00<00:00, 4.76MB/s]\n",
      "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 9.01MB/s]\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 18.5MB/s]\n",
      "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 740kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 80.5MB/s]\n",
      "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1% 41.7M/4.98G [00:01<02:31, 32.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5% 224M/4.98G [00:01<00:31, 152MB/s]  \u001b[A\n",
      "model-00001-of-00004.safetensors:  20% 987M/4.98G [00:01<00:04, 826MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [00:02<00:04, 894MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31% 1.53G/4.98G [00:02<00:05, 605MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34% 1.69G/4.98G [00:03<00:07, 458MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37% 1.84G/4.98G [00:04<00:09, 331MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [00:05<00:10, 278MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43% 2.14G/4.98G [00:05<00:07, 365MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46% 2.27G/4.98G [00:05<00:06, 405MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [00:05<00:06, 415MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50% 2.50G/4.98G [00:05<00:04, 528MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54% 2.70G/4.98G [00:06<00:04, 540MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [00:06<00:03, 564MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59% 2.92G/4.98G [00:06<00:03, 537MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60% 3.00G/4.98G [00:06<00:03, 528MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63% 3.11G/4.98G [00:07<00:03, 485MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [00:09<00:12, 136MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69% 3.41G/4.98G [00:09<00:07, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72% 3.58G/4.98G [00:09<00:04, 315MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75% 3.72G/4.98G [00:09<00:03, 399MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78% 3.88G/4.98G [00:09<00:02, 458MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81% 4.01G/4.98G [00:09<00:01, 543MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84% 4.18G/4.98G [00:10<00:01, 622MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87% 4.32G/4.98G [00:10<00:01, 577MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89% 4.45G/4.98G [00:10<00:01, 486MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91% 4.52G/4.98G [00:11<00:01, 455MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92% 4.58G/4.98G [00:11<00:00, 428MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93% 4.65G/4.98G [00:13<00:02, 114MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:13<00:00, 368MB/s]\n",
      "Downloading shards:  25% 1/4 [00:14<00:42, 14.07s/it]\n",
      "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0% 34.5k/5.00G [00:01<48:45:20, 28.5kB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7% 361M/5.00G [00:01<00:18, 249MB/s]     \u001b[A\n",
      "model-00002-of-00004.safetensors:  19% 935M/5.00G [00:02<00:06, 616MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22% 1.10G/5.00G [00:02<00:06, 648MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25% 1.24G/5.00G [00:02<00:06, 571MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26% 1.32G/5.00G [00:02<00:06, 537MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29% 1.43G/5.00G [00:03<00:07, 490MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30% 1.52G/5.00G [00:03<00:07, 461MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32% 1.61G/5.00G [00:03<00:06, 503MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:05<00:26, 126MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35% 1.77G/5.00G [00:06<00:24, 130MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:06<00:05, 475MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54% 2.67G/5.00G [00:06<00:04, 570MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57% 2.87G/5.00G [00:07<00:03, 547MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61% 3.03G/5.00G [00:07<00:03, 525MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63% 3.17G/5.00G [00:07<00:03, 506MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65% 3.26G/5.00G [00:09<00:09, 178MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69% 3.45G/5.00G [00:10<00:06, 253MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72% 3.60G/5.00G [00:10<00:04, 324MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75% 3.76G/5.00G [00:10<00:02, 414MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78% 3.91G/5.00G [00:10<00:02, 486MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81% 4.06G/5.00G [00:10<00:01, 569MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85% 4.25G/5.00G [00:10<00:01, 569MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87% 4.36G/5.00G [00:11<00:01, 520MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90% 4.50G/5.00G [00:11<00:01, 498MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93% 4.63G/5.00G [00:11<00:00, 506MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94% 4.70G/5.00G [00:11<00:00, 447MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [00:12<00:00, 354MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [00:12<00:00, 332MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:12<00:00, 385MB/s]\n",
      "Downloading shards:  50% 2/4 [00:27<00:27, 13.54s/it]\n",
      "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0% 407k/4.92G [00:01<3:52:54, 352kB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8% 404M/4.92G [00:01<00:13, 338MB/s]  \u001b[A\n",
      "model-00003-of-00004.safetensors:  15% 758M/4.92G [00:01<00:06, 680MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20% 967M/4.92G [00:01<00:05, 670MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23% 1.15G/4.92G [00:02<00:07, 532MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26% 1.26G/4.92G [00:04<00:20, 178MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30% 1.47G/4.92G [00:04<00:13, 257MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34% 1.65G/4.92G [00:05<00:11, 296MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43% 2.09G/4.92G [00:05<00:05, 542MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46% 2.27G/4.92G [00:05<00:04, 618MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49% 2.42G/4.92G [00:05<00:04, 622MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52% 2.58G/4.92G [00:06<00:04, 540MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55% 2.72G/4.92G [00:06<00:04, 537MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [00:06<00:04, 492MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59% 2.89G/4.92G [00:08<00:13, 154MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63% 3.09G/4.92G [00:09<00:07, 242MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65% 3.19G/4.92G [00:09<00:05, 294MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69% 3.39G/4.92G [00:09<00:03, 415MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72% 3.56G/4.92G [00:09<00:02, 486MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75% 3.70G/4.92G [00:09<00:02, 580MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79% 3.88G/4.92G [00:09<00:01, 735MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82% 4.01G/4.92G [00:10<00:01, 615MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84% 4.14G/4.92G [00:10<00:01, 532MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87% 4.27G/4.92G [00:10<00:01, 471MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89% 4.39G/4.92G [00:11<00:01, 431MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91% 4.46G/4.92G [00:11<00:01, 395MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92% 4.53G/4.92G [00:11<00:01, 346MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93% 4.60G/4.92G [00:11<00:00, 329MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95% 4.65G/4.92G [00:12<00:00, 322MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96% 4.71G/4.92G [00:12<00:00, 298MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97% 4.78G/4.92G [00:12<00:00, 293MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:12<00:00, 383MB/s]\n",
      "Downloading shards:  75% 3/4 [00:40<00:13, 13.37s/it]\n",
      "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6% 67.1M/1.17G [00:00<00:16, 68.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14% 164M/1.17G [00:01<00:05, 175MB/s]  \u001b[A\n",
      "model-00004-of-00004.safetensors:  26% 298M/1.17G [00:01<00:02, 343MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42% 489M/1.17G [00:01<00:01, 574MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  53% 623M/1.17G [00:01<00:00, 707MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  71% 824M/1.17G [00:01<00:00, 935MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87% 1.01G/1.17G [00:02<00:00, 520MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:02<00:00, 477MB/s]\n",
      "Downloading shards: 100% 4/4 [00:43<00:00, 10.77s/it]\n",
      "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n",
      "generation_config.json: 100% 177/177 [00:00<00:00, 1.40MB/s]\n",
      "\u001b[32m[2025-10-31 05:30:19 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 29.5MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 2.58MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:00<00:00, 27.5MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 2.38MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 106423.55 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 740607.92 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 592233.39 examples/s]\n",
      "\u001b[32m[2025-10-31 05:30:37 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-31 05:30:40 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: aggressive, Target Avg: 2.5 bits\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [4, 6, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4]\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.53 bits\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 6.32x vs FP16\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-31 05:30:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-31 05:31:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 5.57e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 05:31:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 2.07e-05 | Time: 27.2s\n",
      "\u001b[32m[2025-10-31 05:32:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000001 | Val Loss: 0.000001 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 27.3s\n",
      "\u001b[32m[2025-10-31 05:32:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-31 05:32:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-31 05:32:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-31 05:32:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000045 | Val Loss: 0.000050 | Grad Norm: 0.08 | LR: 5.71e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:33:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000075 | Val Loss: 0.000028 | Grad Norm: 0.09 | LR: 3.46e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:33:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000016 | Val Loss: 0.000014 | Grad Norm: 0.04 | LR: 1.23e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:34:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ⚠️  Block 1 Epoch 3/4 | Train Loss: 0.000009 | Val Loss: 0.000014 | Grad Norm: 0.02 | LR: 3.33e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:34:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-31 05:34:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-31 05:34:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-31 05:35:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000019 | Val Loss: 0.000024 | Grad Norm: 0.00 | LR: 5.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:35:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000018 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 2.11e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:36:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000018 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-31 05:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-31 05:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:36:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000037 | Val Loss: 0.000043 | Grad Norm: 0.00 | LR: 5.75e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:37:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000035 | Val Loss: 0.000042 | Grad Norm: 0.00 | LR: 2.14e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:37:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000034 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:38:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-31 05:38:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-31 05:38:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:38:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000061 | Val Loss: 0.000068 | Grad Norm: 0.00 | LR: 5.88e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:39:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000058 | Val Loss: 0.000066 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:39:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000055 | Val Loss: 0.000065 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:39:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-31 05:39:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-31 05:39:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:40:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000175 | Val Loss: 0.000171 | Grad Norm: 0.00 | LR: 5.93e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:40:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000152 | Val Loss: 0.000163 | Grad Norm: 0.00 | LR: 2.20e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:41:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000146 | Val Loss: 0.000161 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:41:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-31 05:41:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-31 05:41:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:42:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000290 | Val Loss: 0.000287 | Grad Norm: 0.00 | LR: 6.02e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:42:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000256 | Val Loss: 0.000276 | Grad Norm: 0.00 | LR: 2.24e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:43:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000246 | Val Loss: 0.000272 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:43:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-31 05:43:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-31 05:43:27 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:44:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000453 | Val Loss: 0.000445 | Grad Norm: 0.00 | LR: 6.16e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:44:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000396 | Val Loss: 0.000425 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:44:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000382 | Val Loss: 0.000421 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:45:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-31 05:45:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:45:13 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:45:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.001669 | Val Loss: 0.001363 | Grad Norm: 0.00 | LR: 6.25e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:46:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.001210 | Val Loss: 0.001208 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:46:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.001128 | Val Loss: 0.001178 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-31 05:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:47:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.002534 | Val Loss: 0.002177 | Grad Norm: 0.00 | LR: 6.31e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 05:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.001950 | Val Loss: 0.001968 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 05:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.001840 | Val Loss: 0.001931 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:48:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-31 05:48:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:48:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:49:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.003027 | Val Loss: 0.002749 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:49:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.002510 | Val Loss: 0.002572 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:50:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.002412 | Val Loss: 0.002539 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:50:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-31 05:50:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:50:33 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:51:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.003678 | Val Loss: 0.003362 | Grad Norm: 0.00 | LR: 6.44e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:51:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.003070 | Val Loss: 0.003158 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:52:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.002958 | Val Loss: 0.003113 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:52:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-31 05:52:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:52:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:52:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.004124 | Val Loss: 0.003813 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:53:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.003508 | Val Loss: 0.003619 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:53:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.003397 | Val Loss: 0.003578 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:54:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-31 05:54:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:54:05 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:54:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.005003 | Val Loss: 0.004678 | Grad Norm: 0.01 | LR: 6.39e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 05:55:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.004287 | Val Loss: 0.004422 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:55:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.004141 | Val Loss: 0.004372 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:55:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-31 05:55:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:55:51 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 05:56:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.005878 | Val Loss: 0.005544 | Grad Norm: 0.01 | LR: 6.49e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 05:56:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.005069 | Val Loss: 0.005243 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:57:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.004899 | Val Loss: 0.005174 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:57:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-31 05:57:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:57:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:58:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.007019 | Val Loss: 0.006683 | Grad Norm: 0.01 | LR: 6.42e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:58:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.006121 | Val Loss: 0.006373 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 05:59:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.005935 | Val Loss: 0.006309 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 05:59:24 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-31 05:59:24 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 05:59:24 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 05:59:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.008155 | Val Loss: 0.007926 | Grad Norm: 0.01 | LR: 6.47e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:00:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.007266 | Val Loss: 0.007615 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:00:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.007073 | Val Loss: 0.007550 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:01:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-31 06:01:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:01:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:01:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.009994 | Val Loss: 0.009870 | Grad Norm: 0.01 | LR: 6.62e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:02:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.008973 | Val Loss: 0.009460 | Grad Norm: 0.00 | LR: 2.46e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:02:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.008723 | Val Loss: 0.009378 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:02:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-31 06:02:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:02:56 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:03:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.011872 | Val Loss: 0.011950 | Grad Norm: 0.01 | LR: 4.67e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:03:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.010985 | Val Loss: 0.011730 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:04:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-31 06:04:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:04:15 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:04:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.014347 | Val Loss: 0.014528 | Grad Norm: 0.01 | LR: 4.77e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:05:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.013395 | Val Loss: 0.014286 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:05:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-31 06:05:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:05:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:06:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.017373 | Val Loss: 0.017683 | Grad Norm: 0.01 | LR: 4.86e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.016349 | Val Loss: 0.017422 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:06:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-31 06:06:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:06:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:07:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.022388 | Val Loss: 0.022945 | Grad Norm: 0.01 | LR: 4.77e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:07:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.021106 | Val Loss: 0.022621 | Grad Norm: 0.01 | LR: 4.64e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:08:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-31 06:08:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:08:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:08:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.027459 | Val Loss: 0.028418 | Grad Norm: 0.01 | LR: 4.79e-05 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:09:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.026142 | Val Loss: 0.028103 | Grad Norm: 0.01 | LR: 4.67e-06 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:09:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-31 06:09:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:09:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:10:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.033798 | Val Loss: 0.035208 | Grad Norm: 0.01 | LR: 5.00e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:10:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.032278 | Val Loss: 0.034864 | Grad Norm: 0.01 | LR: 4.87e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:10:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-31 06:10:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:10:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:11:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.041035 | Val Loss: 0.042945 | Grad Norm: 0.01 | LR: 5.06e-05 | Time: 27.4s\n",
      "\u001b[32m[2025-10-31 06:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.039362 | Val Loss: 0.042554 | Grad Norm: 0.01 | LR: 4.92e-06 | Time: 27.5s\n",
      "\u001b[32m[2025-10-31 06:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-31 06:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:12:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.050272 | Val Loss: 0.052661 | Grad Norm: 0.01 | LR: 5.08e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:13:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.048259 | Val Loss: 0.052215 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:13:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-31 06:13:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:13:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:14:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.061657 | Val Loss: 0.064642 | Grad Norm: 0.01 | LR: 5.09e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-31 06:14:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.059188 | Val Loss: 0.064085 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:14:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-31 06:14:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:14:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:15:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.075616 | Val Loss: 0.079361 | Grad Norm: 0.02 | LR: 5.09e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:15:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.072685 | Val Loss: 0.078661 | Grad Norm: 0.01 | LR: 4.96e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:16:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-31 06:16:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:16:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:16:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.094812 | Val Loss: 0.099083 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:17:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.090864 | Val Loss: 0.098162 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:17:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-31 06:17:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:17:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:17:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.121019 | Val Loss: 0.126081 | Grad Norm: 0.03 | LR: 4.95e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-31 06:18:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.115533 | Val Loss: 0.124805 | Grad Norm: 0.02 | LR: 4.82e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:18:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-31 06:18:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 06:18:45 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 06:19:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.166643 | Val Loss: 0.171647 | Grad Norm: 0.06 | LR: 4.61e-05 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:19:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.156954 | Val Loss: 0.169453 | Grad Norm: 0.04 | LR: 4.49e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:20:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-31 06:20:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-31 06:20:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 06:20:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.278482 | Val Loss: 0.311418 | Grad Norm: 0.30 | LR: 5.67e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-31 06:21:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.264375 | Val Loss: 0.304969 | Grad Norm: 0.28 | LR: 2.11e-05 | Time: 27.7s\n",
      "\u001b[32m[2025-10-31 06:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.259764 | Val Loss: 0.303134 | Grad Norm: 0.28 | LR: 3.75e-06 | Time: 27.6s\n",
      "\u001b[32m[2025-10-31 06:21:51 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_2.0bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 06:21:51 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_2.0bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 06:21:51 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3092.06s (51.53min)\n",
      "\u001b[32m[2025-10-31 06:21:51 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-31 06:22:05 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_2.0bit/model\n",
      "\u001b[32m[2025-10-31 06:22:05 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:28<00:00,  1.06s/it]\n",
      "wikitext2:29.382177352905273\n",
      "\u001b[32m[2025-10-31 06:24:39 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 29.38\n",
      "2025-10-31 06:24:41.490819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761891881.728115    2559 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761891881.795847    2559 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761891882.301810    2559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761891882.301854    2559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761891882.301859    2559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761891882.301865    2559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 12.7MB/s]\n",
      "\u001b[32m[2025-10-31 06:24:49 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 06:24:50 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 06:24:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 19.0MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 28.0MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:00<00:00, 83.2MB/s]\n",
      "Downloading data: 815kB [00:00, 39.9MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 34357.78 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37389.17 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 35269.80 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 32.4MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.99MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 3.07MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 784kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 386941.73 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 481479.67 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 263618.18 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 24.0MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 78.6MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 11.8MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 29.4MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 247007.77 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 262417.82 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 241678.24 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 5935.84 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6573.86 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 42.7MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 9.45MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 591kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 309kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1182755.08 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 690389.86 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 540114.15 examples/s]\n",
      "\u001b[32m[2025-10-31 06:25:25 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 06:25:25 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 06:25:25 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 06:25:25 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 06:25:25 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 118882.87it/s]\n",
      "\u001b[32m[2025-10-31 06:25:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2211.58it/s]\n",
      "\u001b[32m[2025-10-31 06:25:31 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1049.83it/s]\n",
      "\u001b[32m[2025-10-31 06:25:34 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1023.18it/s]\n",
      "\u001b[32m[2025-10-31 06:25:36 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:22<00:00, 25.60it/s]\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5659|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4145|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5323|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.5160|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4819|±  |0.0103|\n",
      "|piqa      |      1|none  |     0|acc     |0.6801|±  |0.0109|\n",
      "|          |       |none  |     0|acc_norm|0.6725|±  |0.0109|\n",
      "\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 54.41%\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_2.0bit/results.json\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-31 07:02:37 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 128 \\\n",
    "         --val_size 32 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy aggressive \\\n",
    "         --target_avg_bits 2.5 \\\n",
    "         --use_adaptive_training \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_2.0bit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_2.0bit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "WMl0BbYkE__f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6982433,
     "status": "ok",
     "timestamp": 1761908155497,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "WMl0BbYkE__f",
    "outputId": "a8f50e15-54c7-4443-d57a-c1fb5c9fced8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '256', '--val_size', '64', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '2.0', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/llama3_optimized_2_256mpqbit', '--save_quant_dir', './output/llama3_optimized_2_256mpqbit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.0\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_2_256mpqbit', save_quant_dir='./output/llama3_optimized_2_256mpqbit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=2.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-31 08:59:37 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.03it/s]\n",
      "\u001b[32m[2025-10-31 08:59:42 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-31 08:59:42 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_train.cache\n",
      "\u001b[32m[2025-10-31 08:59:42 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_val.cache\n",
      "\u001b[32m[2025-10-31 08:59:42 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-31 08:59:47 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 2.0 bits\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.03 bits\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 7.88x vs FP16\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-31 08:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:00:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000026 | Val Loss: 0.000025 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 53.6s\n",
      "\u001b[32m[2025-10-31 09:01:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000023 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 53.6s\n",
      "\u001b[32m[2025-10-31 09:02:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-31 09:02:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-31 09:03:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000197 | Val Loss: 0.000248 | Grad Norm: 0.02 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 09:04:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000090 | Val Loss: 0.000212 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 09:04:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-31 09:04:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:05:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000201 | Val Loss: 0.000327 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.0s\n",
      "\u001b[32m[2025-10-31 09:06:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000187 | Val Loss: 0.000320 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.0s\n",
      "\u001b[32m[2025-10-31 09:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-31 09:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:07:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000413 | Val Loss: 0.000546 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:08:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000387 | Val Loss: 0.000532 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:09:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-31 09:09:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:10:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000676 | Val Loss: 0.000820 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.1s\n",
      "\u001b[32m[2025-10-31 09:10:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000630 | Val Loss: 0.000799 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.1s\n",
      "\u001b[32m[2025-10-31 09:11:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-31 09:11:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:12:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.001015 | Val Loss: 0.001177 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:13:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000948 | Val Loss: 0.001145 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.0s\n",
      "\u001b[32m[2025-10-31 09:13:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-31 09:13:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 09:14:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.001369 | Val Loss: 0.001557 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.001286 | Val Loss: 0.001516 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 53.9s\n",
      "\u001b[32m[2025-10-31 09:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-31 09:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:17:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.001859 | Val Loss: 0.002062 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:18:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.001751 | Val Loss: 0.002009 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.0s\n",
      "\u001b[32m[2025-10-31 09:18:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-31 09:18:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:19:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.002279 | Val Loss: 0.002493 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:20:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.002156 | Val Loss: 0.002437 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.1s\n",
      "\u001b[32m[2025-10-31 09:20:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-31 09:20:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:21:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.002751 | Val Loss: 0.002984 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:22:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.002606 | Val Loss: 0.002918 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.1s\n",
      "\u001b[32m[2025-10-31 09:23:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-31 09:23:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:24:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.003167 | Val Loss: 0.003432 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:25:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.003024 | Val Loss: 0.003365 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:25:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-31 09:25:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:26:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.003557 | Val Loss: 0.003834 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:27:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.003411 | Val Loss: 0.003765 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:27:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-31 09:27:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:28:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.003901 | Val Loss: 0.004189 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:29:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.003737 | Val Loss: 0.004113 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:30:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-31 09:30:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:31:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.004558 | Val Loss: 0.004840 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:32:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.004361 | Val Loss: 0.004752 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:32:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-31 09:32:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:33:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.005207 | Val Loss: 0.005488 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 09:34:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.004981 | Val Loss: 0.005386 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:35:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-31 09:35:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:36:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.006182 | Val Loss: 0.006502 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:37:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.005876 | Val Loss: 0.006364 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.1s\n",
      "\u001b[32m[2025-10-31 09:37:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-31 09:37:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:38:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.007238 | Val Loss: 0.007658 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:39:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.006897 | Val Loss: 0.007509 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:39:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-31 09:39:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:40:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.008854 | Val Loss: 0.009422 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:41:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.008439 | Val Loss: 0.009244 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:42:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-31 09:42:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:43:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.010598 | Val Loss: 0.011294 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:44:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.010174 | Val Loss: 0.011126 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:44:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-31 09:44:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:45:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.012507 | Val Loss: 0.013436 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:46:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.012035 | Val Loss: 0.013237 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-31 09:47:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:48:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.014881 | Val Loss: 0.015984 | Grad Norm: 0.00 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 09:49:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.014379 | Val Loss: 0.015784 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 09:49:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-31 09:49:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:50:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.019052 | Val Loss: 0.020667 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:51:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.018325 | Val Loss: 0.020365 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:51:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-31 09:51:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:52:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.023183 | Val Loss: 0.025435 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:53:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.022416 | Val Loss: 0.025146 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 09:54:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-31 09:54:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:55:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.028340 | Val Loss: 0.031414 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.6s\n",
      "\u001b[32m[2025-10-31 09:56:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.027415 | Val Loss: 0.031059 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:56:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-31 09:56:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 09:57:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.034089 | Val Loss: 0.038250 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 09:58:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.033061 | Val Loss: 0.037864 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 09:58:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-31 09:58:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:00:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.041177 | Val Loss: 0.046497 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 10:00:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.039993 | Val Loss: 0.046063 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 10:01:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-31 10:01:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:02:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.049885 | Val Loss: 0.056829 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 10:03:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.048491 | Val Loss: 0.056257 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 10:03:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-31 10:03:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:04:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.060564 | Val Loss: 0.069222 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 10:05:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.058918 | Val Loss: 0.068552 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 10:06:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-31 10:06:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:07:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.074828 | Val Loss: 0.085971 | Grad Norm: 0.02 | LR: 5.19e-05 | Time: 54.6s\n",
      "\u001b[32m[2025-10-31 10:08:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.072817 | Val Loss: 0.085135 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 10:08:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-31 10:08:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:09:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.094080 | Val Loss: 0.108036 | Grad Norm: 0.03 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 10:10:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.091505 | Val Loss: 0.106944 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 10:10:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-31 10:10:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-31 10:12:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.127102 | Val Loss: 0.145363 | Grad Norm: 0.05 | LR: 5.19e-05 | Time: 54.4s\n",
      "\u001b[32m[2025-10-31 10:12:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.123194 | Val Loss: 0.143767 | Grad Norm: 0.04 | LR: 5.00e-06 | Time: 54.2s\n",
      "\u001b[32m[2025-10-31 10:13:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-31 10:13:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-31 10:14:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.257895 | Val Loss: 0.304926 | Grad Norm: 0.22 | LR: 5.19e-05 | Time: 54.5s\n",
      "\u001b[32m[2025-10-31 10:15:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.244043 | Val Loss: 0.298710 | Grad Norm: 0.15 | LR: 5.00e-06 | Time: 54.3s\n",
      "\u001b[32m[2025-10-31 10:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_2_256mpqbit/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 10:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_2_256mpqbit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 10:15:42 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 4560.21s (76.00min)\n",
      "\u001b[32m[2025-10-31 10:15:42 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-31 10:15:55 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_2_256mpqbit/model\n",
      "\u001b[32m[2025-10-31 10:15:55 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:24<00:00,  1.03s/it]\n",
      "wikitext2:31.184154510498047\n",
      "\u001b[32m[2025-10-31 10:18:25 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 31.18\n",
      "2025-10-31 10:18:26.211498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761905906.234548   55593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761905906.240983   55593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761905906.261297   55593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761905906.261326   55593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761905906.261332   55593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761905906.261335   55593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-31 10:18:32 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 10:18:33 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 10:18:37 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 129573.14it/s]\n",
      "\u001b[32m[2025-10-31 10:18:53 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2230.77it/s]\n",
      "\u001b[32m[2025-10-31 10:18:59 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1041.47it/s]\n",
      "\u001b[32m[2025-10-31 10:19:01 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1004.44it/s]\n",
      "\u001b[32m[2025-10-31 10:19:03 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:09<00:00, 25.75it/s]\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5556|±  |0.0140|\n",
      "|hellaswag |      1|none  |     0|acc     |0.3774|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.4605|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.4554|±  |0.0102|\n",
      "|          |       |none  |     0|acc_norm|0.4036|±  |0.0101|\n",
      "|piqa      |      1|none  |     0|acc     |0.6306|±  |0.0113|\n",
      "|          |       |none  |     0|acc_norm|0.6273|±  |0.0113|\n",
      "\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 50.48%\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_2_256mpqbit/results.json\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-31 10:55:52 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 256 \\\n",
    "         --val_size 64 \\\n",
    "         --use_mixed_precision \\\n",
    "         --mpq_strategy adaptive \\\n",
    "         --target_avg_bits 2.0 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_2_256mpqbit\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_2_256mpqbit/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "BrNyQYc0veeT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6700071,
     "status": "ok",
     "timestamp": 1761900923691,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "BrNyQYc0veeT",
    "outputId": "7985f4ff-d739-4bfa-d06b-850078a56730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--calib_dataset', 'wikitext2', '--train_size', '256', '--val_size', '16', '--wbits', '2', '--group_size', '64', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--output_dir', './output/llama3_baseline256', '--save_quant_dir', './output/llama3_baseline256/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 07:03:48 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_baseline256', save_quant_dir='./output/llama3_baseline256/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=64, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-31 07:03:48 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:13<00:00,  3.30s/it]\n",
      "\u001b[32m[2025-10-31 07:04:02 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "get_wikitext2\n",
      "\u001b[32m[2025-10-31 07:04:18 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-31 07:04:22 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-31 07:04:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:05:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:2.180611772928387e-05 val_loss:2.1367904992075637e-05 quant_lr:5.192416827251822e-05 norm:0.00026492 max memory_allocated 7437.673828125 time 49.96047377586365 \n",
      "\u001b[32m[2025-10-31 07:06:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:1.961147427209653e-05 val_loss:2.0404579117894173e-05 quant_lr:5e-06 norm:0.00012966 max memory_allocated 7499.673828125 time 49.984435081481934 \n",
      "\u001b[32m[2025-10-31 07:06:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:06:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:07:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0003340070543345064 val_loss:0.0003636165929492563 quant_lr:5.192416827251822e-05 norm:0.02790756 max memory_allocated 7499.673828125 time 50.048750162124634 \n",
      "\u001b[32m[2025-10-31 07:08:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.00023020732623990625 val_loss:0.0002410418528597802 quant_lr:5e-06 norm:0.01054124 max memory_allocated 7499.673828125 time 50.37610340118408 \n",
      "\u001b[32m[2025-10-31 07:08:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:08:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:09:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.0003422681475058198 val_loss:0.00036058557452633977 quant_lr:5.192416827251822e-05 norm:0.00068736 max memory_allocated 7499.673828125 time 50.19321632385254 \n",
      "\u001b[32m[2025-10-31 07:10:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.0003215302131138742 val_loss:0.0003479769511613995 quant_lr:5e-06 norm:0.00027533 max memory_allocated 7500.048828125 time 50.5270254611969 \n",
      "\u001b[32m[2025-10-31 07:10:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:10:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:11:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0005455242935568094 val_loss:0.0005715437000617385 quant_lr:5.192416827251822e-05 norm:0.00069308 max memory_allocated 7500.048828125 time 50.2252984046936 \n",
      "\u001b[32m[2025-10-31 07:12:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0005106836324557662 val_loss:0.0005521976854652166 quant_lr:5e-06 norm:0.00042595 max memory_allocated 7500.048828125 time 50.60944747924805 \n",
      "\u001b[32m[2025-10-31 07:12:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:12:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:12:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:12:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:12:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:13:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:13:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:13:06 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:14:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0007797456346452236 val_loss:0.0008220127783715725 quant_lr:5.192416827251822e-05 norm:0.00091176 max memory_allocated 7500.048828125 time 50.135953426361084 \n",
      "\u001b[32m[2025-10-31 07:14:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0007299756980501115 val_loss:0.0007955386536195874 quant_lr:5e-06 norm:0.00054673 max memory_allocated 7500.048828125 time 50.51334309577942 \n",
      "\u001b[32m[2025-10-31 07:15:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:15:17 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:16:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0010857669403776526 val_loss:0.001155579462647438 quant_lr:5.192416827251822e-05 norm:0.00123029 max memory_allocated 7500.048828125 time 50.22713232040405 \n",
      "\u001b[32m[2025-10-31 07:17:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.0010207612067461014 val_loss:0.0011197320418432355 quant_lr:5e-06 norm:0.00073513 max memory_allocated 7500.048828125 time 50.46273636817932 \n",
      "\u001b[32m[2025-10-31 07:17:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:17:29 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:18:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.001401259796693921 val_loss:0.001498876023106277 quant_lr:5.192416827251822e-05 norm:0.00126906 max memory_allocated 7500.048828125 time 50.25736665725708 \n",
      "\u001b[32m[2025-10-31 07:19:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0013237881939858198 val_loss:0.0014589871279895306 quant_lr:5e-06 norm:0.00077557 max memory_allocated 7500.048828125 time 50.55457377433777 \n",
      "\u001b[32m[2025-10-31 07:19:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:19:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:20:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0017833528108894825 val_loss:0.0019128179410472512 quant_lr:5.192416827251822e-05 norm:0.00154562 max memory_allocated 7500.048828125 time 50.35171556472778 \n",
      "\u001b[32m[2025-10-31 07:21:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0016936854226514697 val_loss:0.0018703305395320058 quant_lr:5e-06 norm:0.00091108 max memory_allocated 7500.048828125 time 50.87279725074768 \n",
      "\u001b[32m[2025-10-31 07:21:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:21:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:22:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.0021123657934367657 val_loss:0.002267617266625166 quant_lr:5.192416827251822e-05 norm:0.00206490 max memory_allocated 7500.048828125 time 50.667150259017944 \n",
      "\u001b[32m[2025-10-31 07:23:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.0020109941251575947 val_loss:0.002218168694525957 quant_lr:5e-06 norm:0.00112544 max memory_allocated 7500.048828125 time 50.57107710838318 \n",
      "\u001b[32m[2025-10-31 07:23:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:23:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:23:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:23:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:24:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:24:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:24:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:24:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:25:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.0024857986718416214 val_loss:0.0026883354876190424 quant_lr:5.192416827251822e-05 norm:0.00261974 max memory_allocated 7500.048828125 time 50.47871661186218 \n",
      "\u001b[32m[2025-10-31 07:25:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.002373672090470791 val_loss:0.0026345099322497845 quant_lr:5e-06 norm:0.00142478 max memory_allocated 7500.048828125 time 50.46025013923645 \n",
      "\u001b[32m[2025-10-31 07:26:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:26:19 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:27:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.002835865132510662 val_loss:0.0030783500988036394 quant_lr:5.192416827251822e-05 norm:0.00244187 max memory_allocated 7500.048828125 time 50.33227753639221 \n",
      "\u001b[32m[2025-10-31 07:28:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.002719400217756629 val_loss:0.003025813726708293 quant_lr:5e-06 norm:0.00130464 max memory_allocated 7500.048828125 time 50.365495681762695 \n",
      "\u001b[32m[2025-10-31 07:28:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:28:32 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:29:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0031647721771150827 val_loss:0.003440100234001875 quant_lr:5.192416827251822e-05 norm:0.00285428 max memory_allocated 7500.048828125 time 50.30359649658203 \n",
      "\u001b[32m[2025-10-31 07:30:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.0030490155331790447 val_loss:0.0033870814368128777 quant_lr:5e-06 norm:0.00147942 max memory_allocated 7503.673828125 time 50.34306335449219 \n",
      "\u001b[32m[2025-10-31 07:30:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:30:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:31:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.0034315031953155994 val_loss:0.0037378978449851274 quant_lr:5.192416827251822e-05 norm:0.00349131 max memory_allocated 7503.673828125 time 50.31041216850281 \n",
      "\u001b[32m[2025-10-31 07:32:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0032968667801469564 val_loss:0.00368183059617877 quant_lr:5e-06 norm:0.00164148 max memory_allocated 7503.673828125 time 50.37336254119873 \n",
      "\u001b[32m[2025-10-31 07:32:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:32:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:33:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.0039672330021858215 val_loss:0.004318857099860907 quant_lr:5.192416827251822e-05 norm:0.00341923 max memory_allocated 7503.673828125 time 50.37175989151001 \n",
      "\u001b[32m[2025-10-31 07:34:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.003800425911322236 val_loss:0.004248718731105328 quant_lr:5e-06 norm:0.00192041 max memory_allocated 7503.673828125 time 50.3951678276062 \n",
      "\u001b[32m[2025-10-31 07:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:34:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:35:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:35:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:35:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:35:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:36:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.004475235007703304 val_loss:0.004845392890274525 quant_lr:5.192416827251822e-05 norm:0.00309450 max memory_allocated 7503.673828125 time 50.45147252082825 \n",
      "\u001b[32m[2025-10-31 07:36:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.004289427772164345 val_loss:0.004768630489706993 quant_lr:5e-06 norm:0.00181182 max memory_allocated 7503.673828125 time 50.396724224090576 \n",
      "\u001b[32m[2025-10-31 07:37:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:37:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:38:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.005221167113631964 val_loss:0.0056694140657782555 quant_lr:5.192416827251822e-05 norm:0.00357693 max memory_allocated 7503.673828125 time 50.43657326698303 \n",
      "\u001b[32m[2025-10-31 07:39:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.004971885588020086 val_loss:0.005563987419009209 quant_lr:5e-06 norm:0.00226377 max memory_allocated 7503.673828125 time 50.403735399246216 \n",
      "\u001b[32m[2025-10-31 07:39:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:39:33 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:40:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.006066364701837301 val_loss:0.006653250195086002 quant_lr:5.192416827251822e-05 norm:0.00474545 max memory_allocated 7503.673828125 time 50.4503116607666 \n",
      "\u001b[32m[2025-10-31 07:41:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.005781743209809065 val_loss:0.006541842129081488 quant_lr:5e-06 norm:0.00294870 max memory_allocated 7503.673828125 time 50.487932443618774 \n",
      "\u001b[32m[2025-10-31 07:41:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:41:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:42:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.00732765719294548 val_loss:0.008128940127789974 quant_lr:5.192416827251822e-05 norm:0.00433486 max memory_allocated 7503.673828125 time 50.51443290710449 \n",
      "\u001b[32m[2025-10-31 07:43:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.007003152742981911 val_loss:0.008012436330318451 quant_lr:5e-06 norm:0.00286789 max memory_allocated 7503.673828125 time 50.46782159805298 \n",
      "\u001b[32m[2025-10-31 07:43:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:43:58 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:44:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.008737334050238132 val_loss:0.009757919237017632 quant_lr:5.192416827251822e-05 norm:0.00387606 max memory_allocated 7503.673828125 time 50.52716684341431 \n",
      "\u001b[32m[2025-10-31 07:45:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.008394956588745117 val_loss:0.009640983305871487 quant_lr:5e-06 norm:0.00310054 max memory_allocated 7503.673828125 time 50.50657796859741 \n",
      "\u001b[32m[2025-10-31 07:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:46:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:47:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.010280904360115528 val_loss:0.011590990237891674 quant_lr:5.192416827251822e-05 norm:0.00455067 max memory_allocated 7503.673828125 time 50.502161264419556 \n",
      "\u001b[32m[2025-10-31 07:48:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.009910006076097488 val_loss:0.011466478928923607 quant_lr:5e-06 norm:0.00350033 max memory_allocated 7503.673828125 time 50.470993757247925 \n",
      "\u001b[32m[2025-10-31 07:48:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:48:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:49:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.012220984324812889 val_loss:0.013853040523827076 quant_lr:5.192416827251822e-05 norm:0.00429816 max memory_allocated 7503.673828125 time 50.61215090751648 \n",
      "\u001b[32m[2025-10-31 07:50:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.011815867386758327 val_loss:0.013710963539779186 quant_lr:5e-06 norm:0.00322592 max memory_allocated 7503.673828125 time 50.4618706703186 \n",
      "\u001b[32m[2025-10-31 07:50:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:50:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:51:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.015505718067288399 val_loss:0.017833160236477852 quant_lr:5.192416827251822e-05 norm:0.00528525 max memory_allocated 7503.673828125 time 50.42588496208191 \n",
      "\u001b[32m[2025-10-31 07:52:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.014937866479158401 val_loss:0.01763620413839817 quant_lr:5e-06 norm:0.00387641 max memory_allocated 7503.673828125 time 50.40326952934265 \n",
      "\u001b[32m[2025-10-31 07:52:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:52:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:53:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.018824400380253792 val_loss:0.021993279457092285 quant_lr:5.192416827251822e-05 norm:0.00501964 max memory_allocated 7503.673828125 time 50.53360342979431 \n",
      "\u001b[32m[2025-10-31 07:54:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.018219271674752235 val_loss:0.02180333063006401 quant_lr:5e-06 norm:0.00375106 max memory_allocated 7503.673828125 time 50.48042273521423 \n",
      "\u001b[32m[2025-10-31 07:54:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:54:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:54:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:54:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:54:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:54:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:55:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:55:01 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:56:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.02290976792573929 val_loss:0.027068888768553734 quant_lr:5.192416827251822e-05 norm:0.00578016 max memory_allocated 7503.673828125 time 50.54466652870178 \n",
      "\u001b[32m[2025-10-31 07:56:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.022194072604179382 val_loss:0.026852890849113464 quant_lr:5e-06 norm:0.00440080 max memory_allocated 7503.673828125 time 50.4937903881073 \n",
      "\u001b[32m[2025-10-31 07:57:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:57:14 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 07:58:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.027520425617694855 val_loss:0.03280787542462349 quant_lr:5.192416827251822e-05 norm:0.00547337 max memory_allocated 7503.673828125 time 50.504995822906494 \n",
      "\u001b[32m[2025-10-31 07:59:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.026711970567703247 val_loss:0.03257244825363159 quant_lr:5e-06 norm:0.00393058 max memory_allocated 7503.673828125 time 50.508774518966675 \n",
      "\u001b[32m[2025-10-31 07:59:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 07:59:27 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:00:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.03318318724632263 val_loss:0.039910152554512024 quant_lr:5.192416827251822e-05 norm:0.00735728 max memory_allocated 7503.673828125 time 50.544095516204834 \n",
      "\u001b[32m[2025-10-31 08:01:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.032258063554763794 val_loss:0.03961692377924919 quant_lr:5e-06 norm:0.00567273 max memory_allocated 7503.673828125 time 50.48316144943237 \n",
      "\u001b[32m[2025-10-31 08:01:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:01:40 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:02:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.04022322595119476 val_loss:0.048987142741680145 quant_lr:5.192416827251822e-05 norm:0.00923636 max memory_allocated 7503.673828125 time 50.567262172698975 \n",
      "\u001b[32m[2025-10-31 08:03:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.03911462798714638 val_loss:0.04863429069519043 quant_lr:5e-06 norm:0.00677424 max memory_allocated 7503.673828125 time 50.48053336143494 \n",
      "\u001b[32m[2025-10-31 08:03:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:03:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:04:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.04883866757154465 val_loss:0.05963084101676941 quant_lr:5.192416827251822e-05 norm:0.01161012 max memory_allocated 7503.673828125 time 50.52753520011902 \n",
      "\u001b[32m[2025-10-31 08:05:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.047549277544021606 val_loss:0.05912666395306587 quant_lr:5e-06 norm:0.00961119 max memory_allocated 7503.673828125 time 50.49591898918152 \n",
      "\u001b[32m[2025-10-31 08:05:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:05:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:05:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:05:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:06:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:06:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:06:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:07:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.06034428998827934 val_loss:0.07422559708356857 quant_lr:5.192416827251822e-05 norm:0.01521491 max memory_allocated 7503.673828125 time 50.54760265350342 \n",
      "\u001b[32m[2025-10-31 08:07:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.05880686640739441 val_loss:0.07365632057189941 quant_lr:5e-06 norm:0.01147640 max memory_allocated 7503.673828125 time 50.46475052833557 \n",
      "\u001b[32m[2025-10-31 08:08:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:08:18 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:09:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.07591159641742706 val_loss:0.09304402768611908 quant_lr:5.192416827251822e-05 norm:0.02346391 max memory_allocated 7503.673828125 time 50.70126438140869 \n",
      "\u001b[32m[2025-10-31 08:10:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.07393214851617813 val_loss:0.0923595130443573 quant_lr:5e-06 norm:0.01657450 max memory_allocated 7503.673828125 time 50.57305431365967 \n",
      "\u001b[32m[2025-10-31 08:10:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:10:32 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:11:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.10278715193271637 val_loss:0.12583580613136292 quant_lr:5.192416827251822e-05 norm:0.04114577 max memory_allocated 7503.673828125 time 50.524141788482666 \n",
      "\u001b[32m[2025-10-31 08:12:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.09965689480304718 val_loss:0.12454377114772797 quant_lr:5e-06 norm:0.03000426 max memory_allocated 7503.673828125 time 50.41472887992859 \n",
      "\u001b[32m[2025-10-31 08:12:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:12:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 08:13:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.21242156624794006 val_loss:0.27936145663261414 quant_lr:5.192416827251822e-05 norm:0.19254106 max memory_allocated 7503.673828125 time 50.60829043388367 \n",
      "\u001b[32m[2025-10-31 08:14:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.2009904384613037 val_loss:0.27417123317718506 quant_lr:5e-06 norm:0.13627611 max memory_allocated 7503.673828125 time 50.4837441444397 \n",
      "\u001b[32m[2025-10-31 08:14:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 08:14:56 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 4253.697849750519\n",
      "\u001b[32m[2025-10-31 08:14:56 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-31 08:15:11 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 141/141 [02:22<00:00,  1.01s/it]\n",
      "wikitext2:18.819412231445312\n",
      "\u001b[32m[2025-10-31 08:17:38 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 18.82\n",
      "2025-10-31 08:17:39.265322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761898659.287510   26373 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761898659.293820   26373 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761898659.313828   26373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761898659.313859   26373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761898659.313865   26373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761898659.313868   26373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-31 08:17:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 08:17:46 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 08:17:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading readme: 9.00kB [00:00, 24.2MB/s]\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127830.83it/s]\n",
      "\u001b[32m[2025-10-31 08:18:05 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2207.76it/s]\n",
      "\u001b[32m[2025-10-31 08:18:11 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1040.09it/s]\n",
      "\u001b[32m[2025-10-31 08:18:13 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1013.27it/s]\n",
      "\u001b[32m[2025-10-31 08:18:15 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:25<00:00, 25.57it/s]\n",
      "\u001b[32m[2025-10-31 08:55:20 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5509|±  |0.0140|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4187|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5206|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.5109|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4726|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6567|±  |0.0111|\n",
      "|          |       |none  |     0|acc_norm|0.6556|±  |0.0111|\n",
      "\n",
      "\u001b[32m[2025-10-31 08:55:20 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 53.43%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "     --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 256 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 2 \\\n",
    "       --group_size 64 \\\n",
    "        --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "       --output_dir ./output/llama3_baseline256 \\\n",
    "       --save_quant_dir ./output/llama3_baseline256/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "N40dIdzwe0sU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6941150,
     "status": "ok",
     "timestamp": 1761923529460,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "N40dIdzwe0sU",
    "outputId": "b02c3d0e-7c5a-45e7-cfb2-ced87391b1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--calib_dataset', 'wikitext2', '--train_size', '256', '--val_size', '64', '--wbits', '2', '--group_size', '64', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--output_dir', './output/llama3_baseline256_64', '--save_quant_dir', './output/llama3_baseline256_64/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 13:16:32 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_baseline256_64', save_quant_dir='./output/llama3_baseline256_64/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=64, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-31 13:16:32 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.04it/s]\n",
      "\u001b[32m[2025-10-31 13:16:37 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-31 13:16:37 root]\u001b[0m\u001b[33m(main_block_ap.py 170)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_train.cache\n",
      "\u001b[32m[2025-10-31 13:16:37 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_val.cache\n",
      "\u001b[32m[2025-10-31 13:16:37 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-31 13:16:42 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-31 13:16:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:17:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:2.1824274881510064e-05 val_loss:2.138486161129549e-05 quant_lr:5.192416827251822e-05 norm:0.00026925 max memory_allocated 7437.673828125 time 52.88324856758118 \n",
      "\u001b[32m[2025-10-31 13:18:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:1.9627943402156234e-05 val_loss:2.0281662727938965e-05 quant_lr:5e-06 norm:0.00013421 max memory_allocated 7498.173828125 time 53.50824594497681 \n",
      "\u001b[32m[2025-10-31 13:18:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:18:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:18:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:18:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:18:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:19:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:19:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:19:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:20:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0003405280876904726 val_loss:0.0004511357983574271 quant_lr:5.192416827251822e-05 norm:0.02810097 max memory_allocated 7498.173828125 time 53.61376929283142 \n",
      "\u001b[32m[2025-10-31 13:21:00 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.00023226517078001052 val_loss:0.0003339051327202469 quant_lr:5e-06 norm:0.01051553 max memory_allocated 7498.298828125 time 53.53431558609009 \n",
      "\u001b[32m[2025-10-31 13:21:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:21:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:22:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.00034601971856318414 val_loss:0.0004513799212872982 quant_lr:5.192416827251822e-05 norm:0.00067646 max memory_allocated 7498.298828125 time 53.70617890357971 \n",
      "\u001b[32m[2025-10-31 13:23:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.0003251515154261142 val_loss:0.0004393878043629229 quant_lr:5e-06 norm:0.00026200 max memory_allocated 7498.298828125 time 53.42990040779114 \n",
      "\u001b[32m[2025-10-31 13:23:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:23:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:24:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0005493001081049442 val_loss:0.0006618633051402867 quant_lr:5.192416827251822e-05 norm:0.00072613 max memory_allocated 7498.298828125 time 53.340800046920776 \n",
      "\u001b[32m[2025-10-31 13:25:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.00051453011110425 val_loss:0.000640799873508513 quant_lr:5e-06 norm:0.00044912 max memory_allocated 7498.298828125 time 53.68284869194031 \n",
      "\u001b[32m[2025-10-31 13:25:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:25:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:26:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0007840169128030539 val_loss:0.0009096241556107998 quant_lr:5.192416827251822e-05 norm:0.00092330 max memory_allocated 7498.298828125 time 53.723531007766724 \n",
      "\u001b[32m[2025-10-31 13:27:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.000733544526156038 val_loss:0.0008815339533612132 quant_lr:5e-06 norm:0.00056780 max memory_allocated 7501.048828125 time 53.537919759750366 \n",
      "\u001b[32m[2025-10-31 13:28:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:28:13 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:29:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0010900759370997548 val_loss:0.0012377772945910692 quant_lr:5.192416827251822e-05 norm:0.00123890 max memory_allocated 7501.048828125 time 53.83041310310364 \n",
      "\u001b[32m[2025-10-31 13:30:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.0010247210739180446 val_loss:0.0012018237030133605 quant_lr:5e-06 norm:0.00072577 max memory_allocated 7501.048828125 time 53.61849927902222 \n",
      "\u001b[32m[2025-10-31 13:30:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:30:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:31:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0014065984869375825 val_loss:0.0015805589500814676 quant_lr:5.192416827251822e-05 norm:0.00131986 max memory_allocated 7501.048828125 time 53.88600254058838 \n",
      "\u001b[32m[2025-10-31 13:32:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.001328553888015449 val_loss:0.0015404033474624157 quant_lr:5e-06 norm:0.00078850 max memory_allocated 7501.048828125 time 53.70092797279358 \n",
      "\u001b[32m[2025-10-31 13:32:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:32:53 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:33:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0017888188594952226 val_loss:0.001986798597499728 quant_lr:5.192416827251822e-05 norm:0.00157924 max memory_allocated 7501.048828125 time 53.79042339324951 \n",
      "\u001b[32m[2025-10-31 13:34:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0016988603165373206 val_loss:0.0019434461137279868 quant_lr:5e-06 norm:0.00091208 max memory_allocated 7501.048828125 time 53.55481147766113 \n",
      "\u001b[32m[2025-10-31 13:35:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:35:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:36:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.0021163951605558395 val_loss:0.0023375891614705324 quant_lr:5.192416827251822e-05 norm:0.00201081 max memory_allocated 7501.048828125 time 53.89936804771423 \n",
      "\u001b[32m[2025-10-31 13:37:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.0020155785605311394 val_loss:0.0022903522476553917 quant_lr:5e-06 norm:0.00110956 max memory_allocated 7501.048828125 time 53.64728093147278 \n",
      "\u001b[32m[2025-10-31 13:37:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:37:37 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:38:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.0024888883344829082 val_loss:0.00274807121604681 quant_lr:5.192416827251822e-05 norm:0.00263609 max memory_allocated 7501.048828125 time 53.785085916519165 \n",
      "\u001b[32m[2025-10-31 13:39:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.0023777135647833347 val_loss:0.0026924724224954844 quant_lr:5e-06 norm:0.00144693 max memory_allocated 7501.048828125 time 53.56090760231018 \n",
      "\u001b[32m[2025-10-31 13:39:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:39:59 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:41:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.0028407215140759945 val_loss:0.003125377930700779 quant_lr:5.192416827251822e-05 norm:0.00243977 max memory_allocated 7501.048828125 time 53.82265257835388 \n",
      "\u001b[32m[2025-10-31 13:41:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.0027236314490437508 val_loss:0.0030703693628311157 quant_lr:5e-06 norm:0.00139489 max memory_allocated 7501.048828125 time 53.56763982772827 \n",
      "\u001b[32m[2025-10-31 13:42:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:42:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:43:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0031678220257163048 val_loss:0.003474761499091983 quant_lr:5.192416827251822e-05 norm:0.00270931 max memory_allocated 7501.048828125 time 53.824251651763916 \n",
      "\u001b[32m[2025-10-31 13:44:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.0030524141620844603 val_loss:0.0034188092686235905 quant_lr:5e-06 norm:0.00151493 max memory_allocated 7501.048828125 time 53.66551065444946 \n",
      "\u001b[32m[2025-10-31 13:44:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:44:43 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:45:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.0034349444322288036 val_loss:0.00375348306261003 quant_lr:5.192416827251822e-05 norm:0.00348296 max memory_allocated 7501.048828125 time 53.85305094718933 \n",
      "\u001b[32m[2025-10-31 13:46:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0033001278061419725 val_loss:0.0036972861271351576 quant_lr:5e-06 norm:0.00172812 max memory_allocated 7501.048828125 time 53.57861328125 \n",
      "\u001b[32m[2025-10-31 13:46:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:46:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:46:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:46:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:46:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:47:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:47:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:47:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:48:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.003970976918935776 val_loss:0.004304182715713978 quant_lr:5.192416827251822e-05 norm:0.00336721 max memory_allocated 7501.048828125 time 53.83259868621826 \n",
      "\u001b[32m[2025-10-31 13:49:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.0038038277998566628 val_loss:0.004233295563608408 quant_lr:5e-06 norm:0.00187096 max memory_allocated 7501.048828125 time 53.588157176971436 \n",
      "\u001b[32m[2025-10-31 13:49:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:49:27 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:50:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.00448055611923337 val_loss:0.004829726181924343 quant_lr:5.192416827251822e-05 norm:0.00314995 max memory_allocated 7501.048828125 time 53.7634334564209 \n",
      "\u001b[32m[2025-10-31 13:51:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.004293423146009445 val_loss:0.004751489032059908 quant_lr:5e-06 norm:0.00187946 max memory_allocated 7501.048828125 time 53.6290647983551 \n",
      "\u001b[32m[2025-10-31 13:51:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:51:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:52:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.0052252900786697865 val_loss:0.005635816138237715 quant_lr:5.192416827251822e-05 norm:0.00348581 max memory_allocated 7501.048828125 time 53.810059785842896 \n",
      "\u001b[32m[2025-10-31 13:53:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.004975135438144207 val_loss:0.005530606489628553 quant_lr:5e-06 norm:0.00231911 max memory_allocated 7501.048828125 time 53.63251495361328 \n",
      "\u001b[32m[2025-10-31 13:53:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:53:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:53:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:53:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:54:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:54:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:54:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:54:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:55:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.0060739475302398205 val_loss:0.006610077805817127 quant_lr:5.192416827251822e-05 norm:0.00475865 max memory_allocated 7501.048828125 time 53.82448601722717 \n",
      "\u001b[32m[2025-10-31 13:56:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.005787963047623634 val_loss:0.006491621024906635 quant_lr:5e-06 norm:0.00279999 max memory_allocated 7501.048828125 time 53.60818958282471 \n",
      "\u001b[32m[2025-10-31 13:56:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:56:31 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:57:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.007331566885113716 val_loss:0.008059362880885601 quant_lr:5.192416827251822e-05 norm:0.00435809 max memory_allocated 7501.048828125 time 53.82380223274231 \n",
      "\u001b[32m[2025-10-31 13:58:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.0070085977204144 val_loss:0.007937713526189327 quant_lr:5e-06 norm:0.00289220 max memory_allocated 7501.048828125 time 53.63379454612732 \n",
      "\u001b[32m[2025-10-31 13:58:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 13:58:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 13:59:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.008741009049117565 val_loss:0.009645700454711914 quant_lr:5.192416827251822e-05 norm:0.00383588 max memory_allocated 7501.048828125 time 53.86941337585449 \n",
      "\u001b[32m[2025-10-31 14:00:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.008397563360631466 val_loss:0.009526386857032776 quant_lr:5e-06 norm:0.00309124 max memory_allocated 7501.048828125 time 53.65049910545349 \n",
      "\u001b[32m[2025-10-31 14:01:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:01:16 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:02:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.01028234139084816 val_loss:0.011458790861070156 quant_lr:5.192416827251822e-05 norm:0.00459350 max memory_allocated 7501.048828125 time 53.994757652282715 \n",
      "\u001b[32m[2025-10-31 14:03:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.009911702014505863 val_loss:0.011334438808262348 quant_lr:5e-06 norm:0.00359078 max memory_allocated 7501.048828125 time 53.750560998916626 \n",
      "\u001b[32m[2025-10-31 14:03:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:03:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:04:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.012215331196784973 val_loss:0.013646556995809078 quant_lr:5.192416827251822e-05 norm:0.00415540 max memory_allocated 7501.048828125 time 53.77362632751465 \n",
      "\u001b[32m[2025-10-31 14:05:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.011810981668531895 val_loss:0.01351274736225605 quant_lr:5e-06 norm:0.00322495 max memory_allocated 7501.048828125 time 53.6563286781311 \n",
      "\u001b[32m[2025-10-31 14:05:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:05:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:05:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:05:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:05:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:05:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:06:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:06:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:07:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.01549489889293909 val_loss:0.0175350122153759 quant_lr:5.192416827251822e-05 norm:0.00521253 max memory_allocated 7501.048828125 time 53.98931169509888 \n",
      "\u001b[32m[2025-10-31 14:07:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.014931745827198029 val_loss:0.0173388309776783 quant_lr:5e-06 norm:0.00386647 max memory_allocated 7501.048828125 time 53.75941562652588 \n",
      "\u001b[32m[2025-10-31 14:08:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:08:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:09:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.01881524920463562 val_loss:0.021583309397101402 quant_lr:5.192416827251822e-05 norm:0.00496505 max memory_allocated 7501.048828125 time 53.87501287460327 \n",
      "\u001b[32m[2025-10-31 14:10:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.018212132155895233 val_loss:0.021393638104200363 quant_lr:5e-06 norm:0.00366452 max memory_allocated 7501.048828125 time 53.66654872894287 \n",
      "\u001b[32m[2025-10-31 14:10:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:10:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:11:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.022900709882378578 val_loss:0.02661067247390747 quant_lr:5.192416827251822e-05 norm:0.00572069 max memory_allocated 7501.048828125 time 53.97691059112549 \n",
      "\u001b[32m[2025-10-31 14:12:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.02218337543308735 val_loss:0.02639804035425186 quant_lr:5e-06 norm:0.00439171 max memory_allocated 7501.048828125 time 53.76526141166687 \n",
      "\u001b[32m[2025-10-31 14:12:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:12:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:12:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:12:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:12:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:13:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:13:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:13:06 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:14:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.027502745389938354 val_loss:0.03239152953028679 quant_lr:5.192416827251822e-05 norm:0.00538251 max memory_allocated 7501.048828125 time 54.015559911727905 \n",
      "\u001b[32m[2025-10-31 14:15:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.026698259636759758 val_loss:0.03213733434677124 quant_lr:5e-06 norm:0.00388182 max memory_allocated 7501.048828125 time 53.799667835235596 \n",
      "\u001b[32m[2025-10-31 14:15:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:15:29 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:16:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.03316216915845871 val_loss:0.039322350174188614 quant_lr:5.192416827251822e-05 norm:0.00734877 max memory_allocated 7501.048828125 time 53.821011543273926 \n",
      "\u001b[32m[2025-10-31 14:17:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.0322384350001812 val_loss:0.039039794355630875 quant_lr:5e-06 norm:0.00568021 max memory_allocated 7502.173828125 time 53.64855718612671 \n",
      "\u001b[32m[2025-10-31 14:17:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:17:50 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:18:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.04019983857870102 val_loss:0.04816434904932976 quant_lr:5.192416827251822e-05 norm:0.00918585 max memory_allocated 7502.173828125 time 53.48239707946777 \n",
      "\u001b[32m[2025-10-31 14:19:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.03909213840961456 val_loss:0.04778921976685524 quant_lr:5e-06 norm:0.00676533 max memory_allocated 7502.173828125 time 53.51715588569641 \n",
      "\u001b[32m[2025-10-31 14:19:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:20:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:21:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.04881265386939049 val_loss:0.058582473546266556 quant_lr:5.192416827251822e-05 norm:0.01169669 max memory_allocated 7502.173828125 time 53.36331391334534 \n",
      "\u001b[32m[2025-10-31 14:22:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.0475228875875473 val_loss:0.05816412344574928 quant_lr:5e-06 norm:0.00965165 max memory_allocated 7502.173828125 time 53.53806495666504 \n",
      "\u001b[32m[2025-10-31 14:22:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:22:32 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:23:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.06030992418527603 val_loss:0.07285818457603455 quant_lr:5.192416827251822e-05 norm:0.01537964 max memory_allocated 7502.173828125 time 53.22999596595764 \n",
      "\u001b[32m[2025-10-31 14:24:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.05876787751913071 val_loss:0.07234268635511398 quant_lr:5e-06 norm:0.01122107 max memory_allocated 7502.173828125 time 53.35380172729492 \n",
      "\u001b[32m[2025-10-31 14:24:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:24:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:25:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.07590916752815247 val_loss:0.09145558625459671 quant_lr:5.192416827251822e-05 norm:0.02340314 max memory_allocated 7502.173828125 time 53.181575298309326 \n",
      "\u001b[32m[2025-10-31 14:26:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.07392501831054688 val_loss:0.09070777893066406 quant_lr:5e-06 norm:0.01739078 max memory_allocated 7502.173828125 time 53.31119441986084 \n",
      "\u001b[32m[2025-10-31 14:27:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:27:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:28:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.10275057703256607 val_loss:0.12303584069013596 quant_lr:5.192416827251822e-05 norm:0.04131002 max memory_allocated 7502.173828125 time 53.19660663604736 \n",
      "\u001b[32m[2025-10-31 14:29:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.09961651265621185 val_loss:0.12172187864780426 quant_lr:5e-06 norm:0.02980994 max memory_allocated 7502.173828125 time 53.280956745147705 \n",
      "\u001b[32m[2025-10-31 14:29:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:29:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 224.927744M\n",
      "\u001b[32m[2025-10-31 14:30:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.21291296184062958 val_loss:0.2630147337913513 quant_lr:5.192416827251822e-05 norm:0.19524112 max memory_allocated 7502.173828125 time 53.248409032821655 \n",
      "\u001b[32m[2025-10-31 14:31:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.20136509835720062 val_loss:0.25729843974113464 quant_lr:5e-06 norm:0.13189907 max memory_allocated 7502.173828125 time 53.23172688484192 \n",
      "\u001b[32m[2025-10-31 14:31:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-31 14:31:58 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 4520.921326875687\n",
      "\u001b[32m[2025-10-31 14:31:58 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-31 14:32:17 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 141/141 [02:20<00:00,  1.01it/s]\n",
      "wikitext2:18.80742073059082\n",
      "\u001b[32m[2025-10-31 14:34:43 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 18.81\n",
      "2025-10-31 14:34:43.400531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761921283.425018  120181 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761921283.432109  120181 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761921283.453067  120181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761921283.453100  120181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761921283.453108  120181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761921283.453115  120181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-31 14:34:49 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 14:34:50 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 14:34:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 128259.68it/s]\n",
      "\u001b[32m[2025-10-31 14:35:09 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2225.34it/s]\n",
      "\u001b[32m[2025-10-31 14:35:15 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1042.69it/s]\n",
      "\u001b[32m[2025-10-31 14:35:17 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1007.63it/s]\n",
      "\u001b[32m[2025-10-31 14:35:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:07<00:00, 25.78it/s]\n",
      "\u001b[32m[2025-10-31 15:12:06 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5770|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4196|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5249|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.5156|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4659|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6583|±  |0.0111|\n",
      "|          |       |none  |     0|acc_norm|0.6540|±  |0.0111|\n",
      "\n",
      "\u001b[32m[2025-10-31 15:12:06 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 54.26%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "     --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 256 \\\n",
    "       --val_size 64 \\\n",
    "       --wbits 2 \\\n",
    "       --group_size 64 \\\n",
    "        --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "       --output_dir ./output/llama3_baseline256_64 \\\n",
    "       --save_quant_dir ./output/llama3_baseline256_64/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "irhbd29n-KCI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8175311,
     "status": "ok",
     "timestamp": 1761916331397,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "irhbd29n-KCI",
    "outputId": "dc6e1ffd-29d8-479f-f19b-8cf689f50c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Meta-Llama-3-8B', '--sensitivity_file', './sensitivity_results_meta_llama_3_8b_corrected.json', '--calib_dataset', 'wikitext2', '--train_size', '256', '--val_size', '64', '--use_adaptive_training', '--group_size', '64', '--wbits', '2', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/llama3_optimized_4bitsgra', '--save_quant_dir', './output/llama3_optimized_4bitsgra/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Meta-Llama-3-8B\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Meta-Llama-3-8B', cache_dir='./cache', output_dir='./output/llama3_optimized_4bitsgra', save_quant_dir='./output/llama3_optimized_4bitsgra/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=64, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_meta_llama_3_8b_corrected.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-31 10:55:59 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Meta-Llama-3-8B\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.04it/s]\n",
      "\u001b[32m[2025-10-31 10:56:04 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-31 10:56:04 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_train.cache\n",
      "\u001b[32m[2025-10-31 10:56:04 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Meta-Llama-3-8B_wikitext2_256_64_2048_val.cache\n",
      "\u001b[32m[2025-10-31 10:56:04 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-31 10:56:10 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_meta_llama_3_8b_corrected.json\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7144, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 10:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.37e-05, Patience: 4\n",
      "\u001b[32m[2025-10-31 10:57:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000024 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 5.59e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 10:58:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000021 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 2.09e-05 | Time: 55.2s\n",
      "\u001b[32m[2025-10-31 10:59:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000020 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 3.68e-06 | Time: 55.2s\n",
      "\u001b[32m[2025-10-31 10:59:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-31 10:59:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 10:59:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-31 11:00:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000288 | Val Loss: 0.000333 | Grad Norm: 0.04 | LR: 5.73e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:01:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000185 | Val Loss: 0.000291 | Grad Norm: 0.01 | LR: 3.48e-05 | Time: 55.2s\n",
      "\u001b[32m[2025-10-31 11:02:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000137 | Val Loss: 0.000260 | Grad Norm: 0.01 | LR: 1.25e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:03:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ⚠️  Block 1 Epoch 3/4 | Train Loss: 0.000127 | Val Loss: 0.000261 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:03:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-31 11:03:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6684, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:03:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 4\n",
      "\u001b[32m[2025-10-31 11:04:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000260 | Val Loss: 0.000400 | Grad Norm: 0.00 | LR: 5.69e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:05:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000242 | Val Loss: 0.000388 | Grad Norm: 0.00 | LR: 2.13e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:06:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000238 | Val Loss: 0.000386 | Grad Norm: 0.00 | LR: 3.75e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:06:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-31 11:06:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6287, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:06:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.61e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:08:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000477 | Val Loss: 0.000624 | Grad Norm: 0.00 | LR: 5.78e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:08:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000446 | Val Loss: 0.000605 | Grad Norm: 0.00 | LR: 2.16e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:09:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000438 | Val Loss: 0.000600 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:10:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-31 11:10:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5716, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:10:15 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.78e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:11:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000732 | Val Loss: 0.000893 | Grad Norm: 0.00 | LR: 5.90e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:12:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000682 | Val Loss: 0.000861 | Grad Norm: 0.00 | LR: 2.21e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:13:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000668 | Val Loss: 0.000854 | Grad Norm: 0.00 | LR: 3.89e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:13:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-31 11:13:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5505, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:13:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.84e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:14:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.001062 | Val Loss: 0.001239 | Grad Norm: 0.00 | LR: 5.95e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:15:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000987 | Val Loss: 0.001192 | Grad Norm: 0.00 | LR: 2.23e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:16:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000968 | Val Loss: 0.001183 | Grad Norm: 0.00 | LR: 3.92e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:16:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-31 11:16:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5101, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:16:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.97e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:17:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.001398 | Val Loss: 0.001599 | Grad Norm: 0.00 | LR: 6.05e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:18:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.001306 | Val Loss: 0.001542 | Grad Norm: 0.00 | LR: 2.26e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:19:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.001281 | Val Loss: 0.001529 | Grad Norm: 0.00 | LR: 3.98e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:20:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-31 11:20:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4531, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:20:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.15e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:21:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.001801 | Val Loss: 0.002024 | Grad Norm: 0.00 | LR: 6.19e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:22:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.001689 | Val Loss: 0.001954 | Grad Norm: 0.00 | LR: 2.32e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:23:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.001659 | Val Loss: 0.001939 | Grad Norm: 0.00 | LR: 4.08e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:23:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-31 11:23:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4170, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:23:23 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.27e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.002144 | Val Loss: 0.002384 | Grad Norm: 0.00 | LR: 6.28e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:25:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.002015 | Val Loss: 0.002306 | Grad Norm: 0.00 | LR: 2.35e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:26:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.001981 | Val Loss: 0.002290 | Grad Norm: 0.00 | LR: 4.14e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-31 11:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3956, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.35e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:27:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.002536 | Val Loss: 0.002804 | Grad Norm: 0.00 | LR: 6.34e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:28:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.002386 | Val Loss: 0.002715 | Grad Norm: 0.00 | LR: 2.37e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:29:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.002348 | Val Loss: 0.002696 | Grad Norm: 0.00 | LR: 4.17e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:29:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-31 11:29:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3484, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:29:58 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:31:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.002887 | Val Loss: 0.003186 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:31:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.002738 | Val Loss: 0.003097 | Grad Norm: 0.00 | LR: 2.42e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:32:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.002698 | Val Loss: 0.003078 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:33:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-31 11:33:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3474, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:33:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.52e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:34:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.003218 | Val Loss: 0.003537 | Grad Norm: 0.00 | LR: 6.47e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:35:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.003067 | Val Loss: 0.003444 | Grad Norm: 0.00 | LR: 2.42e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:36:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.003028 | Val Loss: 0.003424 | Grad Norm: 0.00 | LR: 4.26e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:36:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-31 11:36:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3441, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:36:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.53e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:37:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.003489 | Val Loss: 0.003824 | Grad Norm: 0.00 | LR: 6.48e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:38:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.003326 | Val Loss: 0.003728 | Grad Norm: 0.00 | LR: 2.42e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 11:39:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.003281 | Val Loss: 0.003705 | Grad Norm: 0.00 | LR: 4.27e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:39:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-31 11:39:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3675, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:39:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.45e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:41:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.004065 | Val Loss: 0.004399 | Grad Norm: 0.00 | LR: 6.41e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:41:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.003848 | Val Loss: 0.004274 | Grad Norm: 0.00 | LR: 2.40e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:42:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.003792 | Val Loss: 0.004245 | Grad Norm: 0.00 | LR: 4.22e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-31 11:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3280, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 11:44:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.004584 | Val Loss: 0.004933 | Grad Norm: 0.00 | LR: 6.52e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:45:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.004348 | Val Loss: 0.004797 | Grad Norm: 0.00 | LR: 2.44e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:46:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.004285 | Val Loss: 0.004767 | Grad Norm: 0.00 | LR: 4.30e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-31 11:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3540, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:47:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.005342 | Val Loss: 0.005752 | Grad Norm: 0.00 | LR: 6.45e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:48:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.005051 | Val Loss: 0.005584 | Grad Norm: 0.00 | LR: 2.41e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:49:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.004968 | Val Loss: 0.005544 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:49:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-31 11:49:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3370, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:49:58 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.56e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 11:51:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.006182 | Val Loss: 0.006711 | Grad Norm: 0.00 | LR: 6.50e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:51:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.005865 | Val Loss: 0.006535 | Grad Norm: 0.00 | LR: 2.43e-05 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 11:52:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.005774 | Val Loss: 0.006493 | Grad Norm: 0.00 | LR: 4.28e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-31 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2824, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 11:54:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.007453 | Val Loss: 0.008175 | Grad Norm: 0.00 | LR: 6.65e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:55:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.007087 | Val Loss: 0.007971 | Grad Norm: 0.00 | LR: 2.49e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 11:56:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.006983 | Val Loss: 0.007927 | Grad Norm: 0.00 | LR: 4.38e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:56:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-31 11:56:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2009, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:56:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.09e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 11:57:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.008848 | Val Loss: 0.009761 | Grad Norm: 0.00 | LR: 4.72e-05 | Time: 55.3s\n",
      "\u001b[32m[2025-10-31 11:58:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.008547 | Val Loss: 0.009638 | Grad Norm: 0.00 | LR: 4.54e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 11:59:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-31 11:59:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1552, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 11:59:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:00:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.010548 | Val Loss: 0.011690 | Grad Norm: 0.00 | LR: 4.82e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:01:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.010206 | Val Loss: 0.011554 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 12:01:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-31 12:01:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1150, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:01:27 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.46e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:02:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.012632 | Val Loss: 0.013985 | Grad Norm: 0.00 | LR: 4.91e-05 | Time: 55.8s\n",
      "\u001b[32m[2025-10-31 12:03:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.012257 | Val Loss: 0.013832 | Grad Norm: 0.00 | LR: 4.73e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-31 12:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1546, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:05:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.016142 | Val Loss: 0.018047 | Grad Norm: 0.01 | LR: 4.82e-05 | Time: 55.8s\n",
      "\u001b[32m[2025-10-31 12:05:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.015622 | Val Loss: 0.017835 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:06:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-31 12:06:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1429, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:06:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.33e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:07:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.019727 | Val Loss: 0.022290 | Grad Norm: 0.00 | LR: 4.85e-05 | Time: 55.8s\n",
      "\u001b[32m[2025-10-31 12:08:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.019170 | Val Loss: 0.022073 | Grad Norm: 0.00 | LR: 4.67e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:08:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-31 12:08:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0538, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:08:45 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:09:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.024091 | Val Loss: 0.027510 | Grad Norm: 0.01 | LR: 5.06e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:10:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.023432 | Val Loss: 0.027257 | Grad Norm: 0.00 | LR: 4.87e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 12:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-31 12:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0307, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:12:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.029027 | Val Loss: 0.033532 | Grad Norm: 0.01 | LR: 5.11e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:13:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.028289 | Val Loss: 0.033253 | Grad Norm: 0.00 | LR: 4.92e-06 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 12:13:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-31 12:13:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0204, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:13:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:14:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.035110 | Val Loss: 0.040839 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 55.8s\n",
      "\u001b[32m[2025-10-31 12:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.034264 | Val Loss: 0.040489 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-31 12:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0194, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:17:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.042720 | Val Loss: 0.050138 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 55.9s\n",
      "\u001b[32m[2025-10-31 12:18:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.041689 | Val Loss: 0.049716 | Grad Norm: 0.01 | LR: 4.95e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:18:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-31 12:18:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0164, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:18:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:19:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.052048 | Val Loss: 0.061292 | Grad Norm: 0.01 | LR: 5.15e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:20:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.050847 | Val Loss: 0.060768 | Grad Norm: 0.01 | LR: 4.96e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:20:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-31 12:20:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:20:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:22:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.064565 | Val Loss: 0.076390 | Grad Norm: 0.02 | LR: 5.19e-05 | Time: 55.8s\n",
      "\u001b[32m[2025-10-31 12:22:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.063059 | Val Loss: 0.075712 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-31 12:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0734, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.65e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:24:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.081624 | Val Loss: 0.096671 | Grad Norm: 0.02 | LR: 5.01e-05 | Time: 55.6s\n",
      "\u001b[32m[2025-10-31 12:25:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.079672 | Val Loss: 0.095413 | Grad Norm: 0.02 | LR: 4.82e-06 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 12:25:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-31 12:25:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2267, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:25:45 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 8.98e-05, Patience: 2\n",
      "\u001b[32m[2025-10-31 12:26:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.110915 | Val Loss: 0.129958 | Grad Norm: 0.04 | LR: 4.66e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:27:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.107919 | Val Loss: 0.128437 | Grad Norm: 0.03 | LR: 4.49e-06 | Time: 55.5s\n",
      "\u001b[32m[2025-10-31 12:28:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-31 12:28:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.6659, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-31 12:28:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.50e-05, Patience: 3\n",
      "\u001b[32m[2025-10-31 12:29:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.232278 | Val Loss: 0.278467 | Grad Norm: 0.21 | LR: 5.70e-05 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:30:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.218189 | Val Loss: 0.269272 | Grad Norm: 0.15 | LR: 2.13e-05 | Time: 55.4s\n",
      "\u001b[32m[2025-10-31 12:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.214137 | Val Loss: 0.267352 | Grad Norm: 0.13 | LR: 3.75e-06 | Time: 55.7s\n",
      "\u001b[32m[2025-10-31 12:31:32 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/llama3_optimized_4bitsgra/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 12:31:32 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/llama3_optimized_4bitsgra/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-31 12:31:33 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 5728.26s (95.47min)\n",
      "\u001b[32m[2025-10-31 12:31:33 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-31 12:31:45 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/llama3_optimized_4bitsgra/model\n",
      "\u001b[32m[2025-10-31 12:31:45 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 141/141 [02:24<00:00,  1.03s/it]\n",
      "wikitext2:19.50373649597168\n",
      "\u001b[32m[2025-10-31 12:34:16 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 19.50\n",
      "2025-10-31 12:34:16.790337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761914056.812798   84992 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761914056.819238   84992 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761914056.839314   84992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761914056.839344   84992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761914056.839352   84992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761914056.839355   84992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-31 12:34:22 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m[2025-10-31 12:34:23 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-31 12:34:27 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127910.83it/s]\n",
      "\u001b[32m[2025-10-31 12:34:43 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2226.58it/s]\n",
      "\u001b[32m[2025-10-31 12:34:48 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1054.00it/s]\n",
      "\u001b[32m[2025-10-31 12:34:51 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1022.13it/s]\n",
      "\u001b[32m[2025-10-31 12:34:53 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:36<00:00, 25.45it/s]\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5699|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4224|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5311|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.5198|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4756|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6605|±  |0.0110|\n",
      "|          |       |none  |     0|acc_norm|0.6507|±  |0.0111|\n",
      "\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 54.31%\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/llama3_optimized_4bitsgra/results.json\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-31 13:12:08 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "         --model \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "         --sensitivity_file \"./sensitivity_results_meta_llama_3_8b_corrected.json\" \\\n",
    "         --calib_dataset wikitext2 \\\n",
    "         --train_size 256 \\\n",
    "         --val_size 64 \\\n",
    "         --use_adaptive_training \\\n",
    "          --group_size 64 \\\n",
    "          --wbits 2 \\\n",
    "         --quant_lr 1e-4 \\\n",
    "         --weight_lr 2e-5 \\\n",
    "         --real_quant \\\n",
    "         --output_dir \"./output/llama3_optimized_4bitsgra\" \\\n",
    "         --save_quant_dir \"./output/llama3_optimized_4bitsgra/model\" \\\n",
    "         --eval_ppl \\\n",
    "         --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2BvHqbB3UKQr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761851391234,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "2BvHqbB3UKQr",
    "outputId": "5c7248df-59f6-4f3e-fd0b-4eba980ed7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 4785.67 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = './output/llama_adaptive'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CYvz0KImhqp-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1761851422298,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "CYvz0KImhqp-",
    "outputId": "22563ac4-196d-416d-9490-f5ece89fce92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 5608.78 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_mpq4bit'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w-52Xn4HhzVs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1761851445916,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "w-52Xn4HhzVs",
    "outputId": "d3220c09-e470-43dd-fbfc-32abd3da3b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3799.12 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_4bitsgra'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3xies7OMh3Y6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761851487999,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "3xies7OMh3Y6",
    "outputId": "bf1c4462-3e86-4f2c-8851-fd342ab981ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 4785.66 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_4.5bit'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_t4PR2GAiDZD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1761851511977,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "_t4PR2GAiDZD",
    "outputId": "804280e9-3fd2-405f-d2c9-ee793b89f9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 4807.52 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_3bitsgra'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SgZ_tQ-PiJO8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1761851543072,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "SgZ_tQ-PiJO8",
    "outputId": "9d574c8c-e342-4a64-803a-b8c5447088ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 4807.51 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgj97-XBiQ01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761851568913,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "mgj97-XBiQ01",
    "outputId": "e2077cb3-1805-4f92-eb71-d2b3e8b8b794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3799.11 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline4'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G0rkDY7KiXJN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1761851590017,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "G0rkDY7KiXJN",
    "outputId": "fc287ead-fe72-4a98-adc7-2ea940697ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 5476.19 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline4bit'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KFz9PhhWiaZh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761884498185,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "KFz9PhhWiaZh",
    "outputId": "a93fb9fa-904f-4d4b-d2e6-933e21d5cd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3916.07 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline4/model'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "xlYzpH0UaY99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1761923600120,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "xlYzpH0UaY99",
    "outputId": "760dab47-6621-42ab-a199-33c1fc28741a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3916.11 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline256'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ushX6WYk1I0H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1761923609852,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "ushX6WYk1I0H",
    "outputId": "753a476e-b14b-41b2-b5c1-0aeaaffc7c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3916.11 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_baseline256_64'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "PoJqb0UJ1LNF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1761923627801,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "PoJqb0UJ1LNF",
    "outputId": "8d41f03a-2b19-466c-f491-a136a506f5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 4231.66 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_2.0bit'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hcJakaYe1PlQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1761923648691,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "hcJakaYe1PlQ",
    "outputId": "2ac753ea-f566-4330-8da5-07e2d4008e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3790.65 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_2_256mpqbit'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "-Vdvtk9a1UsM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761923665225,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "-Vdvtk9a1UsM",
    "outputId": "7a68af23-ca55-4561-bc4e-53bb14f6c316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3916.12 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "model_dir = './output/llama3_optimized_4bitsgra'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
