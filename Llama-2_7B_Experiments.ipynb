{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ll6tUOSZZHyk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ll6tUOSZZHyk",
    "outputId": "1087fc8c-0979-4c37-e259-ec03f71f8ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PR'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 197 (delta 98), reused 182 (delta 83), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (197/197), 164.81 KiB | 1.34 MiB/s, done.\n",
      "Resolving deltas: 100% (98/98), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/gautamk01/PR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gz2CJyJPZIyS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz2CJyJPZIyS",
    "outputId": "427c2440-44e4-43a9-e3bd-5e0e8bab7af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/PR\n"
     ]
    }
   ],
   "source": [
    "%cd PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BxDv895YZK2s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BxDv895YZK2s",
    "outputId": "efc85294-2f2f-4c84-b3df-30b4a2ca0faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.28.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.41.0 (from -r requirements.txt (line 2))\n",
      "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lm_eval==0.4.2 (from -r requirements.txt (line 4))\n",
      "  Downloading lm_eval-0.4.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 7))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.40.1 (from -r requirements.txt (line 8))\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0 (from -r requirements.txt (line 9))\n",
      "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (5.29.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (18.1.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Collecting evaluate (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonlines (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (2.14.1)\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.17.1)\n",
      "Collecting pybind11>=2.6.2 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pytablewriter (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (1.6.1)\n",
      "Collecting sqlitedict (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tqdm-multiprocess (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.25.0)\n",
      "Collecting word2number (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (10.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1->-r requirements.txt (line 8))\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2025.10.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (75.2.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (8.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c4ba183367b70debaf314ad2a6712f75921af971584dbdd4228c884341d79e72\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=a00605337be77e4db8e62c7278630cea4ae9d7c1fc521629b328fbb2ab35a00e\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=85abb4ea2d4b8958bf43bcf70dc122f6d77bd92a32120f36ebb5cfe7404bedba\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, bitsandbytes, triton, tqdm, tcolorpy, pybind11, pyarrow-hotfix, portalocker, pathvalidate, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mbstrdecoder, jsonlines, fsspec, colorama, typepy, tqdm-multiprocess, sacrebleu, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, rouge-score, transformers, datasets, DataProperty, accelerate, tabledata, evaluate, pytablewriter, lm_eval\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.64.1 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 accelerate-0.28.0 bitsandbytes-0.41.0 colorama-0.4.6 datasets-2.18.0 evaluate-0.4.6 fsspec-2024.2.0 jsonlines-4.0.0 lm_eval-0.4.2 mbstrdecoder-1.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pathvalidate-3.3.1 portalocker-3.2.0 pyarrow-hotfix-0.7 pybind11-3.0.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tokenizers-0.19.1 torch-2.2.2 tqdm-4.64.1 tqdm-multiprocess-0.0.11 transformers-4.40.1 triton-2.2.0 typepy-1.3.4 word2number-1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "182a58c727d24cfcbf8fda8ca7d4bd0a",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f687b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a08f687b",
    "outputId": "6fbb06e6-9cf5-4254-cfac-960e960b5cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.28.0 in /usr/local/lib/python3.12/dist-packages (0.28.0)\n",
      "Requirement already satisfied: transformers==4.40.1 in /usr/local/lib/python3.12/dist-packages (4.40.1)\n",
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (1.2.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.28.0) (12.6.85)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "Successfully installed peft-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.28.0 transformers==4.40.1 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e65bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21e65bee",
    "outputId": "144c5448-ed6e-442d-e4f6-3e266028c6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `newProject` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `newProject`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token #token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aYzYFaTJdi4R",
   "metadata": {
    "id": "aYzYFaTJdi4R"
   },
   "source": [
    "### Llama-2-hf experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d84805",
   "metadata": {
    "id": "f7d84805"
   },
   "outputs": [],
   "source": [
    "MODEL=\"meta-llama/Llama-2-7b-hf\"\n",
    "SENSITIVITY_FILE=\"./sensitivity_results_llama_2_7b_hf.json\"\n",
    "TRAIN_SIZE=128\n",
    "VAL_SIZE=16\n",
    "BASE_OUTPUT=\"./output/research_experiments\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a82c8",
   "metadata": {},
   "source": [
    "### 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8657cc1",
   "metadata": {
    "id": "d8657cc1"
   },
   "outputs": [],
   "source": [
    "MODEL=\"meta-llama/Llama-2-7b-hf\"\n",
    "SENSITIVITY_FILE=\"./sensitivity_results_llama_2_7b_hf.json\"\n",
    "TRAIN_SIZE=256\n",
    "VAL_SIZE=32\n",
    "BASE_OUTPUT=\"./output/research_experiments\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb1a82",
   "metadata": {},
   "source": [
    "# SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c046f",
   "metadata": {
    "id": "651c046f",
    "outputId": "e7c20a95-0673-4ef0-9341-ba22f21a021c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf.json', '--calib_dataset', 'wikitext2', '--train_size', '256', '--val_size', '32', '--wbits', '4', '--group_size', '128', '--use_adaptive_training', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/sgra', '--save_quant_dir', './output/research_experiments/sgra/model2', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf.json\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/sgra', save_quant_dir='./output/research_experiments/sgra/model2', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=32, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-11 13:09:01 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 17.05it/s]\n",
      "\u001b[32m[2025-10-11 13:09:02 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-11 13:09:02 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_256_32_2048_train.cache\n",
      "\u001b[32m[2025-10-11 13:09:02 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_256_32_2048_val.cache\n",
      "\u001b[32m[2025-10-11 13:09:02 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-11 13:09:04 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf.json\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.9784 to 1.0000\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 2-3\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:09:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000001 | Val Loss: 0.000001 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 37.9s\n",
      "\u001b[32m[2025-10-11 13:10:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000001 | Val Loss: 0.000001 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 37.8s\n",
      "\u001b[32m[2025-10-11 13:11:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000001 | Val Loss: 0.000001 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.0s\n",
      "\u001b[32m[2025-10-11 13:11:18 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-11 13:11:18 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:11:18 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:12:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/3 | Train Loss: 0.000679 | Val Loss: 0.000431 | Grad Norm: 0.21 | LR: 5.06e-05 | Time: 38.0s\n",
      "\u001b[32m[2025-10-11 13:12:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/3 | Train Loss: 0.000577 | Val Loss: 0.000351 | Grad Norm: 0.28 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:13:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/3 | Train Loss: 0.000266 | Val Loss: 0.000273 | Grad Norm: 0.18 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-11 13:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:14:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000259 | Val Loss: 0.000279 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:14:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000241 | Val Loss: 0.000278 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:15:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000241 | Val Loss: 0.000270 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:15:37 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-11 13:15:37 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:15:37 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:16:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000279 | Val Loss: 0.000312 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:16:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000267 | Val Loss: 0.000303 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:17:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000258 | Val Loss: 0.000297 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:17:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-11 13:17:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:17:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:18:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000330 | Val Loss: 0.000374 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:19:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000320 | Val Loss: 0.000365 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:19:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000316 | Val Loss: 0.000364 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:20:02 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-11 13:20:02 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:20:02 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000419 | Val Loss: 0.000472 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:21:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000408 | Val Loss: 0.000465 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:22:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000399 | Val Loss: 0.000461 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:22:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-11 13:22:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:22:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:23:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000543 | Val Loss: 0.000611 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:23:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000524 | Val Loss: 0.000598 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:24:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000513 | Val Loss: 0.000593 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.1s\n",
      "\u001b[32m[2025-10-11 13:24:28 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-11 13:24:28 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:24:28 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:25:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000699 | Val Loss: 0.000791 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:25:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000668 | Val Loss: 0.000772 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:26:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000655 | Val Loss: 0.000765 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-11 13:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:27:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000903 | Val Loss: 0.001036 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:28:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000862 | Val Loss: 0.001010 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:28:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000842 | Val Loss: 0.000999 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:28:54 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-11 13:28:54 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:28:54 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:29:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.001132 | Val Loss: 0.001324 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:30:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.001087 | Val Loss: 0.001295 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:30:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.001063 | Val Loss: 0.001284 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-11 13:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:31:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.001403 | Val Loss: 0.001655 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:32:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.001346 | Val Loss: 0.001620 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:33:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.001315 | Val Loss: 0.001606 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:33:21 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-11 13:33:21 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:33:21 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:34:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.001644 | Val Loss: 0.001967 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:34:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.001580 | Val Loss: 0.001926 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:35:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.001544 | Val Loss: 0.001910 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:35:33 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-11 13:35:33 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:35:33 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:36:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.001941 | Val Loss: 0.002346 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:36:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.001866 | Val Loss: 0.002302 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:37:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.001826 | Val Loss: 0.002285 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:37:46 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-11 13:37:46 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:37:46 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:38:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.002249 | Val Loss: 0.002749 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:39:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.002163 | Val Loss: 0.002699 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:39:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.002117 | Val Loss: 0.002676 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:39:58 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-11 13:39:58 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:39:58 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:40:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.002634 | Val Loss: 0.003247 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:41:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.002531 | Val Loss: 0.003188 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:41:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.002477 | Val Loss: 0.003164 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:42:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-11 13:42:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:42:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:42:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.003088 | Val Loss: 0.003831 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:43:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.002972 | Val Loss: 0.003764 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:44:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.002910 | Val Loss: 0.003738 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:44:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-11 13:44:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:44:24 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:45:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.003903 | Val Loss: 0.004881 | Grad Norm: 0.00 | LR: 5.06e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:45:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.003751 | Val Loss: 0.004793 | Grad Norm: 0.00 | LR: 1.89e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:46:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.003667 | Val Loss: 0.004759 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-11 13:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9993, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.74e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:47:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.004593 | Val Loss: 0.005761 | Grad Norm: 0.01 | LR: 5.12e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.004425 | Val Loss: 0.005670 | Grad Norm: 0.00 | LR: 1.92e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:48:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.004346 | Val Loss: 0.005636 | Grad Norm: 0.00 | LR: 3.37e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:48:50 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-11 13:48:50 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9975, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:48:50 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.93e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:49:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.005656 | Val Loss: 0.007152 | Grad Norm: 0.01 | LR: 5.26e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:50:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.005464 | Val Loss: 0.007048 | Grad Norm: 0.01 | LR: 1.97e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:50:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.005356 | Val Loss: 0.007008 | Grad Norm: 0.00 | LR: 3.47e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:51:03 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-11 13:51:03 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9957, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:51:03 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.14e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:51:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.006917 | Val Loss: 0.008834 | Grad Norm: 0.01 | LR: 5.42e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:52:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.006699 | Val Loss: 0.008717 | Grad Norm: 0.01 | LR: 2.03e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:53:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.006580 | Val Loss: 0.008673 | Grad Norm: 0.00 | LR: 3.57e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-11 13:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9946, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:53:16 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.27e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 13:54:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.008674 | Val Loss: 0.011201 | Grad Norm: 0.01 | LR: 5.52e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:54:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.008412 | Val Loss: 0.011075 | Grad Norm: 0.01 | LR: 2.07e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:55:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.008258 | Val Loss: 0.011023 | Grad Norm: 0.01 | LR: 3.64e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:55:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-11 13:55:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9900, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:55:30 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.89e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 13:56:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.010508 | Val Loss: 0.013721 | Grad Norm: 0.01 | LR: 5.99e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:56:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.010198 | Val Loss: 0.013578 | Grad Norm: 0.01 | LR: 2.24e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:57:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.010032 | Val Loss: 0.013526 | Grad Norm: 0.00 | LR: 3.95e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-11 13:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9905, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.81e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 13:58:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.012984 | Val Loss: 0.017144 | Grad Norm: 0.01 | LR: 5.93e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 13:59:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.012616 | Val Loss: 0.016998 | Grad Norm: 0.01 | LR: 2.22e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:59:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.012409 | Val Loss: 0.016920 | Grad Norm: 0.01 | LR: 3.91e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 13:59:56 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-11 13:59:56 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9860, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 13:59:56 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 14:00:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.015519 | Val Loss: 0.020709 | Grad Norm: 0.01 | LR: 6.46e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:01:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.015101 | Val Loss: 0.020535 | Grad Norm: 0.01 | LR: 2.42e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:01:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.014875 | Val Loss: 0.020464 | Grad Norm: 0.01 | LR: 4.26e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-11 14:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9873, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.29e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 14:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.018492 | Val Loss: 0.025145 | Grad Norm: 0.01 | LR: 6.29e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:03:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.018031 | Val Loss: 0.024940 | Grad Norm: 0.01 | LR: 2.35e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:04:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.017780 | Val Loss: 0.024882 | Grad Norm: 0.01 | LR: 4.14e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:04:23 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-11 14:04:23 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9864, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:04:23 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.43e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 14:05:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.021814 | Val Loss: 0.030008 | Grad Norm: 0.01 | LR: 6.40e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:05:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.021298 | Val Loss: 0.029789 | Grad Norm: 0.01 | LR: 2.40e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:06:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.021009 | Val Loss: 0.029700 | Grad Norm: 0.01 | LR: 4.22e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-11 14:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9849, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-11 14:07:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.025972 | Val Loss: 0.036333 | Grad Norm: 0.01 | LR: 6.60e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:07:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.025372 | Val Loss: 0.036075 | Grad Norm: 0.01 | LR: 2.47e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:08:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.025041 | Val Loss: 0.035983 | Grad Norm: 0.01 | LR: 4.34e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:08:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-11 14:08:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9784, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:08:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-11 14:09:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.030487 | Val Loss: 0.043175 | Grad Norm: 0.01 | LR: 5.19e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:10:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.029797 | Val Loss: 0.042965 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-11 14:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9804, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.56e-05, Patience: 2\n",
      "\u001b[32m[2025-10-11 14:11:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.036665 | Val Loss: 0.052328 | Grad Norm: 0.02 | LR: 4.96e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:11:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.035817 | Val Loss: 0.052031 | Grad Norm: 0.01 | LR: 4.78e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:12:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-11 14:12:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9878, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:12:00 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.22e-05, Patience: 3\n",
      "\u001b[32m[2025-10-11 14:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 0.044011 | Val Loss: 0.063305 | Grad Norm: 0.02 | LR: 6.24e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:13:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 0.043036 | Val Loss: 0.062877 | Grad Norm: 0.02 | LR: 2.34e-05 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:14:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 0.042500 | Val Loss: 0.062715 | Grad Norm: 0.01 | LR: 4.11e-06 | Time: 38.2s\n",
      "\u001b[32m[2025-10-11 14:14:13 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-11 14:14:13 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:14:13 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 14:14:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 0.055671 | Val Loss: 0.079303 | Grad Norm: 0.08 | LR: 5.06e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:15:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 0.054041 | Val Loss: 0.079112 | Grad Norm: 0.05 | LR: 1.89e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:16:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 0.053418 | Val Loss: 0.078575 | Grad Norm: 0.05 | LR: 3.33e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:16:26 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-11 14:16:26 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-11 14:16:26 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-11 14:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.093217 | Val Loss: 0.131896 | Grad Norm: 0.19 | LR: 5.06e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:17:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.089755 | Val Loss: 0.129703 | Grad Norm: 0.18 | LR: 1.89e-05 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:18:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.087819 | Val Loss: 0.128802 | Grad Norm: 0.13 | LR: 3.33e-06 | Time: 38.3s\n",
      "\u001b[32m[2025-10-11 14:18:40 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/sgra/layer_statistics.json\n",
      "\u001b[32m[2025-10-11 14:18:40 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/sgra/model2/layer_statistics.json\n",
      "\u001b[32m[2025-10-11 14:18:41 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 4179.01s (69.65min)\n",
      "\u001b[32m[2025-10-11 14:18:41 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-11 14:18:44 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/sgra/model2\n",
      "\u001b[32m[2025-10-11 14:18:44 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 166/166 [01:55<00:00,  1.44it/s]\n",
      "wikitext2:5.556035995483398\n",
      "\u001b[32m[2025-10-11 14:20:44 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.56\n",
      "\u001b[32m[2025-10-11 14:20:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-11 14:20:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-11 14:20:48 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 168468.91it/s]\n",
      "\u001b[32m[2025-10-11 14:20:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5391.95it/s]\n",
      "\u001b[32m[2025-10-11 14:20:57 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2339.65it/s]\n",
      "\u001b[32m[2025-10-11 14:20:58 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1814.55it/s]\n",
      "\u001b[32m[2025-10-11 14:20:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:30<00:00, 33.85it/s]\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6803|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5657|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7527|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7576|±  |0.0088|\n",
      "|          |       |none  |     0|acc_norm|0.7382|±  |0.0090|\n",
      "|piqa      |      1|none  |     0|acc     |0.7764|±  |0.0097|\n",
      "|          |       |none  |     0|acc_norm|0.7911|±  |0.0095|\n",
      "\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 69.50%\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/sgra/results.json\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-11 14:48:48 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file $SENSITIVITY_FILE \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --wbits 4 \\\n",
    "  --group_size 128 \\\n",
    "  --use_adaptive_training \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/sgra \\\n",
    "  --save_quant_dir $BASE_OUTPUT/sgra/model2 \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8927cee",
   "metadata": {},
   "source": [
    "# MPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aQVPLVaRMLIk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQVPLVaRMLIk",
    "outputId": "36cb831b-56b3-4af7-f036-a12bdf6a3c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--mpq_strategy', 'conservative', '--target_avg_bits', '4.0', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/mpq4', '--save_quant_dir', './output/research_experiments/mpq4/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/mpq4', save_quant_dir='./output/research_experiments/mpq4/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=4.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 04:46:22 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.61it/s]\n",
      "\u001b[32m[2025-10-30 04:46:23 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-30 04:46:23 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-30 04:46:23 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-30 04:46:23 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 04:46:26 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5]\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 04:46:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:46:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:47:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 04:47:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 04:47:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.002578 | Val Loss: 0.001322 | Grad Norm: 1.28 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 04:48:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.004369 | Val Loss: 0.000724 | Grad Norm: 1.44 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:48:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 04:48:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:49:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.001832 | Val Loss: 0.000730 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 04:49:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.001820 | Val Loss: 0.000728 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:49:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 04:49:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:50:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.001837 | Val Loss: 0.000743 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:50:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.001830 | Val Loss: 0.000741 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:51:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 04:51:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:51:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.001861 | Val Loss: 0.000761 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:52:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.001851 | Val Loss: 0.000760 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:52:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 04:52:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:52:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.001893 | Val Loss: 0.000793 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:53:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.001878 | Val Loss: 0.000789 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:53:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 04:53:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:53:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.001938 | Val Loss: 0.000842 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:54:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.001919 | Val Loss: 0.000830 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:54:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 04:54:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:55:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.001997 | Val Loss: 0.000901 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:55:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.001968 | Val Loss: 0.000884 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:55:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 04:55:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:56:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.002060 | Val Loss: 0.000974 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:56:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.002026 | Val Loss: 0.000957 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 04:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 04:57:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.002279 | Val Loss: 0.001221 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:57:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.002225 | Val Loss: 0.001187 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:58:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 04:58:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:58:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.002347 | Val Loss: 0.001312 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 04:59:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.002288 | Val Loss: 0.001277 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 04:59:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 04:59:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 04:59:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.002595 | Val Loss: 0.001599 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:00:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.002525 | Val Loss: 0.001554 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:00:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 05:00:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:01:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.002866 | Val Loss: 0.001904 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:01:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.002760 | Val Loss: 0.001862 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:01:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 05:01:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:02:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.003143 | Val Loss: 0.002260 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:02:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.003032 | Val Loss: 0.002197 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:02:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 05:02:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:03:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.003477 | Val Loss: 0.002674 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:03:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.003332 | Val Loss: 0.002597 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:04:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 05:04:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:04:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.003849 | Val Loss: 0.003132 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:04:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.003680 | Val Loss: 0.003065 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:05:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 05:05:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:05:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.004532 | Val Loss: 0.003993 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:06:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.004321 | Val Loss: 0.003870 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:06:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 05:06:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:06:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.005075 | Val Loss: 0.004696 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:07:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.004857 | Val Loss: 0.004584 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:07:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 05:07:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 05:08:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.005908 | Val Loss: 0.005768 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:08:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.005634 | Val Loss: 0.005643 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:08:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 05:08:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 05:09:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.010541 | Val Loss: 0.010149 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:09:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.009578 | Val Loss: 0.009925 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:09:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 05:09:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 05:10:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.015909 | Val Loss: 0.015896 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 05:10:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.014561 | Val Loss: 0.015559 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:11:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 05:11:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 05:11:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.021312 | Val Loss: 0.022085 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:12:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.019700 | Val Loss: 0.021749 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:12:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 05:12:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 05:12:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.028415 | Val Loss: 0.030267 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:13:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.026438 | Val Loss: 0.029863 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 05:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:13:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.037382 | Val Loss: 0.039924 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 05:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.034781 | Val Loss: 0.039440 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:14:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 05:14:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:15:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.047409 | Val Loss: 0.051575 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:15:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.044418 | Val Loss: 0.050999 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:15:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 05:15:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.059035 | Val Loss: 0.064624 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 05:16:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.055527 | Val Loss: 0.063939 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:16:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 05:16:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:17:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.073588 | Val Loss: 0.081429 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 05:17:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.069506 | Val Loss: 0.080665 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:18:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 05:18:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:18:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.089605 | Val Loss: 0.099931 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:19:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.085032 | Val Loss: 0.099012 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:19:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 05:19:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:19:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.111113 | Val Loss: 0.125189 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 05:20:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.105451 | Val Loss: 0.123929 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:20:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 05:20:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 05:20:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.136853 | Val Loss: 0.155044 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 05:21:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.130013 | Val Loss: 0.153449 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 05:21:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 05:21:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 05:22:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.148187 | Val Loss: 0.176469 | Grad Norm: 0.15 | LR: 5.14e-05 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 05:22:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.143245 | Val Loss: 0.175073 | Grad Norm: 0.08 | LR: 5.00e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:22:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 05:22:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 05:23:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.209912 | Val Loss: 0.257224 | Grad Norm: 0.82 | LR: 5.14e-05 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 05:23:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.202671 | Val Loss: 0.253980 | Grad Norm: 0.70 | LR: 5.00e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 05:23:59 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/mpq4/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 05:23:59 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/mpq4/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 05:24:00 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2256.15s (37.60min)\n",
      "\u001b[32m[2025-10-30 05:24:00 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 05:24:11 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/mpq4/model\n",
      "\u001b[32m[2025-10-30 05:24:11 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:42<00:00,  1.02it/s]\n",
      "wikitext2:5.640643119812012\n",
      "\u001b[32m[2025-10-30 05:27:02 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.64\n",
      "2025-10-30 05:27:03.269050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761802023.288058   26522 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761802023.293946   26522 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761802023.310315   26522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761802023.310340   26522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761802023.310343   26522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761802023.310346   26522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 19.2MB/s]\n",
      "\u001b[32m[2025-10-30 05:27:07 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 05:27:08 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 05:27:12 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading readme: 7.02kB [00:00, 23.2MB/s]\n",
      "Downloading readme: 11.2kB [00:00, 41.6MB/s]\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127957.02it/s]\n",
      "\u001b[32m[2025-10-30 05:27:28 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2398.84it/s]\n",
      "\u001b[32m[2025-10-30 05:27:34 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1053.13it/s]\n",
      "\u001b[32m[2025-10-30 05:27:36 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1017.95it/s]\n",
      "\u001b[32m[2025-10-30 05:27:38 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:48<00:00, 24.63it/s]\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6914|±  |0.0130|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5645|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7483|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7534|±  |0.0088|\n",
      "|          |       |none  |     0|acc_norm|0.7332|±  |0.0091|\n",
      "|piqa      |      1|none  |     0|acc     |0.7775|±  |0.0097|\n",
      "|          |       |none  |     0|acc_norm|0.7867|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 69.67%\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/mpq4/results.json\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 06:06:09 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy conservative \\\n",
    "  --target_avg_bits 4.0 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/mpq4 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/mpq4/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f983a7",
   "metadata": {},
   "source": [
    "# Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2eee4",
   "metadata": {
    "id": "c5e2eee4",
    "outputId": "ccaa4519-f53b-4f60-ab12-bfda7ed31b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Llama-2-7b-hf', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '4', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/baseline', '--save_quant_dir', './output/research_experiments/baseline/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(main_block_ap.py 115)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/baseline', save_quant_dir='./output/research_experiments/baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(main_block_ap.py 119)\u001b[0m: INFO net is None, setting as Llama-2-7b-hf\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 17.01it/s]\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(main_block_ap.py 134)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(main_block_ap.py 141)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(main_block_ap.py 143)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-11 12:12:50 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-11 12:12:52 root]\u001b[0m\u001b[33m(block_ap.py 125)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-11 12:12:52 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:13:14 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:1.6712079968783655e-06 val_loss:8.384470220335061e-07 quant_lr:5.1362214303938806e-05 norm:0.00025013 max memory_allocated 6758.517578125 time 18.45390033721924 \n",
      "\u001b[32m[2025-10-11 12:13:33 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:7.637599424015207e-07 val_loss:7.273423534570611e-07 quant_lr:5e-06 norm:0.00004257 max memory_allocated 6821.986328125 time 18.392817497253418 \n",
      "\u001b[32m[2025-10-11 12:13:36 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:37 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:37 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:38 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:39 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:40 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:41 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:13:41 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:14:02 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0024309654254466295 val_loss:0.001357993227429688 quant_lr:5.1362214303938806e-05 norm:0.44934312 max memory_allocated 6821.986328125 time 18.514383792877197 \n",
      "\u001b[32m[2025-10-11 12:14:20 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.002375029493123293 val_loss:0.0004589686286635697 quant_lr:5e-06 norm:0.56425631 max memory_allocated 6822.486328125 time 18.52733540534973 \n",
      "\u001b[32m[2025-10-11 12:14:23 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:25 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:26 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:27 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:28 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:14:28 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:14:49 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.0009106125216931105 val_loss:0.00045781783410348 quant_lr:5.1362214303938806e-05 norm:0.00097528 max memory_allocated 6822.486328125 time 18.55099058151245 \n",
      "\u001b[32m[2025-10-11 12:15:08 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.0008624120382592082 val_loss:0.00043946548248641193 quant_lr:5e-06 norm:0.00091541 max memory_allocated 6822.486328125 time 18.550533533096313 \n",
      "\u001b[32m[2025-10-11 12:15:11 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:11 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:12 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:12 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:13 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:14 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:15 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:15:15 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:15:38 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0008951010531745851 val_loss:0.00045340327778831124 quant_lr:5.1362214303938806e-05 norm:0.00062881 max memory_allocated 6822.486328125 time 18.56622052192688 \n",
      "\u001b[32m[2025-10-11 12:15:56 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0008620148873887956 val_loss:0.0004439626936800778 quant_lr:5e-06 norm:0.00050204 max memory_allocated 6822.486328125 time 18.575427532196045 \n",
      "\u001b[32m[2025-10-11 12:16:00 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:03 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:04 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:05 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:05 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:16:27 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0009331217734143138 val_loss:0.0005026102880947292 quant_lr:5.1362214303938806e-05 norm:0.00139700 max memory_allocated 6822.486328125 time 18.575617790222168 \n",
      "\u001b[32m[2025-10-11 12:16:46 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0009007527260109782 val_loss:0.0004896458121947944 quant_lr:5e-06 norm:0.00112283 max memory_allocated 6822.486328125 time 18.575584173202515 \n",
      "\u001b[32m[2025-10-11 12:16:49 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:51 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:52 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:53 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:54 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:16:54 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:17:15 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0010084756650030613 val_loss:0.0005944200092926621 quant_lr:5.1362214303938806e-05 norm:0.00119824 max memory_allocated 6822.486328125 time 18.57510995864868 \n",
      "\u001b[32m[2025-10-11 12:17:34 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.0009821868734434247 val_loss:0.0005851490423083305 quant_lr:5e-06 norm:0.00096190 max memory_allocated 6822.486328125 time 18.573885917663574 \n",
      "\u001b[32m[2025-10-11 12:17:38 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:38 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:38 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:39 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:40 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:41 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:42 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:17:42 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:18:04 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0011380111100152135 val_loss:0.0007334968540817499 quant_lr:5.1362214303938806e-05 norm:0.00207794 max memory_allocated 6822.486328125 time 18.589694499969482 \n",
      "\u001b[32m[2025-10-11 12:18:23 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0011013246839866042 val_loss:0.0007199430838227272 quant_lr:5e-06 norm:0.00210388 max memory_allocated 6822.486328125 time 18.5898015499115 \n",
      "\u001b[32m[2025-10-11 12:18:26 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:27 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:27 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:28 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:29 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:30 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:31 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:18:31 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:18:53 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0013061343925073743 val_loss:0.0009216638864018023 quant_lr:5.1362214303938806e-05 norm:0.00366596 max memory_allocated 6822.486328125 time 18.596463918685913 \n",
      "\u001b[32m[2025-10-11 12:19:11 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.001249669468961656 val_loss:0.0009003505110740662 quant_lr:5e-06 norm:0.00256885 max memory_allocated 6822.486328125 time 18.593981981277466 \n",
      "\u001b[32m[2025-10-11 12:19:15 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:16 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:16 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:17 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:18 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:18 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:20 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:19:20 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:19:42 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.0015185525408014655 val_loss:0.0011785412207245827 quant_lr:5.1362214303938806e-05 norm:0.00388721 max memory_allocated 6822.486328125 time 18.596308708190918 \n",
      "\u001b[32m[2025-10-11 12:20:00 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.0014474004274234176 val_loss:0.0011544973822310567 quant_lr:5e-06 norm:0.00303266 max memory_allocated 6822.486328125 time 18.59971022605896 \n",
      "\u001b[32m[2025-10-11 12:20:04 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:05 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:05 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:05 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:06 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:07 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:08 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:08 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:20:30 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.001764635439030826 val_loss:0.0014945497969165444 quant_lr:5.1362214303938806e-05 norm:0.00399505 max memory_allocated 6822.486328125 time 18.640845775604248 \n",
      "\u001b[32m[2025-10-11 12:20:49 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.001679479144513607 val_loss:0.0014654066180810332 quant_lr:5e-06 norm:0.00304713 max memory_allocated 6822.486328125 time 18.650797843933105 \n",
      "\u001b[32m[2025-10-11 12:20:53 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:53 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:54 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:54 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:55 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:56 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:57 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:20:57 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:21:19 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.0020508181769400835 val_loss:0.0018510040827095509 quant_lr:5.1362214303938806e-05 norm:0.00531414 max memory_allocated 6822.486328125 time 18.60743498802185 \n",
      "\u001b[32m[2025-10-11 12:21:38 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.00194639025721699 val_loss:0.0018117883009836078 quant_lr:5e-06 norm:0.00416648 max memory_allocated 6822.486328125 time 18.608152866363525 \n",
      "\u001b[32m[2025-10-11 12:21:42 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:42 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:43 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:43 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:44 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:45 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:46 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:21:46 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:22:08 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.002286977833136916 val_loss:0.002183102071285248 quant_lr:5.1362214303938806e-05 norm:0.00384887 max memory_allocated 6822.486328125 time 18.654025316238403 \n",
      "\u001b[32m[2025-10-11 12:22:27 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.0021720470394939184 val_loss:0.0021423581056296825 quant_lr:5e-06 norm:0.00386682 max memory_allocated 6822.486328125 time 18.668777227401733 \n",
      "\u001b[32m[2025-10-11 12:22:31 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:31 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:32 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:32 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:33 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:34 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:35 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:22:35 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:22:57 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.002588475588709116 val_loss:0.0025937682949006557 quant_lr:5.1362214303938806e-05 norm:0.00521782 max memory_allocated 6822.486328125 time 18.610556602478027 \n",
      "\u001b[32m[2025-10-11 12:23:16 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0024685573298484087 val_loss:0.002552242949604988 quant_lr:5e-06 norm:0.00522293 max memory_allocated 6822.486328125 time 18.60865616798401 \n",
      "\u001b[32m[2025-10-11 12:23:20 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:20 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:20 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:21 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:22 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:23 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:23:24 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:23:46 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.002910511801019311 val_loss:0.003038401249796152 quant_lr:5.1362214303938806e-05 norm:0.00557241 max memory_allocated 6822.486328125 time 18.6098210811615 \n",
      "\u001b[32m[2025-10-11 12:24:04 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.002765771234408021 val_loss:0.0029880828224122524 quant_lr:5e-06 norm:0.00603841 max memory_allocated 6822.486328125 time 18.612807750701904 \n",
      "\u001b[32m[2025-10-11 12:24:08 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:09 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:09 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:10 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:11 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:12 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:13 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:13 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:24:35 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.003295407397672534 val_loss:0.0035846419632434845 quant_lr:5.1362214303938806e-05 norm:0.00472815 max memory_allocated 6822.486328125 time 18.65579915046692 \n",
      "\u001b[32m[2025-10-11 12:24:53 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.0031157904304564 val_loss:0.0035208200570195913 quant_lr:5e-06 norm:0.00417979 max memory_allocated 6822.486328125 time 18.680798768997192 \n",
      "\u001b[32m[2025-10-11 12:24:57 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:58 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:58 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:24:59 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:00 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:02 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:25:23 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.0037499391473829746 val_loss:0.004229157231748104 quant_lr:5.1362214303938806e-05 norm:0.00512373 max memory_allocated 6822.486328125 time 18.611504077911377 \n",
      "\u001b[32m[2025-10-11 12:25:42 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0035504449624568224 val_loss:0.004157873336225748 quant_lr:5e-06 norm:0.00522691 max memory_allocated 6822.486328125 time 18.609193801879883 \n",
      "\u001b[32m[2025-10-11 12:25:46 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:46 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:47 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:47 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:48 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:49 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:25:50 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:26:12 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.004601246677339077 val_loss:0.005362235009670258 quant_lr:5.1362214303938806e-05 norm:0.00721359 max memory_allocated 6822.486328125 time 18.618316173553467 \n",
      "\u001b[32m[2025-10-11 12:26:31 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.004340264480561018 val_loss:0.005253632087260485 quant_lr:5e-06 norm:0.00654245 max memory_allocated 6822.486328125 time 18.616403102874756 \n",
      "\u001b[32m[2025-10-11 12:26:35 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:35 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:36 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:36 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:37 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:38 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:39 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:26:39 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:27:01 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.00527651933953166 val_loss:0.006330982781946659 quant_lr:5.1362214303938806e-05 norm:0.00804919 max memory_allocated 6822.486328125 time 18.616178274154663 \n",
      "\u001b[32m[2025-10-11 12:27:20 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.0050082434900105 val_loss:0.006222689524292946 quant_lr:5e-06 norm:0.00911498 max memory_allocated 6822.486328125 time 18.61342215538025 \n",
      "\u001b[32m[2025-10-11 12:27:23 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:25 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:26 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:27 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:28 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:27:28 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:27:50 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.0063528032042086124 val_loss:0.00781581923365593 quant_lr:5.1362214303938806e-05 norm:0.00959705 max memory_allocated 6822.486328125 time 18.617131233215332 \n",
      "\u001b[32m[2025-10-11 12:28:08 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.006030951626598835 val_loss:0.007705663330852985 quant_lr:5e-06 norm:0.00918169 max memory_allocated 6822.486328125 time 18.616869926452637 \n",
      "\u001b[32m[2025-10-11 12:28:12 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:13 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:13 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:14 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:15 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:16 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:17 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:28:17 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:28:39 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.007593613583594561 val_loss:0.009604472666978836 quant_lr:5.1362214303938806e-05 norm:0.00864714 max memory_allocated 6822.486328125 time 18.629697561264038 \n",
      "\u001b[32m[2025-10-11 12:28:57 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.00724170682951808 val_loss:0.009485382586717606 quant_lr:5e-06 norm:0.00970494 max memory_allocated 6822.486328125 time 18.635480880737305 \n",
      "\u001b[32m[2025-10-11 12:29:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:04 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:05 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:06 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:06 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:29:28 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.009370949119329453 val_loss:0.01217279676347971 quant_lr:5.1362214303938806e-05 norm:0.01097517 max memory_allocated 6822.486328125 time 18.626951932907104 \n",
      "\u001b[32m[2025-10-11 12:29:46 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.008971916511654854 val_loss:0.01201588660478592 quant_lr:5e-06 norm:0.01300201 max memory_allocated 6822.486328125 time 18.632015705108643 \n",
      "\u001b[32m[2025-10-11 12:29:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:51 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:51 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:52 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:53 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:54 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:55 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:29:55 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:30:17 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.01118712779134512 val_loss:0.014862732030451298 quant_lr:5.1362214303938806e-05 norm:0.00879685 max memory_allocated 6822.486328125 time 18.62653875350952 \n",
      "\u001b[32m[2025-10-11 12:30:35 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.010718992911279202 val_loss:0.014720175415277481 quant_lr:5e-06 norm:0.01007307 max memory_allocated 6822.486328125 time 18.629092931747437 \n",
      "\u001b[32m[2025-10-11 12:30:39 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:40 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:40 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:40 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:41 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:43 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:44 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:30:44 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:31:06 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.01364446897059679 val_loss:0.018546495586633682 quant_lr:5.1362214303938806e-05 norm:0.01980902 max memory_allocated 6822.486328125 time 18.652970790863037 \n",
      "\u001b[32m[2025-10-11 12:31:24 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.013108182698488235 val_loss:0.018376657739281654 quant_lr:5e-06 norm:0.02331218 max memory_allocated 6822.486328125 time 18.62292981147766 \n",
      "\u001b[32m[2025-10-11 12:31:28 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:29 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:29 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:30 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:31 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:32 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:33 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:31:33 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:31:55 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.01614335924386978 val_loss:0.022223830223083496 quant_lr:5.1362214303938806e-05 norm:0.01014803 max memory_allocated 6822.486328125 time 18.617635250091553 \n",
      "\u001b[32m[2025-10-11 12:32:13 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.01552409678697586 val_loss:0.022058458998799324 quant_lr:5e-06 norm:0.01028869 max memory_allocated 6822.486328125 time 18.619298696517944 \n",
      "\u001b[32m[2025-10-11 12:32:17 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:18 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:18 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:18 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:19 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:20 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:22 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:32:22 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:32:43 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.019077053293585777 val_loss:0.026745541021227837 quant_lr:5.1362214303938806e-05 norm:0.01285013 max memory_allocated 6822.486328125 time 18.617470026016235 \n",
      "\u001b[32m[2025-10-11 12:33:02 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.018404310569167137 val_loss:0.02657792717218399 quant_lr:5e-06 norm:0.01654597 max memory_allocated 6822.486328125 time 18.616137981414795 \n",
      "\u001b[32m[2025-10-11 12:33:06 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:06 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:07 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:07 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:08 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:09 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:10 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:11 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:33:32 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.022416573017835617 val_loss:0.03154982998967171 quant_lr:5.1362214303938806e-05 norm:0.01641321 max memory_allocated 6822.486328125 time 18.627638816833496 \n",
      "\u001b[32m[2025-10-11 12:33:51 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.021603023633360863 val_loss:0.031333260238170624 quant_lr:5e-06 norm:0.01441920 max memory_allocated 6822.486328125 time 18.621731758117676 \n",
      "\u001b[32m[2025-10-11 12:33:55 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:55 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:56 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:56 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:57 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:58 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:59 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:33:59 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:34:21 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.026529286056756973 val_loss:0.037755873054265976 quant_lr:5.1362214303938806e-05 norm:0.01526187 max memory_allocated 6822.486328125 time 18.62205481529236 \n",
      "\u001b[32m[2025-10-11 12:34:40 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.025603266432881355 val_loss:0.037502892315387726 quant_lr:5e-06 norm:0.01675004 max memory_allocated 6822.486328125 time 18.62607741355896 \n",
      "\u001b[32m[2025-10-11 12:34:44 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:44 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:45 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:45 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:46 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:47 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:48 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:34:48 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:35:10 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.031020116060972214 val_loss:0.044369667768478394 quant_lr:5.1362214303938806e-05 norm:0.01562155 max memory_allocated 6822.486328125 time 18.624104261398315 \n",
      "\u001b[32m[2025-10-11 12:35:29 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.029977068305015564 val_loss:0.04411578178405762 quant_lr:5e-06 norm:0.01554145 max memory_allocated 6822.486328125 time 18.617672204971313 \n",
      "\u001b[32m[2025-10-11 12:35:33 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:33 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:33 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:34 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:35 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:36 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:37 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:35:37 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:35:59 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.03689241036772728 val_loss:0.053278855979442596 quant_lr:5.1362214303938806e-05 norm:0.01873574 max memory_allocated 6822.486328125 time 18.63616180419922 \n",
      "\u001b[32m[2025-10-11 12:36:18 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.03560861572623253 val_loss:0.05292810499668121 quant_lr:5e-06 norm:0.01883941 max memory_allocated 6822.486328125 time 18.63684844970703 \n",
      "\u001b[32m[2025-10-11 12:36:21 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:22 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:22 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:23 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:24 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:25 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:26 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:36:26 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:36:48 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.04386371374130249 val_loss:0.06361092627048492 quant_lr:5.1362214303938806e-05 norm:0.02204385 max memory_allocated 6822.486328125 time 18.631622076034546 \n",
      "\u001b[32m[2025-10-11 12:37:07 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.042304255068302155 val_loss:0.06318968534469604 quant_lr:5e-06 norm:0.02068797 max memory_allocated 6822.486328125 time 18.634199619293213 \n",
      "\u001b[32m[2025-10-11 12:37:10 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:11 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:11 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:12 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:13 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:14 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:15 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:37:15 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:37:37 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.05582777410745621 val_loss:0.07987265288829803 quant_lr:5.1362214303938806e-05 norm:0.10468683 max memory_allocated 6822.486328125 time 18.622807502746582 \n",
      "\u001b[32m[2025-10-11 12:37:56 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.05242666229605675 val_loss:0.0789833590388298 quant_lr:5e-06 norm:0.07587151 max memory_allocated 6822.486328125 time 18.629682302474976 \n",
      "\u001b[32m[2025-10-11 12:38:00 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:00 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:01 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:02 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:03 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:04 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:04 root]\u001b[0m\u001b[33m(block_ap.py 162)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-11 12:38:26 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.09587650001049042 val_loss:0.13520392775535583 quant_lr:5.1362214303938806e-05 norm:0.31286490 max memory_allocated 6822.486328125 time 18.655665397644043 \n",
      "\u001b[32m[2025-10-11 12:38:45 root]\u001b[0m\u001b[33m(block_ap.py 262)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.08876922726631165 val_loss:0.1323622316122055 quant_lr:5e-06 norm:0.30621797 max memory_allocated 6822.486328125 time 18.639068841934204 \n",
      "\u001b[32m[2025-10-11 12:38:49 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:49 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:50 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:51 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:52 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:53 root]\u001b[0m\u001b[33m(block_ap.py 296)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-11 12:38:54 root]\u001b[0m\u001b[33m(main_block_ap.py 162)\u001b[0m: INFO 1563.549506187439\n",
      "\u001b[32m[2025-10-11 12:38:54 root]\u001b[0m\u001b[33m(main_block_ap.py 165)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-11 12:38:57 root]\u001b[0m\u001b[33m(main_block_ap.py 168)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 166/166 [01:55<00:00,  1.43it/s]\n",
      "wikitext2:5.553813457489014\n",
      "\u001b[32m[2025-10-11 12:40:58 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 5.55\n",
      "\u001b[32m[2025-10-11 12:40:58 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-11 12:40:59 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-11 12:41:01 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 169994.02it/s]\n",
      "\u001b[32m[2025-10-11 12:41:08 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5389.46it/s]\n",
      "\u001b[32m[2025-10-11 12:41:11 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2322.36it/s]\n",
      "\u001b[32m[2025-10-11 12:41:12 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1813.44it/s]\n",
      "\u001b[32m[2025-10-11 12:41:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:27<00:00, 33.92it/s]\n",
      "\u001b[32m[2025-10-11 13:08:58 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6764|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5634|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7517|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7605|±  |0.0088|\n",
      "|          |       |none  |     0|acc_norm|0.7365|±  |0.0090|\n",
      "|piqa      |      1|none  |     0|acc     |0.7775|±  |0.0097|\n",
      "|          |       |none  |     0|acc_norm|0.7797|±  |0.0097|\n",
      "\n",
      "\u001b[32m[2025-10-11 13:08:58 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 69.45%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "  --model $MODEL \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --wbits 4 \\\n",
    "  --group_size 128 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/baseline \\\n",
    "  --save_quant_dir $BASE_OUTPUT/baseline/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3d00c",
   "metadata": {},
   "source": [
    "# MPQ + SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oeJQbmvp4yfl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeJQbmvp4yfl",
    "outputId": "7adb9996-5db1-47b2-ccb4-195dda765c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--use_adaptive_training', '--mpq_strategy', 'conservative', '--target_avg_bits', '4.0', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/combined4', '--save_quant_dir', './output/research_experiments/combined4/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/combined4', save_quant_dir='./output/research_experiments/combined4/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 03:10:22 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 609/609 [00:00<00:00, 4.18MB/s]\n",
      "tokenizer_config.json: 100% 776/776 [00:00<00:00, 6.74MB/s]\n",
      "tokenizer.model: 100% 500k/500k [00:00<00:00, 1.27MB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.72MB/s]\n",
      "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 9.03MB/s]\n",
      "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 104MB/s]\n",
      "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 8.96k/9.98G [00:00<305:39:23, 9.07kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 26.6M/9.98G [00:02<11:12, 14.8MB/s]    \u001b[A\n",
      "model-00001-of-00002.safetensors:  11% 1.11G/9.98G [00:02<00:10, 810MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  17% 1.65G/9.98G [00:03<00:12, 656MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 1.97G/9.98G [00:04<00:15, 533MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22% 2.19G/9.98G [00:06<00:28, 276MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:06<00:23, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25% 2.53G/9.98G [00:06<00:19, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27% 2.68G/9.98G [00:06<00:16, 445MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 2.85G/9.98G [00:10<00:52, 135MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 3.97G/9.98G [00:10<00:13, 438MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43% 4.33G/9.98G [00:11<00:10, 530MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46% 4.63G/9.98G [00:11<00:09, 583MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49% 4.87G/9.98G [00:11<00:08, 576MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 5.05G/9.98G [00:12<00:12, 409MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52% 5.18G/9.98G [00:13<00:14, 336MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53% 5.31G/9.98G [00:14<00:15, 298MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.45G/9.98G [00:15<00:16, 269MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.52G/9.98G [00:15<00:16, 263MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56% 5.57G/9.98G [00:15<00:15, 281MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56% 5.63G/9.98G [00:15<00:16, 263MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57% 5.71G/9.98G [00:15<00:14, 287MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58% 5.77G/9.98G [00:16<00:13, 310MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58% 5.83G/9.98G [00:16<00:21, 193MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 5.86G/9.98G [00:17<00:28, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 5.89G/9.98G [00:17<00:26, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 5.95G/9.98G [00:17<00:22, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 5.99G/9.98G [00:17<00:21, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 6.02G/9.98G [00:18<00:26, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61% 6.04G/9.98G [00:18<00:28, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:18<00:21, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 6.16G/9.98G [00:18<00:16, 233MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 6.20G/9.98G [00:18<00:15, 238MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.25G/9.98G [00:18<00:12, 287MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.30G/9.98G [00:19<00:12, 294MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64% 6.36G/9.98G [00:19<00:14, 257MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64% 6.42G/9.98G [00:19<00:16, 215MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.50G/9.98G [00:20<00:14, 242MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.53G/9.98G [00:20<00:15, 225MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66% 6.57G/9.98G [00:21<00:29, 115MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.64G/9.98G [00:21<00:21, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.69G/9.98G [00:21<00:21, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.74G/9.98G [00:22<00:25, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.78G/9.98G [00:22<00:22, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.81G/9.98G [00:22<00:21, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70% 6.96G/9.98G [00:22<00:09, 317MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70% 7.02G/9.98G [00:22<00:08, 348MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 7.10G/9.98G [00:23<00:09, 313MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72% 7.15G/9.98G [00:23<00:09, 307MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.28G/9.98G [00:23<00:06, 434MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.33G/9.98G [00:23<00:05, 451MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.45G/9.98G [00:23<00:04, 590MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.53G/9.98G [00:23<00:05, 478MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76% 7.60G/9.98G [00:24<00:07, 321MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.68G/9.98G [00:24<00:06, 371MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.73G/9.98G [00:24<00:08, 251MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 7.80G/9.98G [00:25<00:09, 218MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:25<00:11, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.92G/9.98G [00:25<00:08, 242MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80% 7.97G/9.98G [00:25<00:07, 263MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.06G/9.98G [00:26<00:06, 317MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.10G/9.98G [00:26<00:06, 298MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82% 8.20G/9.98G [00:26<00:05, 353MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.27G/9.98G [00:26<00:04, 388MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.32G/9.98G [00:26<00:04, 382MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84% 8.40G/9.98G [00:27<00:04, 372MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 8.48G/9.98G [00:27<00:06, 237MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.54G/9.98G [00:27<00:05, 247MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.61G/9.98G [00:28<00:10, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 8.71G/9.98G [00:28<00:06, 198MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88% 8.78G/9.98G [00:29<00:07, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.86G/9.98G [00:30<00:06, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.88G/9.98G [00:30<00:07, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90% 8.95G/9.98G [00:30<00:05, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:30<00:04, 202MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.05G/9.98G [00:30<00:04, 201MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:31<00:05, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 9.15G/9.98G [00:31<00:05, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 9.19G/9.98G [00:32<00:07, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93% 9.29G/9.98G [00:32<00:04, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 9.33G/9.98G [00:33<00:06, 96.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.43G/9.98G [00:34<00:03, 142MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.50G/9.98G [00:34<00:02, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96% 9.56G/9.98G [00:34<00:02, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96% 9.61G/9.98G [00:35<00:02, 132MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 9.74G/9.98G [00:35<00:01, 223MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 9.78G/9.98G [00:36<00:01, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99% 9.84G/9.98G [00:36<00:00, 136MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99% 9.91G/9.98G [00:37<00:00, 96.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:38<00:00, 260MB/s]\n",
      "Downloading shards:  50% 1/2 [00:38<00:38, 38.57s/it]\n",
      "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 8.66k/3.50G [00:01<150:02:34, 6.48kB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 666k/3.50G [00:01<1:31:51, 635kB/s]    \u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 3.26M/3.50G [00:01<17:16, 3.37MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0% 9.28M/3.50G [00:02<11:58, 4.86MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2% 65.9M/3.50G [00:02<01:10, 48.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2% 85.9M/3.50G [00:02<00:55, 61.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3% 106M/3.50G [00:03<00:47, 70.9MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:   4% 131M/3.50G [00:03<00:36, 91.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4% 149M/3.50G [00:03<00:34, 98.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5% 168M/3.50G [00:03<00:36, 92.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5% 181M/3.50G [00:03<00:34, 97.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6% 194M/3.50G [00:03<00:35, 94.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6% 208M/3.50G [00:04<00:47, 70.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6% 221M/3.50G [00:04<00:41, 79.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7% 246M/3.50G [00:04<00:32, 101MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:   7% 261M/3.50G [00:04<00:32, 98.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 272M/3.50G [00:04<00:41, 76.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 282M/3.50G [00:05<00:59, 54.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 291M/3.50G [00:05<01:04, 49.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9% 310M/3.50G [00:05<00:51, 62.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10% 335M/3.50G [00:05<00:35, 89.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10% 355M/3.50G [00:06<00:42, 73.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11% 383M/3.50G [00:06<00:30, 103MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  12% 412M/3.50G [00:06<00:29, 104MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13% 462M/3.50G [00:06<00:20, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14% 499M/3.50G [00:07<00:31, 95.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15% 527M/3.50G [00:07<00:27, 110MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  19% 651M/3.50G [00:07<00:11, 250MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20% 696M/3.50G [00:07<00:13, 213MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21% 734M/3.50G [00:08<00:13, 210MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23% 791M/3.50G [00:09<00:27, 96.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27% 942M/3.50G [00:09<00:12, 202MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  30% 1.04G/3.50G [00:09<00:11, 214MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31% 1.10G/3.50G [00:10<00:10, 222MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33% 1.14G/3.50G [00:10<00:11, 210MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34% 1.20G/3.50G [00:10<00:10, 222MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36% 1.26G/3.50G [00:10<00:10, 213MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38% 1.32G/3.50G [00:11<00:10, 216MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39% 1.36G/3.50G [00:11<00:11, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40% 1.42G/3.50G [00:11<00:10, 194MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42% 1.46G/3.50G [00:12<00:11, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45% 1.57G/3.50G [00:12<00:07, 272MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46% 1.62G/3.50G [00:12<00:06, 281MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47% 1.66G/3.50G [00:13<00:13, 132MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50% 1.74G/3.50G [00:13<00:10, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52% 1.82G/3.50G [00:14<00:10, 156MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56% 1.97G/3.50G [00:14<00:05, 270MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61% 2.12G/3.50G [00:14<00:04, 298MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63% 2.20G/3.50G [00:15<00:05, 247MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64% 2.24G/3.50G [00:15<00:05, 249MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66% 2.31G/3.50G [00:15<00:04, 268MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70% 2.44G/3.50G [00:15<00:03, 338MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73% 2.56G/3.50G [00:16<00:04, 228MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75% 2.63G/3.50G [00:16<00:03, 236MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76% 2.67G/3.50G [00:18<00:07, 113MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78% 2.75G/3.50G [00:18<00:05, 138MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81% 2.83G/3.50G [00:19<00:04, 141MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83% 2.90G/3.50G [00:19<00:03, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85% 2.96G/3.50G [00:19<00:03, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86% 3.02G/3.50G [00:19<00:02, 208MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87% 3.06G/3.50G [00:20<00:02, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89% 3.12G/3.50G [00:20<00:02, 142MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91% 3.19G/3.50G [00:21<00:02, 151MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93% 3.24G/3.50G [00:21<00:01, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94% 3.31G/3.50G [00:22<00:01, 99.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:23<00:01, 93.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98% 3.43G/3.50G [00:24<00:00, 94.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:24<00:00, 143MB/s]\n",
      "Downloading shards: 100% 2/2 [01:03<00:00, 31.58s/it]\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.95it/s]\n",
      "generation_config.json: 100% 188/188 [00:00<00:00, 1.83MB/s]\n",
      "\u001b[32m[2025-10-30 03:11:29 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 17.1MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 1.96MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:01<00:00, 4.57MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 1.94MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 96942.90 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 748800.05 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 623639.00 examples/s]\n",
      "Downloading readme: 10.5kB [00:00, 37.7MB/s]\n",
      "\u001b[32m[2025-10-30 03:12:23 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 03:12:25 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5]\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:12:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 03:13:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 03:13:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-30 03:14:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 03:14:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:14:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-30 03:14:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000954 | Val Loss: 0.000950 | Grad Norm: 0.57 | LR: 5.71e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:14:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.001907 | Val Loss: 0.000329 | Grad Norm: 0.94 | LR: 3.46e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:15:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000508 | Val Loss: 0.000202 | Grad Norm: 0.42 | LR: 1.23e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:15:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000253 | Val Loss: 0.000096 | Grad Norm: 0.29 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 03:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:16:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:16:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000153 | Val Loss: 0.000102 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:16:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000143 | Val Loss: 0.000098 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:17:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000141 | Val Loss: 0.000096 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:17:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 03:17:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:17:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:18:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000155 | Val Loss: 0.000108 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:18:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000150 | Val Loss: 0.000106 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:19:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000149 | Val Loss: 0.000106 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:19:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 03:19:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:19:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:19:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000172 | Val Loss: 0.000127 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:20:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000166 | Val Loss: 0.000125 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:20:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000164 | Val Loss: 0.000124 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:20:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 03:20:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:20:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:21:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000196 | Val Loss: 0.000154 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:21:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000190 | Val Loss: 0.000152 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:22:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000186 | Val Loss: 0.000150 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:22:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 03:22:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:22:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:23:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000231 | Val Loss: 0.000192 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:23:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000221 | Val Loss: 0.000187 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:23:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000215 | Val Loss: 0.000184 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:24:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 03:24:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:24:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:24:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000274 | Val Loss: 0.000239 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000259 | Val Loss: 0.000233 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:25:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000251 | Val Loss: 0.000228 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:25:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 03:25:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:25:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:26:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000326 | Val Loss: 0.000306 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:26:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000307 | Val Loss: 0.000296 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:27:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000295 | Val Loss: 0.000291 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:27:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 03:27:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:27:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:27:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000542 | Val Loss: 0.000541 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:28:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000507 | Val Loss: 0.000522 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:28:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000486 | Val Loss: 0.000513 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:28:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 03:28:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 03:28:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:29:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.000578 | Val Loss: 0.000608 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:29:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000547 | Val Loss: 0.000592 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:30:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000525 | Val Loss: 0.000584 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:30:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 03:30:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:30:35 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:31:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.000837 | Val Loss: 0.000893 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:31:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.000786 | Val Loss: 0.000868 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:31:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.000755 | Val Loss: 0.000856 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:32:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 03:32:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:32:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:32:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.001103 | Val Loss: 0.001203 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:33:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.001036 | Val Loss: 0.001170 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:33:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.000998 | Val Loss: 0.001157 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:33:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 03:33:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:33:48 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:34:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.001382 | Val Loss: 0.001538 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:34:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.001304 | Val Loss: 0.001502 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:35:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.001256 | Val Loss: 0.001484 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:35:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 03:35:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:35:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:35:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.001711 | Val Loss: 0.001936 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.001614 | Val Loss: 0.001891 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:36:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.001558 | Val Loss: 0.001871 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:37:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 03:37:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:37:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:37:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.002092 | Val Loss: 0.002408 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:37:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.001981 | Val Loss: 0.002356 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:38:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.001916 | Val Loss: 0.002334 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:38:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 03:38:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:38:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:39:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.002748 | Val Loss: 0.003201 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:39:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.002605 | Val Loss: 0.003131 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:40:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.002516 | Val Loss: 0.003100 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:40:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 03:40:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:40:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.74e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:40:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.003297 | Val Loss: 0.003900 | Grad Norm: 0.01 | LR: 5.10e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:41:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.003145 | Val Loss: 0.003822 | Grad Norm: 0.01 | LR: 1.89e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:41:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.003050 | Val Loss: 0.003793 | Grad Norm: 0.00 | LR: 3.37e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:41:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 03:41:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-30 03:41:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.93e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:42:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.004121 | Val Loss: 0.004935 | Grad Norm: 0.01 | LR: 5.24e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:42:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.003946 | Val Loss: 0.004861 | Grad Norm: 0.01 | LR: 1.95e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 03:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.003841 | Val Loss: 0.004827 | Grad Norm: 0.00 | LR: 3.47e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 03:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 03:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.14e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.009091 | Val Loss: 0.009520 | Grad Norm: 0.01 | LR: 5.40e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:44:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.008098 | Val Loss: 0.009225 | Grad Norm: 0.01 | LR: 2.01e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:44:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.007846 | Val Loss: 0.009159 | Grad Norm: 0.00 | LR: 3.57e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:45:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 03:45:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 03:45:09 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.27e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 03:45:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.014639 | Val Loss: 0.015333 | Grad Norm: 0.01 | LR: 5.50e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:46:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.013268 | Val Loss: 0.014908 | Grad Norm: 0.01 | LR: 2.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:46:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.012887 | Val Loss: 0.014811 | Grad Norm: 0.01 | LR: 3.64e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:46:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 03:46:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 03:46:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.89e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:47:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.020175 | Val Loss: 0.021557 | Grad Norm: 0.01 | LR: 5.96e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:47:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.018640 | Val Loss: 0.021103 | Grad Norm: 0.01 | LR: 2.22e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:48:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.018203 | Val Loss: 0.021014 | Grad Norm: 0.01 | LR: 3.95e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:48:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 03:48:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-30 03:48:23 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.81e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:48:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.027563 | Val Loss: 0.029836 | Grad Norm: 0.02 | LR: 5.90e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:49:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.025685 | Val Loss: 0.029287 | Grad Norm: 0.01 | LR: 2.19e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:49:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.025138 | Val Loss: 0.029181 | Grad Norm: 0.01 | LR: 3.91e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:50:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 03:50:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:50:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:50:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.036941 | Val Loss: 0.039645 | Grad Norm: 0.02 | LR: 6.43e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:50:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.034306 | Val Loss: 0.038861 | Grad Norm: 0.01 | LR: 2.39e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:51:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.033625 | Val Loss: 0.038728 | Grad Norm: 0.01 | LR: 4.26e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 03:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.29e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:52:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.047260 | Val Loss: 0.051373 | Grad Norm: 0.02 | LR: 6.26e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:52:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.044310 | Val Loss: 0.050444 | Grad Norm: 0.01 | LR: 2.33e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:52:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.043515 | Val Loss: 0.050277 | Grad Norm: 0.01 | LR: 4.14e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:53:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 03:53:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:53:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.43e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.059254 | Val Loss: 0.064466 | Grad Norm: 0.02 | LR: 6.37e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:54:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.055802 | Val Loss: 0.063359 | Grad Norm: 0.02 | LR: 2.37e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:54:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.054859 | Val Loss: 0.063155 | Grad Norm: 0.01 | LR: 4.22e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:54:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 03:54:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:54:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 03:55:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.074199 | Val Loss: 0.081227 | Grad Norm: 0.03 | LR: 6.57e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.070157 | Val Loss: 0.079966 | Grad Norm: 0.02 | LR: 2.44e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.069047 | Val Loss: 0.079733 | Grad Norm: 0.02 | LR: 4.34e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:56:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 03:56:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:56:27 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-30 03:56:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.090278 | Val Loss: 0.099554 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:57:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.086207 | Val Loss: 0.098722 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:57:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 03:57:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:57:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.56e-05, Patience: 2\n",
      "\u001b[32m[2025-10-30 03:58:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.113642 | Val Loss: 0.125782 | Grad Norm: 0.03 | LR: 4.91e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 03:58:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.108518 | Val Loss: 0.124674 | Grad Norm: 0.02 | LR: 4.78e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 03:58:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 03:58:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-30 03:58:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.22e-05, Patience: 3\n",
      "\u001b[32m[2025-10-30 03:59:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 0.142080 | Val Loss: 0.157049 | Grad Norm: 0.04 | LR: 6.21e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-30 03:59:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 0.135318 | Val Loss: 0.154709 | Grad Norm: 0.03 | LR: 2.31e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 04:00:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 0.133340 | Val Loss: 0.154138 | Grad Norm: 0.03 | LR: 4.11e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-30 04:00:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 04:00:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:00:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 04:00:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 0.154308 | Val Loss: 0.178531 | Grad Norm: 0.09 | LR: 5.04e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 04:01:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 0.151200 | Val Loss: 0.177544 | Grad Norm: 0.07 | LR: 1.87e-05 | Time: 25.7s\n",
      "\u001b[32m[2025-10-30 04:01:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 0.149619 | Val Loss: 0.177194 | Grad Norm: 0.06 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 04:02:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 04:02:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-30 04:02:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-30 04:02:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.217928 | Val Loss: 0.258294 | Grad Norm: 0.41 | LR: 5.04e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-30 04:03:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.212990 | Val Loss: 0.256060 | Grad Norm: 0.37 | LR: 1.87e-05 | Time: 25.7s\n",
      "\u001b[32m[2025-10-30 04:03:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.210016 | Val Loss: 0.255162 | Grad Norm: 0.30 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-30 04:03:42 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/combined4/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 04:03:42 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/combined4/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 04:03:42 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3133.59s (52.23min)\n",
      "\u001b[32m[2025-10-30 04:03:42 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 04:03:54 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/combined4/model\n",
      "\u001b[32m[2025-10-30 04:03:54 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:47<00:00,  1.01s/it]\n",
      "wikitext2:5.643215179443359\n",
      "\u001b[32m[2025-10-30 04:06:51 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.64\n",
      "2025-10-30 04:06:52.881388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761797213.053548    2003 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761797213.097603    2003 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761797213.427983    2003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761797213.428016    2003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761797213.428020    2003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761797213.428025    2003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 15.6MB/s]\n",
      "\u001b[32m[2025-10-30 04:06:57 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 04:06:58 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 04:07:02 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 19.7MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 28.0MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:00<00:00, 82.3MB/s]\n",
      "Downloading data: 815kB [00:00, 45.3MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 34302.99 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37684.36 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 35627.91 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 27.5MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.87MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 2.91MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 638kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 365251.20 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 464773.17 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 267063.59 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 20.7MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 74.4MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 24.4MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 27.3MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 248630.31 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 265504.95 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 266605.91 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 6192.30 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6544.55 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 16.8MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 9.24MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 513kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 394kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1169925.59 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 636603.26 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 534733.67 examples/s]\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 131392.83it/s]\n",
      "\u001b[32m[2025-10-30 04:07:33 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2425.63it/s]\n",
      "\u001b[32m[2025-10-30 04:07:39 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1049.62it/s]\n",
      "\u001b[32m[2025-10-30 04:07:41 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1017.02it/s]\n",
      "\u001b[32m[2025-10-30 04:07:43 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:48<00:00, 24.63it/s]\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6906|±  |0.0130|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5640|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7490|±  |0.0043|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7559|±  |0.0088|\n",
      "|          |       |none  |     0|acc_norm|0.7386|±  |0.0090|\n",
      "|piqa      |      1|none  |     0|acc     |0.7737|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7856|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 69.60%\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/combined4/results.json\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 04:46:14 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --use_adaptive_training \\\n",
    "  --mpq_strategy conservative \\\n",
    "  --target_avg_bits 4.0 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/combined4 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/combined4/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GG5ktgi8Zjlx",
   "metadata": {
    "id": "GG5ktgi8Zjlx"
   },
   "source": [
    "## 3 bit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51106745",
   "metadata": {},
   "source": [
    "# uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vHxdgqf3ZoQc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHxdgqf3ZoQc",
    "outputId": "a220621b-1a75-49b0-f45e-b01911936c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Llama-2-7b-hf', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '3', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/baseline3', '--save_quant_dir', './output/research_experiments/baseline3/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 05:40:43 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/baseline3', save_quant_dir='./output/research_experiments/baseline3/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-29 05:40:43 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.76it/s]\n",
      "\u001b[32m[2025-10-29 05:40:44 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-29 05:40:44 root]\u001b[0m\u001b[33m(main_block_ap.py 170)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 05:40:44 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 05:40:44 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-29 05:40:47 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-29 05:40:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:41:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:5.199485258344794e-06 val_loss:3.5610592021839693e-06 quant_lr:5.1362214303938806e-05 norm:0.00023078 max memory_allocated 6758.517578125 time 24.078912258148193 \n",
      "\u001b[32m[2025-10-29 05:41:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:3.333157110319007e-06 val_loss:3.200935907443636e-06 quant_lr:5e-06 norm:0.00007465 max memory_allocated 6821.986328125 time 24.040998220443726 \n",
      "\u001b[32m[2025-10-29 05:41:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:41:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:42:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0026023518294095993 val_loss:0.0012831788044422865 quant_lr:5.1362214303938806e-05 norm:0.34457669 max memory_allocated 6821.986328125 time 24.3698468208313 \n",
      "\u001b[32m[2025-10-29 05:42:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.00314328633248806 val_loss:0.0022184355184435844 quant_lr:5e-06 norm:0.32928461 max memory_allocated 6822.486328125 time 24.33482527732849 \n",
      "\u001b[32m[2025-10-29 05:42:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:42:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:42:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:42:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:42:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:43:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:43:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:43:03 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:43:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.00215651816688478 val_loss:0.002079333644360304 quant_lr:5.1362214303938806e-05 norm:0.00165796 max memory_allocated 6822.486328125 time 24.27247714996338 \n",
      "\u001b[32m[2025-10-29 05:43:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.001927236095070839 val_loss:0.0019665900617837906 quant_lr:5e-06 norm:0.00255864 max memory_allocated 6822.486328125 time 24.371238946914673 \n",
      "\u001b[32m[2025-10-29 05:44:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:44:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:44:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0020655174739658833 val_loss:0.0020392725709825754 quant_lr:5.1362214303938806e-05 norm:0.00137885 max memory_allocated 6822.486328125 time 24.321160078048706 \n",
      "\u001b[32m[2025-10-29 05:45:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0018939728615805507 val_loss:0.0019596675410866737 quant_lr:5e-06 norm:0.00226588 max memory_allocated 6822.486328125 time 24.347992181777954 \n",
      "\u001b[32m[2025-10-29 05:45:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:45:18 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:45:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0021308506838977337 val_loss:0.0021391743794083595 quant_lr:5.1362214303938806e-05 norm:0.00168351 max memory_allocated 6822.486328125 time 24.338126182556152 \n",
      "\u001b[32m[2025-10-29 05:46:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0019723731093108654 val_loss:0.002085170242935419 quant_lr:5e-06 norm:0.00171239 max memory_allocated 6822.486328125 time 24.410001754760742 \n",
      "\u001b[32m[2025-10-29 05:46:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:46:26 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:46:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0023893117904663086 val_loss:0.0024431361816823483 quant_lr:5.1362214303938806e-05 norm:0.00142378 max memory_allocated 6822.486328125 time 24.332285165786743 \n",
      "\u001b[32m[2025-10-29 05:47:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.002235721331089735 val_loss:0.002397809876129031 quant_lr:5e-06 norm:0.00115158 max memory_allocated 6822.486328125 time 24.399038791656494 \n",
      "\u001b[32m[2025-10-29 05:47:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:47:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:48:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.002820128109306097 val_loss:0.002906082198023796 quant_lr:5.1362214303938806e-05 norm:0.00214073 max memory_allocated 6822.486328125 time 24.34277367591858 \n",
      "\u001b[32m[2025-10-29 05:48:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.00262980698607862 val_loss:0.0028568492271006107 quant_lr:5e-06 norm:0.00160714 max memory_allocated 6822.486328125 time 24.40580916404724 \n",
      "\u001b[32m[2025-10-29 05:48:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:48:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:49:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.0033849612809717655 val_loss:0.00354138039983809 quant_lr:5.1362214303938806e-05 norm:0.00254867 max memory_allocated 6822.486328125 time 24.34660267829895 \n",
      "\u001b[32m[2025-10-29 05:49:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.003143504960462451 val_loss:0.0034788239281624556 quant_lr:5e-06 norm:0.00180110 max memory_allocated 6822.486328125 time 24.406134128570557 \n",
      "\u001b[32m[2025-10-29 05:49:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:49:53 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:50:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.004165603779256344 val_loss:0.004434251692146063 quant_lr:5.1362214303938806e-05 norm:0.00297265 max memory_allocated 6822.486328125 time 24.363606452941895 \n",
      "\u001b[32m[2025-10-29 05:50:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.003868798026815057 val_loss:0.004366747103631496 quant_lr:5e-06 norm:0.00221777 max memory_allocated 6822.486328125 time 24.409435033798218 \n",
      "\u001b[32m[2025-10-29 05:50:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:50:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:50:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:50:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:50:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:51:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:51:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:51:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:51:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.005088480189442635 val_loss:0.005512667819857597 quant_lr:5.1362214303938806e-05 norm:0.00396125 max memory_allocated 6822.486328125 time 24.37281608581543 \n",
      "\u001b[32m[2025-10-29 05:51:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.004736320581287146 val_loss:0.00543542206287384 quant_lr:5e-06 norm:0.00270725 max memory_allocated 6822.486328125 time 24.413056135177612 \n",
      "\u001b[32m[2025-10-29 05:52:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:52:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:52:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.006161672528833151 val_loss:0.006781411357223988 quant_lr:5.1362214303938806e-05 norm:0.00430698 max memory_allocated 6822.486328125 time 24.368531942367554 \n",
      "\u001b[32m[2025-10-29 05:53:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.0057324911467731 val_loss:0.006683600600808859 quant_lr:5e-06 norm:0.00356574 max memory_allocated 6822.486328125 time 24.406479120254517 \n",
      "\u001b[32m[2025-10-29 05:53:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:53:21 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:53:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.007123973686248064 val_loss:0.007983269169926643 quant_lr:5.1362214303938806e-05 norm:0.00511828 max memory_allocated 6822.486328125 time 24.37579345703125 \n",
      "\u001b[32m[2025-10-29 05:54:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.00663876673206687 val_loss:0.007878036238253117 quant_lr:5e-06 norm:0.00427312 max memory_allocated 6822.486328125 time 24.40892505645752 \n",
      "\u001b[32m[2025-10-29 05:54:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:54:31 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:55:00 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.008297045715153217 val_loss:0.009437396191060543 quant_lr:5.1362214303938806e-05 norm:0.00475053 max memory_allocated 6822.486328125 time 24.392587184906006 \n",
      "\u001b[32m[2025-10-29 05:55:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0077457367442548275 val_loss:0.009323546662926674 quant_lr:5e-06 norm:0.00399446 max memory_allocated 6822.486328125 time 24.40921926498413 \n",
      "\u001b[32m[2025-10-29 05:55:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:55:40 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:56:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.009556259959936142 val_loss:0.011029733344912529 quant_lr:5.1362214303938806e-05 norm:0.00504655 max memory_allocated 6822.486328125 time 24.37957787513733 \n",
      "\u001b[32m[2025-10-29 05:56:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.008920385502278805 val_loss:0.010892662219703197 quant_lr:5e-06 norm:0.00421582 max memory_allocated 6822.486328125 time 24.4521427154541 \n",
      "\u001b[32m[2025-10-29 05:56:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:56:51 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:57:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.011097862385213375 val_loss:0.012966658920049667 quant_lr:5.1362214303938806e-05 norm:0.00524244 max memory_allocated 6822.486328125 time 24.414947032928467 \n",
      "\u001b[32m[2025-10-29 05:57:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.010357780382037163 val_loss:0.012804269790649414 quant_lr:5e-06 norm:0.00417529 max memory_allocated 6822.486328125 time 24.472147941589355 \n",
      "\u001b[32m[2025-10-29 05:57:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:57:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:57:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:57:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:57:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:57:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:58:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:58:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:58:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.012979046441614628 val_loss:0.015344420447945595 quant_lr:5.1362214303938806e-05 norm:0.00619840 max memory_allocated 6822.486328125 time 24.418601751327515 \n",
      "\u001b[32m[2025-10-29 05:58:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.012123062275350094 val_loss:0.015155792236328125 quant_lr:5e-06 norm:0.00474737 max memory_allocated 6822.486328125 time 24.4996554851532 \n",
      "\u001b[32m[2025-10-29 05:59:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 05:59:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 05:59:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.016382364556193352 val_loss:0.01947086863219738 quant_lr:5.1362214303938806e-05 norm:0.00783660 max memory_allocated 6822.486328125 time 24.416710138320923 \n",
      "\u001b[32m[2025-10-29 06:00:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.015256467275321484 val_loss:0.019224321469664574 quant_lr:5e-06 norm:0.00653370 max memory_allocated 6822.486328125 time 24.49654746055603 \n",
      "\u001b[32m[2025-10-29 06:00:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:00:20 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:00:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.019198814406991005 val_loss:0.023141128942370415 quant_lr:5.1362214303938806e-05 norm:0.00787870 max memory_allocated 6822.486328125 time 24.43076205253601 \n",
      "\u001b[32m[2025-10-29 06:01:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.01800348423421383 val_loss:0.022897493094205856 quant_lr:5e-06 norm:0.00644213 max memory_allocated 6822.486328125 time 24.489450216293335 \n",
      "\u001b[32m[2025-10-29 06:01:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:01:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:01:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.023603973910212517 val_loss:0.028730981051921844 quant_lr:5.1362214303938806e-05 norm:0.00885441 max memory_allocated 6822.486328125 time 24.42217779159546 \n",
      "\u001b[32m[2025-10-29 06:02:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.02225041389465332 val_loss:0.028472252190113068 quant_lr:5e-06 norm:0.00750175 max memory_allocated 6822.486328125 time 24.490055799484253 \n",
      "\u001b[32m[2025-10-29 06:02:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:02:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:03:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.028856532648205757 val_loss:0.03547967970371246 quant_lr:5.1362214303938806e-05 norm:0.00873705 max memory_allocated 6822.486328125 time 24.391074657440186 \n",
      "\u001b[32m[2025-10-29 06:03:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.02733120694756508 val_loss:0.03519223630428314 quant_lr:5e-06 norm:0.00725706 max memory_allocated 6822.486328125 time 24.49328637123108 \n",
      "\u001b[32m[2025-10-29 06:03:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:03:49 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:04:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.0362413264811039 val_loss:0.04511801898479462 quant_lr:5.1362214303938806e-05 norm:0.01258474 max memory_allocated 6822.486328125 time 24.438868761062622 \n",
      "\u001b[32m[2025-10-29 06:04:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.03440698981285095 val_loss:0.044765405356884 quant_lr:5e-06 norm:0.01121593 max memory_allocated 6822.486328125 time 24.44955039024353 \n",
      "\u001b[32m[2025-10-29 06:04:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:04:58 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:05:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.043882109224796295 val_loss:0.05536769703030586 quant_lr:5.1362214303938806e-05 norm:0.01153172 max memory_allocated 6822.486328125 time 24.402848958969116 \n",
      "\u001b[32m[2025-10-29 06:05:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.041849005967378616 val_loss:0.05500434339046478 quant_lr:5e-06 norm:0.00978591 max memory_allocated 6822.486328125 time 24.470256090164185 \n",
      "\u001b[32m[2025-10-29 06:05:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:05:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:06:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:06:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.05429008975625038 val_loss:0.06931521743535995 quant_lr:5.1362214303938806e-05 norm:0.02112333 max memory_allocated 6822.486328125 time 24.39689564704895 \n",
      "\u001b[32m[2025-10-29 06:07:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.051836345344781876 val_loss:0.06886522471904755 quant_lr:5e-06 norm:0.01752194 max memory_allocated 6822.486328125 time 24.491845846176147 \n",
      "\u001b[32m[2025-10-29 06:07:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:07:18 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:07:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.06490962207317352 val_loss:0.08314543962478638 quant_lr:5.1362214303938806e-05 norm:0.01398352 max memory_allocated 6822.486328125 time 24.37965750694275 \n",
      "\u001b[32m[2025-10-29 06:08:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.06221846491098404 val_loss:0.08270926773548126 quant_lr:5e-06 norm:0.01122062 max memory_allocated 6822.486328125 time 24.47380828857422 \n",
      "\u001b[32m[2025-10-29 06:08:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:08:28 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:08:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.07734297215938568 val_loss:0.09997177124023438 quant_lr:5.1362214303938806e-05 norm:0.01679079 max memory_allocated 6822.486328125 time 24.39521098136902 \n",
      "\u001b[32m[2025-10-29 06:09:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.07434941083192825 val_loss:0.09953538328409195 quant_lr:5e-06 norm:0.01451930 max memory_allocated 6822.486328125 time 24.486297369003296 \n",
      "\u001b[32m[2025-10-29 06:09:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:09:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:10:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.09119251370429993 val_loss:0.1177653893828392 quant_lr:5.1362214303938806e-05 norm:0.02092739 max memory_allocated 6822.486328125 time 24.389621257781982 \n",
      "\u001b[32m[2025-10-29 06:10:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.08778288215398788 val_loss:0.11722840368747711 quant_lr:5e-06 norm:0.01685079 max memory_allocated 6822.486328125 time 24.463091135025024 \n",
      "\u001b[32m[2025-10-29 06:10:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:10:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:11:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.10860703140497208 val_loss:0.1408781111240387 quant_lr:5.1362214303938806e-05 norm:0.02256995 max memory_allocated 6822.486328125 time 24.405367374420166 \n",
      "\u001b[32m[2025-10-29 06:11:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.10472320765256882 val_loss:0.1402588188648224 quant_lr:5e-06 norm:0.01990850 max memory_allocated 6822.486328125 time 24.46105933189392 \n",
      "\u001b[32m[2025-10-29 06:11:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:11:57 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:12:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.12749098241329193 val_loss:0.1654919683933258 quant_lr:5.1362214303938806e-05 norm:0.02003587 max memory_allocated 6822.486328125 time 24.400782585144043 \n",
      "\u001b[32m[2025-10-29 06:12:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.12314596772193909 val_loss:0.16476058959960938 quant_lr:5e-06 norm:0.01728080 max memory_allocated 6822.486328125 time 24.477741956710815 \n",
      "\u001b[32m[2025-10-29 06:12:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:12:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:12:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:12:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:13:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:13:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:13:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:13:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:13:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.15213046967983246 val_loss:0.198242649435997 quant_lr:5.1362214303938806e-05 norm:0.02838547 max memory_allocated 6822.486328125 time 24.38744616508484 \n",
      "\u001b[32m[2025-10-29 06:14:00 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.14684133231639862 val_loss:0.19726037979125977 quant_lr:5e-06 norm:0.02393646 max memory_allocated 6822.486328125 time 24.49103283882141 \n",
      "\u001b[32m[2025-10-29 06:14:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:14:17 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:14:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.1812105029821396 val_loss:0.2361493855714798 quant_lr:5.1362214303938806e-05 norm:0.03309410 max memory_allocated 6822.486328125 time 24.384294033050537 \n",
      "\u001b[32m[2025-10-29 06:15:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.1748693287372589 val_loss:0.23485420644283295 quant_lr:5e-06 norm:0.02788612 max memory_allocated 6822.486328125 time 24.479170322418213 \n",
      "\u001b[32m[2025-10-29 06:15:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:15:26 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:15:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.23108136653900146 val_loss:0.3004096746444702 quant_lr:5.1362214303938806e-05 norm:0.16895565 max memory_allocated 6822.486328125 time 24.392051696777344 \n",
      "\u001b[32m[2025-10-29 06:16:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.22059673070907593 val_loss:0.29858720302581787 quant_lr:5e-06 norm:0.10016509 max memory_allocated 6822.486328125 time 24.478777408599854 \n",
      "\u001b[32m[2025-10-29 06:16:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:16:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 06:17:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.405621737241745 val_loss:0.5080599188804626 quant_lr:5.1362214303938806e-05 norm:0.40305889 max memory_allocated 6822.486328125 time 24.40426230430603 \n",
      "\u001b[32m[2025-10-29 06:17:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.3726040720939636 val_loss:0.4990672171115875 quant_lr:5e-06 norm:0.29903337 max memory_allocated 6822.486328125 time 24.488814115524292 \n",
      "\u001b[32m[2025-10-29 06:17:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 06:17:47 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2222.492461204529\n",
      "\u001b[32m[2025-10-29 06:17:47 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-29 06:18:02 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 166/166 [02:43<00:00,  1.02it/s]\n",
      "wikitext2:5.861960411071777\n",
      "\u001b[32m[2025-10-29 06:20:59 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 5.86\n",
      "2025-10-29 06:21:02.017775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761718862.246258    3872 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761718862.308282    3872 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761718862.646003    3872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761718862.646040    3872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761718862.646043    3872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761718862.646046    3872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 15.7MB/s]\n",
      "\u001b[32m[2025-10-29 06:21:08 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 06:21:08 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 06:21:12 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 19.8MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 28.5MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:01<00:00, 1.65MB/s]\n",
      "Downloading data: 815kB [00:00, 79.3MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 33817.35 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37393.71 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 34833.30 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 34.3MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.25MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.38MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 348kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 349421.85 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 453129.01 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 268292.37 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 24.0MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 33.7MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 12.2MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 12.4MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 245046.62 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 256291.11 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 259820.25 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 5839.71 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 5211.03 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 35.7MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 4.15MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 217kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 173kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1071004.58 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 589792.71 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 433209.68 examples/s]\n",
      "\u001b[32m[2025-10-29 06:22:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 06:22:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 06:22:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 06:22:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 06:22:04 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126624.65it/s]\n",
      "\u001b[32m[2025-10-29 06:22:05 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2409.68it/s]\n",
      "\u001b[32m[2025-10-29 06:22:10 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1055.94it/s]\n",
      "\u001b[32m[2025-10-29 06:22:12 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1009.22it/s]\n",
      "\u001b[32m[2025-10-29 06:22:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:59<00:00, 25.18it/s]\n",
      "\u001b[32m[2025-10-29 06:59:57 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6780|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5492|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7324|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7483|±  |0.0089|\n",
      "|          |       |none  |     0|acc_norm|0.7163|±  |0.0092|\n",
      "|piqa      |      1|none  |     0|acc     |0.7742|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7780|±  |0.0097|\n",
      "\n",
      "\u001b[32m[2025-10-29 06:59:57 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 68.74%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "  --model $MODEL \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --wbits 3 \\\n",
    "  --group_size 128 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/baseline3 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/baseline3/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bde2f",
   "metadata": {},
   "source": [
    "# MPQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Oo4-WlzExMrI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oo4-WlzExMrI",
    "outputId": "2eb6f1b9-78d0-497c-981e-3d1fa30816d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '3.0', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/mpq_adaptive', '--save_quant_dir', './output/research_experiments/mpq_adaptive/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/mpq_adaptive', save_quant_dir='./output/research_experiments/mpq_adaptive/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=3.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-29 08:51:28 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  5.97it/s]\n",
      "\u001b[32m[2025-10-29 08:51:30 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-29 08:51:30 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 08:51:30 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 08:51:30 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-29 08:51:32 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-29 08:51:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:52:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 08:52:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 08:52:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-29 08:52:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-29 08:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.002266 | Val Loss: 0.000901 | Grad Norm: 0.43 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 08:53:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.003324 | Val Loss: 0.000795 | Grad Norm: 0.61 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:53:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-29 08:53:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:54:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.001604 | Val Loss: 0.000811 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:54:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.001563 | Val Loss: 0.000805 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:55:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-29 08:55:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:55:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.001694 | Val Loss: 0.000913 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:55:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.001652 | Val Loss: 0.000899 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-29 08:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:56:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.001872 | Val Loss: 0.001092 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:57:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.001816 | Val Loss: 0.001081 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 08:57:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-29 08:57:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:57:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.002153 | Val Loss: 0.001386 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 08:58:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.002084 | Val Loss: 0.001369 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 08:58:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-29 08:58:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 08:59:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.002559 | Val Loss: 0.001806 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 08:59:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.002447 | Val Loss: 0.001779 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 08:59:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-29 08:59:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:00:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.003069 | Val Loss: 0.002367 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:00:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.002914 | Val Loss: 0.002331 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:00:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-29 09:00:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:01:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.003742 | Val Loss: 0.003135 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:01:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.003530 | Val Loss: 0.003086 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-29 09:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:02:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.004503 | Val Loss: 0.004038 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:03:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.004243 | Val Loss: 0.003980 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:03:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-29 09:03:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:03:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.005410 | Val Loss: 0.005098 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:04:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.005077 | Val Loss: 0.005023 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:04:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-29 09:04:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:04:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.006208 | Val Loss: 0.006110 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:05:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.005825 | Val Loss: 0.006018 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:05:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-29 09:05:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:06:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.007188 | Val Loss: 0.007319 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:06:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.006754 | Val Loss: 0.007230 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:06:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-29 09:06:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:07:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.008233 | Val Loss: 0.008652 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:07:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.007736 | Val Loss: 0.008535 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:08:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-29 09:08:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:08:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.009515 | Val Loss: 0.010252 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:08:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.008931 | Val Loss: 0.010125 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:09:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-29 09:09:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:09:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.011050 | Val Loss: 0.012207 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:10:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.010375 | Val Loss: 0.012056 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:10:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-29 09:10:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:10:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.013871 | Val Loss: 0.015673 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:11:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.012975 | Val Loss: 0.015468 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:11:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-29 09:11:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:12:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.016146 | Val Loss: 0.018677 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:12:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.015217 | Val Loss: 0.018491 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-29 09:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:13:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.019722 | Val Loss: 0.023302 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:13:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.018651 | Val Loss: 0.023098 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-29 09:13:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 09:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.024762 | Val Loss: 0.029455 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:14:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.023421 | Val Loss: 0.029200 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:15:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-29 09:15:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 09:15:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.031624 | Val Loss: 0.038147 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:16:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.029929 | Val Loss: 0.037786 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:16:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-29 09:16:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 09:16:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.038603 | Val Loss: 0.047365 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:17:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.036691 | Val Loss: 0.047013 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:17:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-29 09:17:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 09:17:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.048093 | Val Loss: 0.059840 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:18:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.045807 | Val Loss: 0.059421 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:18:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-29 09:18:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:19:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.059221 | Val Loss: 0.073324 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:19:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.056350 | Val Loss: 0.072837 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:19:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-29 09:19:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:20:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.071809 | Val Loss: 0.089585 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:20:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.068562 | Val Loss: 0.089013 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:20:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-29 09:20:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:21:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.086046 | Val Loss: 0.107135 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:21:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.082280 | Val Loss: 0.106422 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:22:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-29 09:22:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.103963 | Val Loss: 0.129752 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:23:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.099613 | Val Loss: 0.128977 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-29 09:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:23:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.123338 | Val Loss: 0.154023 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:24:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.118492 | Val Loss: 0.153096 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-29 09:24:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:24:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.148988 | Val Loss: 0.186693 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 09:25:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.143019 | Val Loss: 0.185434 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:25:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-29 09:25:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 09:26:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.179373 | Val Loss: 0.224627 | Grad Norm: 0.04 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:26:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.172211 | Val Loss: 0.223071 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 09:26:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-29 09:26:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:27:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.218848 | Val Loss: 0.277445 | Grad Norm: 0.10 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 09:27:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.210763 | Val Loss: 0.275446 | Grad Norm: 0.08 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:28:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-29 09:28:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 09:28:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.372099 | Val Loss: 0.459547 | Grad Norm: 0.40 | LR: 5.14e-05 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:28:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.342387 | Val Loss: 0.450818 | Grad Norm: 0.31 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 09:29:13 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/mpq_adaptive/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 09:29:13 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/mpq_adaptive/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 09:29:14 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2264.01s (37.73min)\n",
      "\u001b[32m[2025-10-29 09:29:14 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-29 09:29:24 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/mpq_adaptive/model\n",
      "\u001b[32m[2025-10-29 09:29:24 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:39<00:00,  1.04it/s]\n",
      "wikitext2:5.822196006774902\n",
      "\u001b[32m[2025-10-29 09:32:18 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.82\n",
      "2025-10-29 09:32:19.326140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761730339.346550   52832 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761730339.352685   52832 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761730339.369963   52832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761730339.370002   52832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761730339.370006   52832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761730339.370008   52832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 09:32:24 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 09:32:25 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 09:32:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126269.62it/s]\n",
      "\u001b[32m[2025-10-29 09:32:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2400.60it/s]\n",
      "\u001b[32m[2025-10-29 09:33:02 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1037.31it/s]\n",
      "\u001b[32m[2025-10-29 09:33:04 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1003.65it/s]\n",
      "\u001b[32m[2025-10-29 09:33:06 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:10<00:00, 25.05it/s]\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6946|±  |0.0129|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5524|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7392|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7487|±  |0.0089|\n",
      "|          |       |none  |     0|acc_norm|0.7210|±  |0.0092|\n",
      "|piqa      |      1|none  |     0|acc     |0.7737|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7840|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 69.23%\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/mpq_adaptive/results.json\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-29 10:10:59 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy adaptive \\\n",
    "  --target_avg_bits 3.0 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/mpq_adaptive \\\n",
    "  --save_quant_dir $BASE_OUTPUT/mpq_adaptive/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158698e",
   "metadata": {},
   "source": [
    "# SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ETduDbfHa-sA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETduDbfHa-sA",
    "outputId": "de0726fe-ab7a-4758-e620-833af32e25c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_adaptive_training', '--wbits', '3', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/sgra_adaptive', '--save_quant_dir', './output/research_experiments/sgra_adaptive/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/sgra_adaptive', save_quant_dir='./output/research_experiments/sgra_adaptive/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-29 10:11:48 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.07it/s]\n",
      "\u001b[32m[2025-10-29 10:11:50 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-29 10:11:50 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 10:11:50 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 10:11:50 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-29 10:11:52 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:11:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:12:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000007 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 10:12:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000004 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 10:13:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:13:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-29 10:13:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:13:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-29 10:13:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.001887 | Val Loss: 0.004277 | Grad Norm: nan | LR: 5.71e-05 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 10:14:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.001475 | Val Loss: 0.003548 | Grad Norm: 0.27 | LR: 3.46e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:14:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.001274 | Val Loss: 0.003477 | Grad Norm: 0.17 | LR: 1.23e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.001166 | Val Loss: 0.003277 | Grad Norm: 0.11 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:15:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-29 10:15:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:15:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:15:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.001195 | Val Loss: 0.003080 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:16:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.001104 | Val Loss: 0.003027 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:16:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.001073 | Val Loss: 0.003012 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-29 10:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.001300 | Val Loss: 0.003178 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:17:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.001200 | Val Loss: 0.003133 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:18:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.001172 | Val Loss: 0.003122 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:18:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-29 10:18:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:18:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:19:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.001514 | Val Loss: 0.003395 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:19:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.001380 | Val Loss: 0.003347 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:20:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.001348 | Val Loss: 0.003333 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:20:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-29 10:20:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:20:15 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:20:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.001861 | Val Loss: 0.003763 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:21:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.001723 | Val Loss: 0.003714 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:21:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.001689 | Val Loss: 0.003704 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:21:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-29 10:21:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:21:52 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:22:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.002385 | Val Loss: 0.004290 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:22:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.002190 | Val Loss: 0.004225 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:23:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.002143 | Val Loss: 0.004211 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:23:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-29 10:23:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:23:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:23:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.003022 | Val Loss: 0.004977 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:24:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.002783 | Val Loss: 0.004900 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:24:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.002724 | Val Loss: 0.004883 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:25:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-29 10:25:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:25:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:25:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.003890 | Val Loss: 0.005937 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:26:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.003594 | Val Loss: 0.005840 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:26:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.003519 | Val Loss: 0.005820 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:26:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-29 10:26:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:26:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:27:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.004903 | Val Loss: 0.007073 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:27:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.004551 | Val Loss: 0.006964 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:28:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.004464 | Val Loss: 0.006939 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:28:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-29 10:28:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:28:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:28:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.006080 | Val Loss: 0.008409 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:29:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.005653 | Val Loss: 0.008272 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:29:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.005544 | Val Loss: 0.008240 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:29:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-29 10:29:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:29:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:30:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.007130 | Val Loss: 0.009670 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:30:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.006651 | Val Loss: 0.009514 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:31:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.006523 | Val Loss: 0.009478 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:31:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-29 10:31:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:31:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:32:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.008423 | Val Loss: 0.011182 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:32:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.007885 | Val Loss: 0.011011 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:32:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.007742 | Val Loss: 0.010973 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:33:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-29 10:33:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:33:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:33:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.009835 | Val Loss: 0.012862 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:34:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.009214 | Val Loss: 0.012663 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:34:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.009047 | Val Loss: 0.012616 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:34:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-29 10:34:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:34:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:35:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.011527 | Val Loss: 0.014870 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:35:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.010818 | Val Loss: 0.014642 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:36:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.010625 | Val Loss: 0.014597 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:36:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-29 10:36:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:36:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:36:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.013623 | Val Loss: 0.017368 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:37:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.012793 | Val Loss: 0.017085 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:37:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.012573 | Val Loss: 0.017030 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:38:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-29 10:38:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:38:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:38:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.017367 | Val Loss: 0.021677 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:38:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.016285 | Val Loss: 0.021316 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:39:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.015981 | Val Loss: 0.021237 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:39:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-29 10:39:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:39:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.74e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:40:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.020446 | Val Loss: 0.025475 | Grad Norm: 0.01 | LR: 5.10e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:40:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.019306 | Val Loss: 0.025100 | Grad Norm: 0.01 | LR: 1.89e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:41:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.018990 | Val Loss: 0.025025 | Grad Norm: 0.01 | LR: 3.37e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:41:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-29 10:41:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:41:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.93e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:41:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.025307 | Val Loss: 0.031266 | Grad Norm: 0.01 | LR: 5.24e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:42:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.024000 | Val Loss: 0.030872 | Grad Norm: 0.01 | LR: 1.95e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:42:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.023640 | Val Loss: 0.030788 | Grad Norm: 0.01 | LR: 3.47e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:42:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-29 10:42:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:42:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.14e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:43:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.031034 | Val Loss: 0.038226 | Grad Norm: 0.01 | LR: 5.40e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:43:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.029593 | Val Loss: 0.037803 | Grad Norm: 0.01 | LR: 2.01e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:44:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.029184 | Val Loss: 0.037713 | Grad Norm: 0.01 | LR: 3.57e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:44:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-29 10:44:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:44:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.27e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 10:45:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.039089 | Val Loss: 0.048138 | Grad Norm: 0.01 | LR: 5.50e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:45:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.037323 | Val Loss: 0.047600 | Grad Norm: 0.01 | LR: 2.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:45:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.036806 | Val Loss: 0.047503 | Grad Norm: 0.01 | LR: 3.64e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:46:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-29 10:46:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:46:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.89e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.047321 | Val Loss: 0.058606 | Grad Norm: 0.01 | LR: 5.96e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:47:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.045374 | Val Loss: 0.058050 | Grad Norm: 0.01 | LR: 2.22e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:47:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.044811 | Val Loss: 0.057948 | Grad Norm: 0.01 | LR: 3.95e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:47:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-29 10:47:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:47:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.81e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:48:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.058571 | Val Loss: 0.072917 | Grad Norm: 0.02 | LR: 5.90e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:48:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.056249 | Val Loss: 0.072269 | Grad Norm: 0.02 | LR: 2.19e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:49:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.055566 | Val Loss: 0.072129 | Grad Norm: 0.01 | LR: 3.91e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-29 10:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:49:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:49:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.069990 | Val Loss: 0.086986 | Grad Norm: 0.02 | LR: 6.43e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:50:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.067399 | Val Loss: 0.086346 | Grad Norm: 0.01 | LR: 2.39e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:50:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.066657 | Val Loss: 0.086210 | Grad Norm: 0.01 | LR: 4.26e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:50:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-29 10:50:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:50:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.29e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:51:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.083258 | Val Loss: 0.104223 | Grad Norm: 0.02 | LR: 6.26e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 10:51:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.080450 | Val Loss: 0.103473 | Grad Norm: 0.01 | LR: 2.33e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:52:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.079613 | Val Loss: 0.103310 | Grad Norm: 0.01 | LR: 4.14e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-29 10:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.43e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:53:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.098022 | Val Loss: 0.122336 | Grad Norm: 0.02 | LR: 6.37e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:53:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.094814 | Val Loss: 0.121484 | Grad Norm: 0.02 | LR: 2.37e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:53:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.093861 | Val Loss: 0.121273 | Grad Norm: 0.02 | LR: 4.22e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:54:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-29 10:54:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:54:13 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 10:54:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.116591 | Val Loss: 0.145813 | Grad Norm: 0.03 | LR: 6.57e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:55:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.112924 | Val Loss: 0.144862 | Grad Norm: 0.02 | LR: 2.44e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:55:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.111814 | Val Loss: 0.144614 | Grad Norm: 0.02 | LR: 4.34e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:55:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-29 10:55:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:55:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-29 10:56:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.136366 | Val Loss: 0.170690 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:56:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.132557 | Val Loss: 0.170023 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:57:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-29 10:57:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:57:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.56e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 10:57:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.163674 | Val Loss: 0.204810 | Grad Norm: 0.03 | LR: 4.91e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:57:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.158952 | Val Loss: 0.203882 | Grad Norm: 0.02 | LR: 4.78e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:58:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-29 10:58:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:58:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.22e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 10:58:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 0.196286 | Val Loss: 0.244448 | Grad Norm: 0.04 | LR: 6.21e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:59:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 0.190209 | Val Loss: 0.242502 | Grad Norm: 0.03 | LR: 2.31e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 10:59:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 0.188354 | Val Loss: 0.242046 | Grad Norm: 0.03 | LR: 4.11e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 10:59:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-29 10:59:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 10:59:51 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:00:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 0.281679 | Val Loss: 0.327399 | Grad Norm: 0.63 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:00:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 0.244832 | Val Loss: 0.317374 | Grad Norm: 0.19 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 11:01:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 0.239601 | Val Loss: 0.315528 | Grad Norm: 0.15 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 11:01:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-29 11:01:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 11:01:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:01:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.448175 | Val Loss: 0.536326 | Grad Norm: 0.39 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 11:02:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.409663 | Val Loss: 0.522527 | Grad Norm: 0.26 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 11:02:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.400506 | Val Loss: 0.519560 | Grad Norm: 0.22 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 11:03:05 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/sgra_adaptive/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 11:03:05 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/sgra_adaptive/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 11:03:06 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3075.90s (51.26min)\n",
      "\u001b[32m[2025-10-29 11:03:06 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-29 11:03:16 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/sgra_adaptive/model\n",
      "\u001b[32m[2025-10-29 11:03:16 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:39<00:00,  1.04it/s]\n",
      "wikitext2:5.866825103759766\n",
      "\u001b[32m[2025-10-29 11:06:09 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.87\n",
      "2025-10-29 11:06:10.251805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761735970.271722   73327 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761735970.277752   73327 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761735970.295142   73327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761735970.295178   73327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761735970.295184   73327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761735970.295187   73327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 11:06:15 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 11:06:15 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 11:06:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126365.70it/s]\n",
      "\u001b[32m[2025-10-29 11:06:46 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2412.26it/s]\n",
      "\u001b[32m[2025-10-29 11:06:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1051.54it/s]\n",
      "\u001b[32m[2025-10-29 11:06:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1010.56it/s]\n",
      "\u001b[32m[2025-10-29 11:06:57 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:00<00:00, 25.16it/s]\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6827|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5505|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7303|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7479|±  |0.0089|\n",
      "|          |       |none  |     0|acc_norm|0.7260|±  |0.0092|\n",
      "|piqa      |      1|none  |     0|acc     |0.7693|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7813|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 68.76%\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/sgra_adaptive/results.json\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-29 11:44:40 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_adaptive_training \\\n",
    "  --wbits 3 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/sgra_adaptive \\\n",
    "  --save_quant_dir $BASE_OUTPUT/sgra_adaptive/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6fd3e",
   "metadata": {},
   "source": [
    "# MPQ + SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebgJlVC6JOox",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebgJlVC6JOox",
    "outputId": "95b0655c-791b-4225-bb2f-352bc2699ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--use_adaptive_training', '--mpq_strategy', 'conservative', '--target_avg_bits', '3', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/combined3', '--save_quant_dir', './output/research_experiments/combined3/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/combined3', save_quant_dir='./output/research_experiments/combined3/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=3.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-29 16:54:03 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  5.92it/s]\n",
      "\u001b[32m[2025-10-29 16:54:04 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-29 16:54:04 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 16:54:04 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 16:54:04 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 16:54:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 16:54:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000005 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 16:55:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 16:55:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ⚠️  Block 0 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 16:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-29 16:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-29 16:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-29 16:56:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.001745 | Val Loss: 0.003517 | Grad Norm: 0.35 | LR: 5.71e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 16:56:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.001646 | Val Loss: 0.002622 | Grad Norm: 0.47 | LR: 3.46e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 16:57:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000588 | Val Loss: 0.001882 | Grad Norm: 0.23 | LR: 1.23e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 16:57:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000357 | Val Loss: 0.001633 | Grad Norm: 0.15 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 16:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-29 16:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 16:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 16:58:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000355 | Val Loss: 0.001622 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 16:58:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000328 | Val Loss: 0.001606 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 16:59:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000324 | Val Loss: 0.001606 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 16:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-29 16:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 16:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 16:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000473 | Val Loss: 0.001739 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:00:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000443 | Val Loss: 0.001731 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:00:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000434 | Val Loss: 0.001727 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:00:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-29 17:00:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:00:56 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:01:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000685 | Val Loss: 0.001951 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:01:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000631 | Val Loss: 0.001929 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:02:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000620 | Val Loss: 0.001927 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:02:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-29 17:02:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:02:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:03:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000983 | Val Loss: 0.002252 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:03:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000912 | Val Loss: 0.002231 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000895 | Val Loss: 0.002226 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-29 17:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:04:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.001413 | Val Loss: 0.002695 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:05:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.001304 | Val Loss: 0.002664 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:05:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.001275 | Val Loss: 0.002655 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:05:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-29 17:05:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:05:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:06:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.001953 | Val Loss: 0.003279 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.001803 | Val Loss: 0.003235 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:07:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.001763 | Val Loss: 0.003225 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-29 17:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:07:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.002674 | Val Loss: 0.004067 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:08:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.002468 | Val Loss: 0.004008 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:08:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.002413 | Val Loss: 0.003994 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:09:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-29 17:09:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:09:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:09:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.003491 | Val Loss: 0.005004 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:09:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.003242 | Val Loss: 0.004935 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:10:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.003177 | Val Loss: 0.004917 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-29 17:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.004466 | Val Loss: 0.006110 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:11:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.004147 | Val Loss: 0.006017 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:12:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.004060 | Val Loss: 0.005995 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:12:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-29 17:12:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:12:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:12:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.005326 | Val Loss: 0.007155 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:13:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.004956 | Val Loss: 0.007048 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:13:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.004857 | Val Loss: 0.007020 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:13:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-29 17:13:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:13:56 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:14:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.006381 | Val Loss: 0.008419 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:14:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.005968 | Val Loss: 0.008307 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:15:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.005856 | Val Loss: 0.008279 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:15:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-29 17:15:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:15:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:16:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.007512 | Val Loss: 0.009814 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:16:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.007043 | Val Loss: 0.009675 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:16:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.006910 | Val Loss: 0.009640 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-29 17:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:17:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.008907 | Val Loss: 0.011491 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:18:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.008356 | Val Loss: 0.011330 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:18:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.008199 | Val Loss: 0.011291 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:18:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-29 17:18:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:18:48 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:19:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.010583 | Val Loss: 0.013550 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:19:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.009953 | Val Loss: 0.013364 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:20:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.009778 | Val Loss: 0.013321 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:20:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-29 17:20:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:20:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:20:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.013629 | Val Loss: 0.017149 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:21:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.012782 | Val Loss: 0.016906 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:21:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.012546 | Val Loss: 0.016849 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:22:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-29 17:22:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:22:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.74e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:22:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.016089 | Val Loss: 0.020279 | Grad Norm: 0.01 | LR: 5.10e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:22:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.015226 | Val Loss: 0.020026 | Grad Norm: 0.01 | LR: 1.89e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:23:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.014971 | Val Loss: 0.019970 | Grad Norm: 0.00 | LR: 3.37e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:23:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-29 17:23:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:23:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.93e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:24:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.019948 | Val Loss: 0.025064 | Grad Norm: 0.01 | LR: 5.24e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:24:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.018978 | Val Loss: 0.024808 | Grad Norm: 0.01 | LR: 1.95e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:25:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.018693 | Val Loss: 0.024739 | Grad Norm: 0.01 | LR: 3.47e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:25:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-29 17:25:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 17:25:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.14e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:25:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.025463 | Val Loss: 0.031517 | Grad Norm: 0.01 | LR: 5.40e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:26:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.024179 | Val Loss: 0.031134 | Grad Norm: 0.01 | LR: 2.01e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:26:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.023823 | Val Loss: 0.031058 | Grad Norm: 0.01 | LR: 3.57e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:26:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-29 17:26:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 17:26:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.27e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:27:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.032816 | Val Loss: 0.040409 | Grad Norm: 0.01 | LR: 5.50e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 17:27:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.031194 | Val Loss: 0.039916 | Grad Norm: 0.01 | LR: 2.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:28:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.030717 | Val Loss: 0.039828 | Grad Norm: 0.01 | LR: 3.64e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-29 17:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 17:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.89e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:29:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.040281 | Val Loss: 0.049857 | Grad Norm: 0.01 | LR: 5.96e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:29:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.038491 | Val Loss: 0.049343 | Grad Norm: 0.01 | LR: 2.22e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:29:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.037963 | Val Loss: 0.049235 | Grad Norm: 0.01 | LR: 3.95e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:30:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-29 17:30:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-29 17:30:09 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.81e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:30:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.050408 | Val Loss: 0.062662 | Grad Norm: 0.02 | LR: 5.90e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:31:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.048287 | Val Loss: 0.062049 | Grad Norm: 0.01 | LR: 2.19e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:31:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.047644 | Val Loss: 0.061919 | Grad Norm: 0.01 | LR: 3.91e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:31:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-29 17:31:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:31:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:32:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.062377 | Val Loss: 0.076535 | Grad Norm: 0.02 | LR: 6.43e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:32:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.059505 | Val Loss: 0.075667 | Grad Norm: 0.01 | LR: 2.39e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:33:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.058731 | Val Loss: 0.075493 | Grad Norm: 0.01 | LR: 4.26e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:33:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-29 17:33:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:33:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.29e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:33:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.075678 | Val Loss: 0.093097 | Grad Norm: 0.02 | LR: 6.26e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:34:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.072512 | Val Loss: 0.092113 | Grad Norm: 0.01 | LR: 2.33e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:34:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.071627 | Val Loss: 0.091915 | Grad Norm: 0.01 | LR: 4.14e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:35:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-29 17:35:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:35:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.43e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:35:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.090684 | Val Loss: 0.110889 | Grad Norm: 0.02 | LR: 6.37e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:35:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.087027 | Val Loss: 0.109715 | Grad Norm: 0.02 | LR: 2.37e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:36:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.085995 | Val Loss: 0.109459 | Grad Norm: 0.02 | LR: 4.22e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-29 17:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 17:37:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.109566 | Val Loss: 0.133786 | Grad Norm: 0.03 | LR: 6.57e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:37:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.105299 | Val Loss: 0.132424 | Grad Norm: 0.02 | LR: 2.44e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:38:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.104068 | Val Loss: 0.132203 | Grad Norm: 0.02 | LR: 4.34e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:38:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-29 17:38:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:38:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-29 17:38:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.129527 | Val Loss: 0.158090 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:39:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.125243 | Val Loss: 0.157276 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:39:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-29 17:39:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:39:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.56e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 17:39:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.157812 | Val Loss: 0.192106 | Grad Norm: 0.03 | LR: 4.91e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:40:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.152447 | Val Loss: 0.190941 | Grad Norm: 0.03 | LR: 4.78e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:40:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-29 17:40:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-29 17:40:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.22e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 17:41:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 0.191833 | Val Loss: 0.231945 | Grad Norm: 0.04 | LR: 6.21e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:41:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 0.184819 | Val Loss: 0.229509 | Grad Norm: 0.03 | LR: 2.31e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:42:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 0.182699 | Val Loss: 0.228873 | Grad Norm: 0.03 | LR: 4.11e-06 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 17:42:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-29 17:42:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:42:17 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:42:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 0.234689 | Val Loss: 0.285316 | Grad Norm: 0.11 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:43:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 0.226745 | Val Loss: 0.282402 | Grad Norm: 0.08 | LR: 1.87e-05 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:43:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 0.224177 | Val Loss: 0.281455 | Grad Norm: 0.07 | LR: 3.33e-06 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:43:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-29 17:43:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 17:43:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 17:44:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.401796 | Val Loss: 0.476834 | Grad Norm: 0.34 | LR: 5.04e-05 | Time: 25.5s\n",
      "\u001b[32m[2025-10-29 17:44:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.372316 | Val Loss: 0.466146 | Grad Norm: 0.24 | LR: 1.87e-05 | Time: 25.7s\n",
      "\u001b[32m[2025-10-29 17:45:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.364702 | Val Loss: 0.463614 | Grad Norm: 0.19 | LR: 3.33e-06 | Time: 25.6s\n",
      "\u001b[32m[2025-10-29 17:45:33 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/combined3/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 17:45:33 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/combined3/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 17:45:34 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3089.51s (51.49min)\n",
      "\u001b[32m[2025-10-29 17:45:34 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-29 17:45:44 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/combined3/model\n",
      "\u001b[32m[2025-10-29 17:45:44 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:40<00:00,  1.04it/s]\n",
      "wikitext2:5.835604190826416\n",
      "\u001b[32m[2025-10-29 17:48:39 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 5.84\n",
      "2025-10-29 17:48:39.920666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761760119.940821  175447 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761760119.946852  175447 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761760119.964510  175447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761760119.964545  175447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761760119.964548  175447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761760119.964550  175447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 17:48:44 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 17:48:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 17:48:49 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 125328.60it/s]\n",
      "\u001b[32m[2025-10-29 17:49:16 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2397.33it/s]\n",
      "\u001b[32m[2025-10-29 17:49:22 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1037.83it/s]\n",
      "\u001b[32m[2025-10-29 17:49:24 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1001.14it/s]\n",
      "\u001b[32m[2025-10-29 17:49:26 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:09<00:00, 25.06it/s]\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6890|±  |0.0130|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5519|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7378|±  |0.0044|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7386|±  |0.0090|\n",
      "|          |       |none  |     0|acc_norm|0.7163|±  |0.0092|\n",
      "|piqa      |      1|none  |     0|acc     |0.7693|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7807|±  |0.0097|\n",
      "\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 68.72%\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/combined3/results.json\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-29 18:27:19 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --use_adaptive_training \\\n",
    "  --mpq_strategy conservative \\\n",
    "  --target_avg_bits 3 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/combined3 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/combined3/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2e4d9",
   "metadata": {},
   "source": [
    "## 2 bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defd44d",
   "metadata": {},
   "source": [
    "# MPQ 2.5 bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SmXK3bn3J3hA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmXK3bn3J3hA",
    "outputId": "daca828b-b6a5-4c79-dcd4-d33ab02ea25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '2.5', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/mpq_adaptive12', '--save_quant_dir', './output/research_experiments/mpq_adaptive12/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.5\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/mpq_adaptive12', save_quant_dir='./output/research_experiments/mpq_adaptive12/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=2.5, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-29 14:15:41 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  5.91it/s]\n",
      "\u001b[32m[2025-10-29 14:15:42 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-29 14:15:42 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 14:15:42 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 14:15:42 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 2.5 bits\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3]\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.53 bits\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 6.32x vs FP16\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-29 14:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:16:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.6s\n",
      "\u001b[32m[2025-10-29 14:16:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.5s\n",
      "\u001b[32m[2025-10-29 14:16:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-29 14:16:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:17:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.002397 | Val Loss: 0.001135 | Grad Norm: 0.34 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:17:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ⚠️  Block 1 Epoch 1/2 | Train Loss: 0.003854 | Val Loss: 0.002347 | Grad Norm: 0.41 | LR: 5.00e-06 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:18:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-29 14:18:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:18:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.002801 | Val Loss: 0.002142 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:18:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.002525 | Val Loss: 0.002001 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:19:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-29 14:19:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:19:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.002652 | Val Loss: 0.002059 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:20:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.002452 | Val Loss: 0.001965 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:20:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-29 14:20:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:20:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.002664 | Val Loss: 0.002109 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:21:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.002493 | Val Loss: 0.002048 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-29 14:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:22:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.002854 | Val Loss: 0.002351 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:22:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.002703 | Val Loss: 0.002305 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:22:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-29 14:22:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:23:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.003208 | Val Loss: 0.002745 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:23:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.003030 | Val Loss: 0.002696 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:23:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-29 14:23:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:24:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.003682 | Val Loss: 0.003292 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:24:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.003464 | Val Loss: 0.003235 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-29 14:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:25:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.004329 | Val Loss: 0.004071 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:25:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.004074 | Val Loss: 0.004012 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:26:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-29 14:26:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:26:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.005095 | Val Loss: 0.005010 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:27:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.004800 | Val Loss: 0.004944 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:27:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-29 14:27:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:27:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.006002 | Val Loss: 0.006113 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:28:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.005644 | Val Loss: 0.006033 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:28:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-29 14:28:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:29:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.006794 | Val Loss: 0.007148 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:29:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.006388 | Val Loss: 0.007058 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:29:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-29 14:29:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:30:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.007768 | Val Loss: 0.008404 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:30:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.007315 | Val Loss: 0.008309 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:31:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-29 14:31:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:31:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.008829 | Val Loss: 0.009790 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:31:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.008307 | Val Loss: 0.009667 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:32:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-29 14:32:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:32:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.010126 | Val Loss: 0.011445 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:33:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.009515 | Val Loss: 0.011319 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 14:33:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-29 14:33:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-29 14:33:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.021458 | Val Loss: 0.021553 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:34:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.018574 | Val Loss: 0.020879 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:34:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-29 14:34:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-29 14:35:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.035167 | Val Loss: 0.035501 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:35:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.031182 | Val Loss: 0.034568 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:35:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-29 14:35:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-29 14:36:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.047588 | Val Loss: 0.048853 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:36:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.043220 | Val Loss: 0.047839 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:36:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-29 14:36:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-29 14:37:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.064648 | Val Loss: 0.067087 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-29 14:37:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.059625 | Val Loss: 0.065938 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:38:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-29 14:38:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 14:38:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.089887 | Val Loss: 0.092978 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:39:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.083226 | Val Loss: 0.091413 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:39:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-29 14:39:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 14:39:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.123931 | Val Loss: 0.129159 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:40:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.115619 | Val Loss: 0.127277 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-29 14:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 14:40:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.159748 | Val Loss: 0.168895 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:41:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.150860 | Val Loss: 0.166967 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:41:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-29 14:41:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 14:42:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.208137 | Val Loss: 0.221702 | Grad Norm: 0.04 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:42:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.197404 | Val Loss: 0.219416 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:42:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-29 14:42:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:43:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.268787 | Val Loss: 0.283576 | Grad Norm: 0.05 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:43:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.254663 | Val Loss: 0.280593 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:43:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-29 14:43:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:44:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.336126 | Val Loss: 0.357045 | Grad Norm: 0.05 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:44:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.320887 | Val Loss: 0.353833 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:45:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-29 14:45:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:45:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.412520 | Val Loss: 0.436356 | Grad Norm: 0.07 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:46:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.394660 | Val Loss: 0.432660 | Grad Norm: 0.04 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:46:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-29 14:46:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:46:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.507628 | Val Loss: 0.537365 | Grad Norm: 0.07 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.487508 | Val Loss: 0.533296 | Grad Norm: 0.05 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:47:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-29 14:47:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:48:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.609719 | Val Loss: 0.644724 | Grad Norm: 0.06 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:48:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.587700 | Val Loss: 0.639959 | Grad Norm: 0.04 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-29 14:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:49:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.742314 | Val Loss: 0.785458 | Grad Norm: 0.08 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:49:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.715289 | Val Loss: 0.779754 | Grad Norm: 0.06 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:49:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-29 14:49:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-29 14:50:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.897020 | Val Loss: 0.947002 | Grad Norm: 0.10 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-29 14:50:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.864072 | Val Loss: 0.940524 | Grad Norm: 0.07 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-29 14:51:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-29 14:51:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:51:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.998938 | Val Loss: 1.082454 | Grad Norm: 0.19 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:51:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.972623 | Val Loss: 1.077377 | Grad Norm: 0.14 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 14:52:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-29 14:52:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-29 14:52:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 1.437310 | Val Loss: 1.565466 | Grad Norm: 0.57 | LR: 5.14e-05 | Time: 25.0s\n",
      "\u001b[32m[2025-10-29 14:53:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 1.379346 | Val Loss: 1.550990 | Grad Norm: 0.40 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 14:53:24 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/mpq_adaptive12/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 14:53:24 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/mpq_adaptive12/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 14:53:25 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2262.55s (37.71min)\n",
      "\u001b[32m[2025-10-29 14:53:25 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-29 14:53:33 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/mpq_adaptive12/model\n",
      "\u001b[32m[2025-10-29 14:53:33 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:35<00:00,  1.06it/s]\n",
      "wikitext2:7.715788841247559\n",
      "\u001b[32m[2025-10-29 14:56:23 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.72\n",
      "2025-10-29 14:56:23.779888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761749783.799989  135509 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761749783.806105  135509 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761749783.823766  135509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761749783.823806  135509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761749783.823812  135509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761749783.823814  135509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 14:56:28 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 14:56:29 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 14:56:33 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 123790.05it/s]\n",
      "\u001b[32m[2025-10-29 14:57:00 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2397.21it/s]\n",
      "\u001b[32m[2025-10-29 14:57:06 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1038.48it/s]\n",
      "\u001b[32m[2025-10-29 14:57:08 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 991.56it/s]\n",
      "\u001b[32m[2025-10-29 14:57:10 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:24<00:00, 25.57it/s]\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6788|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5274|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7046|±  |0.0046|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.6864|±  |0.0095|\n",
      "|          |       |none  |     0|acc_norm|0.6566|±  |0.0097|\n",
      "|piqa      |      1|none  |     0|acc     |0.7350|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.7508|±  |0.0101|\n",
      "\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 65.69%\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/mpq_adaptive12/results.json\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-29 15:34:18 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy adaptive \\\n",
    "  --target_avg_bits 2.5 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/mpq_adaptive12 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/mpq_adaptive12/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6b607",
   "metadata": {},
   "source": [
    "# SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qZ2ar6ucc7bY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZ2ar6ucc7bY",
    "outputId": "98adf5ee-7d8a-4398-88b8-43375a1fad06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_adaptive_training', '--wbits', '2', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/sgra_adaptive', '--save_quant_dir', './output/research_experiments/sgra_adaptive/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/sgra_adaptive', save_quant_dir='./output/research_experiments/sgra_adaptive/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-29 11:44:47 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.07it/s]\n",
      "\u001b[32m[2025-10-29 11:44:49 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-29 11:44:49 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 11:44:49 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 11:44:49 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-29 11:44:51 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:44:52 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:45:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000063 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:45:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000027 | Val Loss: 0.000024 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:46:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000024 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 11:46:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-29 11:46:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:46:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-29 11:46:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.013818 | Val Loss: 0.004709 | Grad Norm: nan | LR: 5.71e-05 | Time: 25.1s\n",
      "\u001b[32m[2025-10-29 11:47:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.003408 | Val Loss: 0.002769 | Grad Norm: 0.37 | LR: 3.46e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:47:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.002233 | Val Loss: 0.002548 | Grad Norm: 0.25 | LR: 1.23e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:48:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.001929 | Val Loss: 0.002295 | Grad Norm: 0.20 | LR: 3.33e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 11:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-29 11:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:48:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:48:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.002754 | Val Loss: 0.003012 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:49:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.002522 | Val Loss: 0.002935 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:49:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.002475 | Val Loss: 0.002919 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:50:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-29 11:50:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:50:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:50:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.003640 | Val Loss: 0.003898 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:50:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.003343 | Val Loss: 0.003785 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:51:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.003273 | Val Loss: 0.003758 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:51:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-29 11:51:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:51:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:52:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.005255 | Val Loss: 0.005492 | Grad Norm: 0.00 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.004820 | Val Loss: 0.005295 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:53:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.004711 | Val Loss: 0.005260 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-29 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:53:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.007602 | Val Loss: 0.007733 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:54:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.006958 | Val Loss: 0.007471 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:54:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.006803 | Val Loss: 0.007424 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:54:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-29 11:54:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:54:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:55:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.010804 | Val Loss: 0.010845 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:55:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.009833 | Val Loss: 0.010458 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:56:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.009602 | Val Loss: 0.010380 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-29 11:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:57:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.014745 | Val Loss: 0.014836 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:57:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.013510 | Val Loss: 0.014345 | Grad Norm: 0.00 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:57:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.013208 | Val Loss: 0.014239 | Grad Norm: 0.00 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:58:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-29 11:58:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:58:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 11:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.019799 | Val Loss: 0.019974 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:59:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.018258 | Val Loss: 0.019326 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:59:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.017869 | Val Loss: 0.019198 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 11:59:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-29 11:59:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 11:59:42 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:00:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.025779 | Val Loss: 0.025978 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:00:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.023818 | Val Loss: 0.025193 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:01:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.023336 | Val Loss: 0.025015 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:01:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-29 12:01:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:01:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:01:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.032588 | Val Loss: 0.032902 | Grad Norm: 0.01 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:02:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.030219 | Val Loss: 0.031952 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:02:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.029646 | Val Loss: 0.031734 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:02:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-29 12:02:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:02:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:03:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.038359 | Val Loss: 0.039039 | Grad Norm: 0.02 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.035858 | Val Loss: 0.037990 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:04:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.035212 | Val Loss: 0.037782 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:04:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-29 12:04:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:04:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:05:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.046274 | Val Loss: 0.047156 | Grad Norm: 0.02 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:05:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.043342 | Val Loss: 0.045922 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:05:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.042592 | Val Loss: 0.045733 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:06:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-29 12:06:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:06:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:06:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.055600 | Val Loss: 0.056984 | Grad Norm: 0.02 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:07:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.052209 | Val Loss: 0.055550 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:07:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.051314 | Val Loss: 0.055247 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:07:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-29 12:07:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:07:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:08:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.065763 | Val Loss: 0.067627 | Grad Norm: 0.02 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:08:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.062031 | Val Loss: 0.066029 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:09:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.061010 | Val Loss: 0.065678 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:09:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-29 12:09:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:09:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:09:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.079531 | Val Loss: 0.082134 | Grad Norm: 0.02 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:10:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.074983 | Val Loss: 0.080159 | Grad Norm: 0.01 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:10:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.073759 | Val Loss: 0.079750 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-29 12:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:11:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.104766 | Val Loss: 0.108619 | Grad Norm: 0.03 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:11:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.099136 | Val Loss: 0.106210 | Grad Norm: 0.02 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:12:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.097516 | Val Loss: 0.105717 | Grad Norm: 0.02 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:12:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-29 12:12:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:12:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.74e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:13:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.126141 | Val Loss: 0.132056 | Grad Norm: 0.03 | LR: 5.10e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:13:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.120215 | Val Loss: 0.129504 | Grad Norm: 0.02 | LR: 1.89e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:14:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.118487 | Val Loss: 0.128975 | Grad Norm: 0.02 | LR: 3.37e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:14:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-29 12:14:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:14:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.93e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:14:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.159461 | Val Loss: 0.167581 | Grad Norm: 0.03 | LR: 5.24e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.152581 | Val Loss: 0.164658 | Grad Norm: 0.02 | LR: 1.95e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.150556 | Val Loss: 0.164016 | Grad Norm: 0.02 | LR: 3.47e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:15:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-29 12:15:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:15:56 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.14e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:16:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.198694 | Val Loss: 0.209938 | Grad Norm: 0.03 | LR: 5.40e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:16:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.191123 | Val Loss: 0.206749 | Grad Norm: 0.02 | LR: 2.01e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:17:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.188871 | Val Loss: 0.206052 | Grad Norm: 0.02 | LR: 3.57e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-29 12:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.27e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:18:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.256530 | Val Loss: 0.273926 | Grad Norm: 0.04 | LR: 5.50e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:18:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.247239 | Val Loss: 0.270202 | Grad Norm: 0.03 | LR: 2.04e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:18:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.244381 | Val Loss: 0.269329 | Grad Norm: 0.03 | LR: 3.64e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-29 12:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.89e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:19:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.313696 | Val Loss: 0.339405 | Grad Norm: 0.04 | LR: 5.96e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:20:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.304022 | Val Loss: 0.335593 | Grad Norm: 0.03 | LR: 2.22e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:20:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.300999 | Val Loss: 0.334735 | Grad Norm: 0.03 | LR: 3.95e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-29 12:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.81e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:21:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.392254 | Val Loss: 0.428325 | Grad Norm: 0.05 | LR: 5.90e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:21:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.380847 | Val Loss: 0.423970 | Grad Norm: 0.04 | LR: 2.19e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:22:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.377335 | Val Loss: 0.422921 | Grad Norm: 0.04 | LR: 3.91e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-29 12:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:22:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.469333 | Val Loss: 0.511596 | Grad Norm: 0.05 | LR: 6.43e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:23:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.456313 | Val Loss: 0.506656 | Grad Norm: 0.03 | LR: 2.39e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:23:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.452425 | Val Loss: 0.505622 | Grad Norm: 0.03 | LR: 4.26e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:24:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-29 12:24:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:24:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.29e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:24:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.559259 | Val Loss: 0.612908 | Grad Norm: 0.06 | LR: 6.26e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:24:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.545093 | Val Loss: 0.607726 | Grad Norm: 0.05 | LR: 2.33e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:25:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.540758 | Val Loss: 0.606409 | Grad Norm: 0.04 | LR: 4.14e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:25:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-29 12:25:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:25:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.43e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:26:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.653213 | Val Loss: 0.712744 | Grad Norm: 0.07 | LR: 6.37e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:26:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.636984 | Val Loss: 0.706913 | Grad Norm: 0.05 | LR: 2.37e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:27:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.632251 | Val Loss: 0.705606 | Grad Norm: 0.05 | LR: 4.22e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:27:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-29 12:27:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:27:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 12:27:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.773803 | Val Loss: 0.842855 | Grad Norm: 0.09 | LR: 6.57e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:28:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.754637 | Val Loss: 0.836527 | Grad Norm: 0.06 | LR: 2.44e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:28:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.749165 | Val Loss: 0.834810 | Grad Norm: 0.06 | LR: 4.34e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-29 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-29 12:29:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.895467 | Val Loss: 0.973486 | Grad Norm: 0.07 | LR: 5.14e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:29:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.876930 | Val Loss: 0.969451 | Grad Norm: 0.05 | LR: 5.00e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:30:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-29 12:30:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:30:07 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.56e-05, Patience: 2\n",
      "\u001b[32m[2025-10-29 12:30:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 1.060874 | Val Loss: 1.150612 | Grad Norm: 0.09 | LR: 4.91e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:31:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 1.037825 | Val Loss: 1.145350 | Grad Norm: 0.07 | LR: 4.78e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:31:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-29 12:31:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:31:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.22e-05, Patience: 3\n",
      "\u001b[32m[2025-10-29 12:31:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 1.255676 | Val Loss: 1.353099 | Grad Norm: 0.12 | LR: 6.21e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:32:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 1.222866 | Val Loss: 1.341431 | Grad Norm: 0.09 | LR: 2.31e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:32:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 1.214217 | Val Loss: 1.338808 | Grad Norm: 0.08 | LR: 4.11e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:32:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-29 12:32:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:32:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:33:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 1.747820 | Val Loss: 1.748644 | Grad Norm: nan | LR: 5.04e-05 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 12:33:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 1.765301 | Val Loss: 1.697678 | Grad Norm: nan | LR: 1.87e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:34:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 1.759934 | Val Loss: 1.690378 | Grad Norm: nan | LR: 3.33e-06 | Time: 25.2s\n",
      "\u001b[32m[2025-10-29 12:34:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-29 12:34:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-29 12:34:34 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 6.67e-05, Patience: 4\n",
      "\u001b[32m[2025-10-29 12:35:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 3.010244 | Val Loss: 2.881070 | Grad Norm: 1.31 | LR: 5.04e-05 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:35:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 2.769216 | Val Loss: 2.797470 | Grad Norm: 0.81 | LR: 1.87e-05 | Time: 25.4s\n",
      "\u001b[32m[2025-10-29 12:35:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 2.720712 | Val Loss: 2.782697 | Grad Norm: 0.70 | LR: 3.33e-06 | Time: 25.3s\n",
      "\u001b[32m[2025-10-29 12:36:12 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/sgra_adaptive/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 12:36:12 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/sgra_adaptive/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-29 12:36:12 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 3083.08s (51.38min)\n",
      "\u001b[32m[2025-10-29 12:36:12 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-29 12:36:18 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/sgra_adaptive/model\n",
      "\u001b[32m[2025-10-29 12:36:18 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:32<00:00,  1.09it/s]\n",
      "wikitext2:15.281258583068848\n",
      "\u001b[32m[2025-10-29 12:39:05 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 15.28\n",
      "2025-10-29 12:39:05.448347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761741545.468561   97008 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761741545.474595   97008 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761741545.492347   97008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761741545.492390   97008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761741545.492393   97008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761741545.492396   97008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 12:39:10 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 12:39:11 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 12:39:15 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 124273.49it/s]\n",
      "\u001b[32m[2025-10-29 12:39:42 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2388.59it/s]\n",
      "\u001b[32m[2025-10-29 12:39:48 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1035.60it/s]\n",
      "\u001b[32m[2025-10-29 12:39:50 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 999.42it/s]\n",
      "\u001b[32m[2025-10-29 12:39:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [35:33<00:00, 26.19it/s]\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5572|±  |0.0140|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4112|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5272|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.4899|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4508|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6425|±  |0.0112|\n",
      "|          |       |none  |     0|acc_norm|0.6746|±  |0.0109|\n",
      "\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 52.52%\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/sgra_adaptive/results.json\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-29 13:16:08 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_adaptive_training \\\n",
    "  --wbits 2 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --group_size 32 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/sgra_adaptive \\\n",
    "  --save_quant_dir $BASE_OUTPUT/sgra_adaptive/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0611dc2",
   "metadata": {},
   "source": [
    "# Uniform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prhW2YGMn67m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prhW2YGMn67m",
    "outputId": "5bf07ac2-1969-44e7-c0f5-8961cd813009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'meta-llama/Llama-2-7b-hf', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '2', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/baseline2', '--save_quant_dir', './output/research_experiments/baseline/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-29 15:34:25 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/baseline2', save_quant_dir='./output/research_experiments/baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-29 15:34:25 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  5.89it/s]\n",
      "\u001b[32m[2025-10-29 15:34:27 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-29 15:34:27 root]\u001b[0m\u001b[33m(main_block_ap.py 170)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-29 15:34:27 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-29 15:34:27 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-29 15:34:29 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-29 15:34:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:34:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:4.1657120164018124e-05 val_loss:2.232501719845459e-05 quant_lr:5.1362214303938806e-05 norm:0.00116574 max memory_allocated 6758.517578125 time 24.23160743713379 \n",
      "\u001b[32m[2025-10-29 15:35:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:2.0673391190939583e-05 val_loss:2.019976091105491e-05 quant_lr:5e-06 norm:0.00018876 max memory_allocated 6821.986328125 time 24.247385501861572 \n",
      "\u001b[32m[2025-10-29 15:35:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:35:40 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:36:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.009859480895102024 val_loss:0.008401842787861824 quant_lr:5.1362214303938806e-05 norm:nan max memory_allocated 6821.986328125 time 23.98904037475586 \n",
      "\u001b[32m[2025-10-29 15:36:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.003294529626145959 val_loss:0.0066707804799079895 quant_lr:5e-06 norm:0.38859493 max memory_allocated 6821.986328125 time 24.189125061035156 \n",
      "\u001b[32m[2025-10-29 15:36:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:36:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:37:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.0029059022199362516 val_loss:0.007057900074869394 quant_lr:5.1362214303938806e-05 norm:0.00236106 max memory_allocated 6821.986328125 time 24.26565456390381 \n",
      "\u001b[32m[2025-10-29 15:37:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.0027267204131931067 val_loss:0.007007333915680647 quant_lr:5e-06 norm:0.00165162 max memory_allocated 6822.486328125 time 24.299295902252197 \n",
      "\u001b[32m[2025-10-29 15:37:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:37:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:38:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.003616657806560397 val_loss:0.007759311236441135 quant_lr:5.1362214303938806e-05 norm:0.00213056 max memory_allocated 6822.486328125 time 24.269298315048218 \n",
      "\u001b[32m[2025-10-29 15:38:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0033547438215464354 val_loss:0.007685424759984016 quant_lr:5e-06 norm:0.00173008 max memory_allocated 6822.486328125 time 24.313220500946045 \n",
      "\u001b[32m[2025-10-29 15:38:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:38:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:38:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:38:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:38:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:38:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:39:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:39:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:39:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.004974230192601681 val_loss:0.0090969018638134 quant_lr:5.1362214303938806e-05 norm:0.00292623 max memory_allocated 6822.486328125 time 24.30140781402588 \n",
      "\u001b[32m[2025-10-29 15:39:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.004584926646202803 val_loss:0.008977806195616722 quant_lr:5e-06 norm:0.00228045 max memory_allocated 6822.486328125 time 24.339447498321533 \n",
      "\u001b[32m[2025-10-29 15:40:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:40:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:40:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.007083422038704157 val_loss:0.011147595942020416 quant_lr:5.1362214303938806e-05 norm:0.00389697 max memory_allocated 6822.486328125 time 24.290396451950073 \n",
      "\u001b[32m[2025-10-29 15:41:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.006491431966423988 val_loss:0.011008210480213165 quant_lr:5e-06 norm:0.00236414 max memory_allocated 6822.486328125 time 24.332338094711304 \n",
      "\u001b[32m[2025-10-29 15:41:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:41:20 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:41:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.010061324574053288 val_loss:0.014120085164904594 quant_lr:5.1362214303938806e-05 norm:0.00637614 max memory_allocated 6822.486328125 time 24.2914617061615 \n",
      "\u001b[32m[2025-10-29 15:42:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.009158585220575333 val_loss:0.013916092924773693 quant_lr:5e-06 norm:0.00432443 max memory_allocated 6822.486328125 time 24.390777587890625 \n",
      "\u001b[32m[2025-10-29 15:42:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:42:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:42:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.013642415404319763 val_loss:0.017821477726101875 quant_lr:5.1362214303938806e-05 norm:0.00707219 max memory_allocated 6822.486328125 time 24.288891553878784 \n",
      "\u001b[32m[2025-10-29 15:43:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.012510303407907486 val_loss:0.017573557794094086 quant_lr:5e-06 norm:0.00532338 max memory_allocated 6822.486328125 time 24.35669732093811 \n",
      "\u001b[32m[2025-10-29 15:43:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:43:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:44:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.018223272636532784 val_loss:0.022562945261597633 quant_lr:5.1362214303938806e-05 norm:0.00838460 max memory_allocated 6822.486328125 time 24.314701318740845 \n",
      "\u001b[32m[2025-10-29 15:44:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.016759121790528297 val_loss:0.02220418117940426 quant_lr:5e-06 norm:0.00533032 max memory_allocated 6822.486328125 time 24.37956213951111 \n",
      "\u001b[32m[2025-10-29 15:44:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:44:49 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:45:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.023518480360507965 val_loss:0.028021378442645073 quant_lr:5.1362214303938806e-05 norm:0.00942820 max memory_allocated 6822.486328125 time 24.32280993461609 \n",
      "\u001b[32m[2025-10-29 15:45:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.02171158976852894 val_loss:0.027604393661022186 quant_lr:5e-06 norm:0.00576349 max memory_allocated 6822.486328125 time 24.367557048797607 \n",
      "\u001b[32m[2025-10-29 15:45:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:45:59 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:46:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.029553378000855446 val_loss:0.03426843136548996 quant_lr:5.1362214303938806e-05 norm:0.01168356 max memory_allocated 6822.486328125 time 24.32013201713562 \n",
      "\u001b[32m[2025-10-29 15:46:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.02737550437450409 val_loss:0.033794064074754715 quant_lr:5e-06 norm:0.00775332 max memory_allocated 6822.486328125 time 24.394634246826172 \n",
      "\u001b[32m[2025-10-29 15:46:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:47:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:47:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0348985455930233 val_loss:0.039999820291996 quant_lr:5.1362214303938806e-05 norm:0.01320083 max memory_allocated 6822.486328125 time 24.328014373779297 \n",
      "\u001b[32m[2025-10-29 15:48:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.032448820769786835 val_loss:0.039432983845472336 quant_lr:5e-06 norm:0.00818346 max memory_allocated 6822.486328125 time 24.393468618392944 \n",
      "\u001b[32m[2025-10-29 15:48:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:48:20 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:48:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.04178635776042938 val_loss:0.047227680683135986 quant_lr:5.1362214303938806e-05 norm:0.01250414 max memory_allocated 6822.486328125 time 24.328129768371582 \n",
      "\u001b[32m[2025-10-29 15:49:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.03898302838206291 val_loss:0.04659176990389824 quant_lr:5e-06 norm:0.00838844 max memory_allocated 6822.486328125 time 24.393917560577393 \n",
      "\u001b[32m[2025-10-29 15:49:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:49:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:49:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.04982282593846321 val_loss:0.05591985583305359 quant_lr:5.1362214303938806e-05 norm:0.01449380 max memory_allocated 6822.486328125 time 24.317917823791504 \n",
      "\u001b[32m[2025-10-29 15:50:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.046581488102674484 val_loss:0.05516577139496803 quant_lr:5e-06 norm:0.01013350 max memory_allocated 6822.486328125 time 24.39188861846924 \n",
      "\u001b[32m[2025-10-29 15:50:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:50:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:51:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.05892182141542435 val_loss:0.06560345739126205 quant_lr:5.1362214303938806e-05 norm:0.01498499 max memory_allocated 6822.486328125 time 24.315859079360962 \n",
      "\u001b[32m[2025-10-29 15:51:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.05526021122932434 val_loss:0.06474988907575607 quant_lr:5e-06 norm:0.01068017 max memory_allocated 6822.486328125 time 24.372672080993652 \n",
      "\u001b[32m[2025-10-29 15:51:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:51:51 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:52:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.07112997770309448 val_loss:0.07874435931444168 quant_lr:5.1362214303938806e-05 norm:0.01679766 max memory_allocated 6822.486328125 time 24.332536458969116 \n",
      "\u001b[32m[2025-10-29 15:52:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.06667577475309372 val_loss:0.07766237109899521 quant_lr:5e-06 norm:0.01182793 max memory_allocated 6822.486328125 time 24.39672350883484 \n",
      "\u001b[32m[2025-10-29 15:52:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:52:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:52:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:52:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:52:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:52:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:53:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:53:01 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:53:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.09363746643066406 val_loss:0.10282815247774124 quant_lr:5.1362214303938806e-05 norm:0.02232996 max memory_allocated 6822.486328125 time 24.302027702331543 \n",
      "\u001b[32m[2025-10-29 15:53:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.08785348385572433 val_loss:0.10151341557502747 quant_lr:5e-06 norm:0.01575232 max memory_allocated 6822.486328125 time 24.37485098838806 \n",
      "\u001b[32m[2025-10-29 15:54:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:54:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:54:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.11263245344161987 val_loss:0.1242222934961319 quant_lr:5.1362214303938806e-05 norm:0.02188808 max memory_allocated 6822.486328125 time 24.2883780002594 \n",
      "\u001b[32m[2025-10-29 15:55:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.10647374391555786 val_loss:0.12285029888153076 quant_lr:5e-06 norm:0.01551284 max memory_allocated 6822.486328125 time 24.39021897315979 \n",
      "\u001b[32m[2025-10-29 15:55:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:55:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:55:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.14208711683750153 val_loss:0.15681225061416626 quant_lr:5.1362214303938806e-05 norm:0.02427382 max memory_allocated 6822.486328125 time 24.292138814926147 \n",
      "\u001b[32m[2025-10-29 15:56:15 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.13509127497673035 val_loss:0.15521147847175598 quant_lr:5e-06 norm:0.01745318 max memory_allocated 6822.486328125 time 24.368489503860474 \n",
      "\u001b[32m[2025-10-29 15:56:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:56:32 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:57:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.177174910902977 val_loss:0.19588696956634521 quant_lr:5.1362214303938806e-05 norm:0.02408209 max memory_allocated 6822.486328125 time 24.306846380233765 \n",
      "\u001b[32m[2025-10-29 15:57:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.169442817568779 val_loss:0.1941610872745514 quant_lr:5e-06 norm:0.01828207 max memory_allocated 6822.486328125 time 24.35416316986084 \n",
      "\u001b[32m[2025-10-29 15:57:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:57:43 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:58:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.2287740707397461 val_loss:0.2549588084220886 quant_lr:5.1362214303938806e-05 norm:0.03666005 max memory_allocated 6822.486328125 time 24.281402826309204 \n",
      "\u001b[32m[2025-10-29 15:58:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.21909373998641968 val_loss:0.25291335582733154 quant_lr:5e-06 norm:0.02770841 max memory_allocated 6822.486328125 time 24.36426305770874 \n",
      "\u001b[32m[2025-10-29 15:58:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 15:58:53 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 15:59:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.28060582280158997 val_loss:0.3163573443889618 quant_lr:5.1362214303938806e-05 norm:0.03246418 max memory_allocated 6822.486328125 time 24.28016448020935 \n",
      "\u001b[32m[2025-10-29 15:59:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.2706431448459625 val_loss:0.31446996331214905 quant_lr:5e-06 norm:0.02510284 max memory_allocated 6822.486328125 time 24.371225595474243 \n",
      "\u001b[32m[2025-10-29 15:59:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 15:59:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 15:59:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 15:59:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 15:59:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:00:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:00:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:00:03 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:00:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.35172349214553833 val_loss:0.39923810958862305 quant_lr:5.1362214303938806e-05 norm:0.04746825 max memory_allocated 6822.486328125 time 24.308324337005615 \n",
      "\u001b[32m[2025-10-29 16:00:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.3398725688457489 val_loss:0.39712852239608765 quant_lr:5e-06 norm:0.03600278 max memory_allocated 6822.486328125 time 24.379576683044434 \n",
      "\u001b[32m[2025-10-29 16:01:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:01:13 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:01:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.42267167568206787 val_loss:0.47859442234039307 quant_lr:5.1362214303938806e-05 norm:0.04050549 max memory_allocated 6822.486328125 time 24.306752920150757 \n",
      "\u001b[32m[2025-10-29 16:02:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.409773588180542 val_loss:0.4762285053730011 quant_lr:5e-06 norm:0.03018297 max memory_allocated 6822.486328125 time 24.39150857925415 \n",
      "\u001b[32m[2025-10-29 16:02:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:02:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:02:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.505647599697113 val_loss:0.5748249292373657 quant_lr:5.1362214303938806e-05 norm:0.05222048 max memory_allocated 6822.486328125 time 24.304342031478882 \n",
      "\u001b[32m[2025-10-29 16:03:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.4913249611854553 val_loss:0.5723211169242859 quant_lr:5e-06 norm:0.03980375 max memory_allocated 6822.486328125 time 24.39164972305298 \n",
      "\u001b[32m[2025-10-29 16:03:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:03:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:04:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.5923436880111694 val_loss:0.6695154309272766 quant_lr:5.1362214303938806e-05 norm:0.06017631 max memory_allocated 6822.486328125 time 24.312942028045654 \n",
      "\u001b[32m[2025-10-29 16:04:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.5765581727027893 val_loss:0.6667625308036804 quant_lr:5e-06 norm:0.04722350 max memory_allocated 6822.486328125 time 24.371564388275146 \n",
      "\u001b[32m[2025-10-29 16:04:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:04:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:05:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.7039570212364197 val_loss:0.793724536895752 quant_lr:5.1362214303938806e-05 norm:0.07430077 max memory_allocated 6822.486328125 time 24.32454013824463 \n",
      "\u001b[32m[2025-10-29 16:05:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.685688853263855 val_loss:0.7911940217018127 quant_lr:5e-06 norm:0.05298639 max memory_allocated 6822.486328125 time 24.386348724365234 \n",
      "\u001b[32m[2025-10-29 16:05:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:05:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:06:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.818724513053894 val_loss:0.919087827205658 quant_lr:5.1362214303938806e-05 norm:0.05902459 max memory_allocated 6822.486328125 time 24.327457666397095 \n",
      "\u001b[32m[2025-10-29 16:06:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.7987495064735413 val_loss:0.9151896834373474 quant_lr:5e-06 norm:0.04622366 max memory_allocated 6822.486328125 time 24.369978666305542 \n",
      "\u001b[32m[2025-10-29 16:06:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:06:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:06:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:06:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:07:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:07:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:07:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:07:05 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:07:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.9664195775985718 val_loss:1.0828195810317993 quant_lr:5.1362214303938806e-05 norm:0.08120660 max memory_allocated 6822.486328125 time 24.328067302703857 \n",
      "\u001b[32m[2025-10-29 16:07:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.9418408274650574 val_loss:1.0784841775894165 quant_lr:5e-06 norm:0.06155280 max memory_allocated 6822.486328125 time 24.39385151863098 \n",
      "\u001b[32m[2025-10-29 16:08:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:08:16 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:08:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:1.1359914541244507 val_loss:1.2671265602111816 quant_lr:5.1362214303938806e-05 norm:0.09864378 max memory_allocated 6822.486328125 time 24.323557138442993 \n",
      "\u001b[32m[2025-10-29 16:09:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:1.1063584089279175 val_loss:1.2620733976364136 quant_lr:5e-06 norm:0.07488012 max memory_allocated 6822.486328125 time 24.38848352432251 \n",
      "\u001b[32m[2025-10-29 16:09:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:09:26 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:09:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:1.8027350902557373 val_loss:1.702923059463501 quant_lr:5.1362214303938806e-05 norm:nan max memory_allocated 6822.486328125 time 24.260924339294434 \n",
      "\u001b[32m[2025-10-29 16:10:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:1.721961259841919 val_loss:1.6807355880737305 quant_lr:5e-06 norm:nan max memory_allocated 6822.486328125 time 24.30984926223755 \n",
      "\u001b[32m[2025-10-29 16:10:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:10:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 205.545472M\n",
      "\u001b[32m[2025-10-29 16:11:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:2.8021132946014404 val_loss:2.7368502616882324 quant_lr:5.1362214303938806e-05 norm:1.05541897 max memory_allocated 6822.486328125 time 24.33643913269043 \n",
      "\u001b[32m[2025-10-29 16:11:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:2.6259140968322754 val_loss:2.7031567096710205 quant_lr:5e-06 norm:0.68028283 max memory_allocated 6822.486328125 time 24.393101930618286 \n",
      "\u001b[32m[2025-10-29 16:11:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-29 16:11:47 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2240.49675822258\n",
      "\u001b[32m[2025-10-29 16:11:47 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-29 16:11:54 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 166/166 [02:31<00:00,  1.10it/s]\n",
      "wikitext2:14.032194137573242\n",
      "\u001b[32m[2025-10-29 16:14:40 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 14.03\n",
      "2025-10-29 16:14:40.688269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761754480.709996  155357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761754480.716380  155357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761754480.734483  155357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761754480.734519  155357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761754480.734522  155357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761754480.734526  155357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-29 16:14:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 16:14:46 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-29 16:14:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 124623.22it/s]\n",
      "\u001b[32m[2025-10-29 16:15:18 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2410.10it/s]\n",
      "\u001b[32m[2025-10-29 16:15:24 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1040.41it/s]\n",
      "\u001b[32m[2025-10-29 16:15:27 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1013.54it/s]\n",
      "\u001b[32m[2025-10-29 16:15:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [35:33<00:00, 26.20it/s]\n",
      "\u001b[32m[2025-10-29 16:51:45 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5659|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4200|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.5475|±  |0.0050|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.4987|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.4596|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6496|±  |0.0111|\n",
      "|          |       |none  |     0|acc_norm|0.6649|±  |0.0110|\n",
      "\n",
      "\u001b[32m[2025-10-29 16:51:45 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 53.36%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "  --model $MODEL \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "--wbits 2 \\\n",
    "--group_size 128 \\\n",
    "--quant_lr 1e-4 \\\n",
    "--weight_lr 2e-5 \\\n",
    "--real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/baseline2 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/baseline/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uz84yORDzxVt",
   "metadata": {
    "id": "Uz84yORDzxVt"
   },
   "source": [
    "# MPQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j9uaP508l8jK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9uaP508l8jK",
    "outputId": "a6b5a076-5227-49a2-d9c6-31c106462996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'meta-llama/Llama-2-7b-hf', '--sensitivity_file', './sensitivity_results_llama_2_7b_hf_128.json', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--use_mixed_precision', '--mpq_strategy', 'aggressive', '--target_avg_bits', '2.0', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--real_quant', '--output_dir', './output/research_experiments/combined3', '--save_quant_dir', './output/research_experiments/combined3/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: meta-llama/Llama-2-7b-hf\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: aggressive\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.0\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='meta-llama/Llama-2-7b-hf', cache_dir='./cache', output_dir='./output/research_experiments/combined3', save_quant_dir='./output/research_experiments/combined3/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_llama_2_7b_hf_128.json', use_mixed_precision=True, mpq_strategy='aggressive', target_avg_bits=2.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-30 06:09:31 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Llama-2-7b-hf\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.42it/s]\n",
      "\u001b[32m[2025-10-30 06:09:32 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-30 06:09:32 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-30 06:09:32 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Llama-2-7b-hf_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-30 06:09:32 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-30 06:09:34 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_llama_2_7b_hf_128.json\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: aggressive, Target Avg: 2.0 bits\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.03 bits\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 7.88x vs FP16\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-30 06:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:10:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000029 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:10:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000016 | Val Loss: 0.000015 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:10:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-30 06:10:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-30 06:11:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.002671 | Val Loss: 0.010642 | Grad Norm: 0.44 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:11:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.004492 | Val Loss: 0.009753 | Grad Norm: 0.47 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:11:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-30 06:11:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:12:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.003133 | Val Loss: 0.009757 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:12:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.002901 | Val Loss: 0.009624 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-30 06:13:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:13:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.003561 | Val Loss: 0.010184 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:13:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.003309 | Val Loss: 0.010087 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:14:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-30 06:14:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:14:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.004451 | Val Loss: 0.011065 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:15:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.004112 | Val Loss: 0.010975 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:15:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-30 06:15:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:15:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.005909 | Val Loss: 0.012526 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:16:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.005460 | Val Loss: 0.012418 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:16:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-30 06:16:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.007986 | Val Loss: 0.014590 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:17:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.007323 | Val Loss: 0.014444 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:17:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-30 06:17:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:18:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.010607 | Val Loss: 0.017314 | Grad Norm: 0.00 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:18:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.009738 | Val Loss: 0.017129 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:18:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-30 06:18:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:19:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.014145 | Val Loss: 0.020951 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:19:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.012983 | Val Loss: 0.020705 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:20:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-30 06:20:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:20:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.018168 | Val Loss: 0.025211 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:20:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.016765 | Val Loss: 0.024915 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:21:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-30 06:21:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:21:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.023002 | Val Loss: 0.030231 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:22:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.021227 | Val Loss: 0.029865 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-30 06:22:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:22:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.027108 | Val Loss: 0.034806 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.025178 | Val Loss: 0.034400 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:23:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-30 06:23:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:24:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.032437 | Val Loss: 0.040540 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:24:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.030240 | Val Loss: 0.040071 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:24:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-30 06:24:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:25:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.038497 | Val Loss: 0.047178 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:25:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.035928 | Val Loss: 0.046626 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:25:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-30 06:25:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:26:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.045719 | Val Loss: 0.055010 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:26:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.042740 | Val Loss: 0.054412 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:27:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-30 06:27:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:27:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.055012 | Val Loss: 0.065246 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:28:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.051422 | Val Loss: 0.064440 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 06:28:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-30 06:28:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:28:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.072015 | Val Loss: 0.083764 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:29:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.067280 | Val Loss: 0.082719 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:29:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-30 06:29:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.9665, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:29:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.086461 | Val Loss: 0.100326 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:30:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.081423 | Val Loss: 0.099244 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.1s\n",
      "\u001b[32m[2025-10-30 06:30:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-30 06:30:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.8856, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.108691 | Val Loss: 0.125401 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:31:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.103021 | Val Loss: 0.124197 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:31:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-30 06:31:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7995, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-30 06:32:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.139877 | Val Loss: 0.159266 | Grad Norm: 0.02 | LR: 5.14e-05 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:32:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.132733 | Val Loss: 0.157670 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:32:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-30 06:32:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.7500, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-30 06:33:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.183507 | Val Loss: 0.208649 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:33:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.174544 | Val Loss: 0.206780 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:34:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-30 06:34:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5348, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-30 06:34:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.228240 | Val Loss: 0.261040 | Grad Norm: 0.03 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:35:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.218851 | Val Loss: 0.259165 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:35:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-30 06:35:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5602, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-30 06:35:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.289236 | Val Loss: 0.331367 | Grad Norm: 0.04 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:36:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.277964 | Val Loss: 0.329291 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:36:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-30 06:36:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3499, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:37:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.359572 | Val Loss: 0.406681 | Grad Norm: 0.04 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:37:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.345575 | Val Loss: 0.403967 | Grad Norm: 0.03 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:37:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-30 06:37:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4137, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:38:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.439162 | Val Loss: 0.496390 | Grad Norm: 0.05 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:38:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.423897 | Val Loss: 0.493552 | Grad Norm: 0.04 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:38:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-30 06:38:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3720, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:39:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.525498 | Val Loss: 0.587894 | Grad Norm: 0.07 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:39:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.507850 | Val Loss: 0.584682 | Grad Norm: 0.05 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:40:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-30 06:40:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3022, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:40:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.635284 | Val Loss: 0.707171 | Grad Norm: 0.08 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:40:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.614934 | Val Loss: 0.703593 | Grad Norm: 0.05 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:41:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-30 06:41:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:41:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.749145 | Val Loss: 0.828994 | Grad Norm: 0.06 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:42:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.727295 | Val Loss: 0.824775 | Grad Norm: 0.05 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:42:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-30 06:42:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0923, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:42:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.896724 | Val Loss: 0.988110 | Grad Norm: 0.08 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:43:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.869705 | Val Loss: 0.982951 | Grad Norm: 0.06 | LR: 5.00e-06 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-30 06:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4326, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-30 06:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 1.066998 | Val Loss: 1.168281 | Grad Norm: 0.10 | LR: 5.14e-05 | Time: 24.7s\n",
      "\u001b[32m[2025-10-30 06:44:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 1.034067 | Val Loss: 1.162361 | Grad Norm: 0.07 | LR: 5.00e-06 | Time: 24.8s\n",
      "\u001b[32m[2025-10-30 06:44:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-30 06:44:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:45:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 1.275475 | Val Loss: 1.388929 | Grad Norm: 0.19 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:45:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 1.234150 | Val Loss: 1.381013 | Grad Norm: 0.14 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:45:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-30 06:45:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 64\n",
      "\u001b[32m[2025-10-30 06:46:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 2.168293 | Val Loss: 2.263161 | Grad Norm: 1.05 | LR: 5.14e-05 | Time: 24.9s\n",
      "\u001b[32m[2025-10-30 06:46:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 1.984418 | Val Loss: 2.231438 | Grad Norm: 0.57 | LR: 5.00e-06 | Time: 25.0s\n",
      "\u001b[32m[2025-10-30 06:47:04 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/research_experiments/combined3/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 06:47:04 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/research_experiments/combined3/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-30 06:47:05 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2252.63s (37.54min)\n",
      "\u001b[32m[2025-10-30 06:47:05 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-30 06:47:13 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/research_experiments/combined3/model\n",
      "\u001b[32m[2025-10-30 06:47:13 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 166/166 [02:33<00:00,  1.08it/s]\n",
      "wikitext2:10.819982528686523\n",
      "\u001b[32m[2025-10-30 06:49:55 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 10.82\n",
      "2025-10-30 06:49:55.714963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761806995.734292   47812 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761806995.740118   47812 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761806995.756602   47812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761806995.756630   47812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761806995.756634   47812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761806995.756637   47812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-30 06:49:59 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 06:50:00 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-30 06:50:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 129883.49it/s]\n",
      "\u001b[32m[2025-10-30 06:50:19 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2430.01it/s]\n",
      "\u001b[32m[2025-10-30 06:50:25 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1042.06it/s]\n",
      "\u001b[32m[2025-10-30 06:50:27 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1025.46it/s]\n",
      "\u001b[32m[2025-10-30 06:50:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [35:46<00:00, 26.03it/s]\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6109|±  |0.0137|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4651|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.6124|±  |0.0049|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.6014|±  |0.0100|\n",
      "|          |       |none  |     0|acc_norm|0.5434|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6785|±  |0.0109|\n",
      "|          |       |none  |     0|acc_norm|0.7057|±  |0.0106|\n",
      "\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 58.90%\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/research_experiments/combined3/results.json\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-30 07:26:57 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model $MODEL \\\n",
    "  --sensitivity_file ./sensitivity_results_llama_2_7b_hf_128.json \\\n",
    "  --calib_dataset wikitext2 \\\n",
    "  --train_size $TRAIN_SIZE \\\n",
    "  --val_size $VAL_SIZE \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy aggressive \\\n",
    "  --target_avg_bits 2.0 \\\n",
    "  --quant_lr 1e-4 \\\n",
    "  --weight_lr 2e-5 \\\n",
    "  --real_quant \\\n",
    "  --output_dir $BASE_OUTPUT/combined3 \\\n",
    "  --save_quant_dir $BASE_OUTPUT/combined3/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d07cd",
   "metadata": {},
   "source": [
    "## Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JNzQnrfkOtV9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNzQnrfkOtV9",
    "outputId": "140370bd-4d4c-43c6-8f72-a0ddf06e55ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.10 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/combined3/model\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsUZmKMsO4Bv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsUZmKMsO4Bv",
    "outputId": "79be44b8-7e81-436d-b36f-e6775f5a3710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.11 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/baseline\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3m9b02-zPA7Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3m9b02-zPA7Y",
    "outputId": "13bcc9f7-544e-4067-def7-1ae37b3ff7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.02 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/baseline3\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JJyrNdHyPMkG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJyrNdHyPMkG",
    "outputId": "0ad23f23-a4fe-4276-a234-8065254ba5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.65 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/mpq_adaptive12\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bnzAFTY8Pi-B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnzAFTY8Pi-B",
    "outputId": "1f6f0a35-535e-4083-eb7b-092d13a78532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.11 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/sgra_adaptive\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HB6JfhtKPrfM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HB6JfhtKPrfM",
    "outputId": "67926c29-f28e-44fa-a7dd-df64939edfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.10 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/combined3\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tGViGjrqPiuy",
   "metadata": {
    "id": "tGViGjrqPiuy"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2DbbtZVHPUop",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DbbtZVHPUop",
    "outputId": "fe57ef43-7d01-4ba6-d9e7-da6b4ca23ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.87 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/mpq4\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JwooEZ3TCPnk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwooEZ3TCPnk",
    "outputId": "e4fce937-fe4c-459a-d48c-7f6e3482545e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.87 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/combined4\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CD-yKCtcCUCY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD-yKCtcCUCY",
    "outputId": "5527c49c-755d-4a2f-9357-becbc681d0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/research_experiments/combined3\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "so6AgYJxCWDz",
   "metadata": {
    "id": "so6AgYJxCWDz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
