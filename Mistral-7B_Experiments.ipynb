{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZsYftmUTUMN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1761643824040,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "MZsYftmUTUMN",
    "outputId": "368d394c-1326-4164-d454-b90905f30439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PR'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 197 (delta 98), reused 182 (delta 83), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (197/197), 164.81 KiB | 20.60 MiB/s, done.\n",
      "Resolving deltas: 100% (98/98), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/gautamk01/PR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rdcwsZJYTvi8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761644005470,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "rdcwsZJYTvi8",
    "outputId": "b285d149-1ae6-47a4-9bb5-e0d7a83762aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/PR\n"
     ]
    }
   ],
   "source": [
    "%cd PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8de07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 158871,
     "status": "ok",
     "timestamp": 1761643985921,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "6cb8de07",
    "outputId": "f236f7f9-f9a8-4a7e-873b-8d49041b6704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.28.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.41.0 (from -r requirements.txt (line 2))\n",
      "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lm_eval==0.4.2 (from -r requirements.txt (line 4))\n",
      "  Downloading lm_eval-0.4.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 7))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.40.1 (from -r requirements.txt (line 8))\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0 (from -r requirements.txt (line 9))\n",
      "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (5.29.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (18.1.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Collecting evaluate (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonlines (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (2.14.1)\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.17.1)\n",
      "Collecting pybind11>=2.6.2 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pytablewriter (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (1.6.1)\n",
      "Collecting sqlitedict (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tqdm-multiprocess (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (0.25.0)\n",
      "Collecting word2number (from lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 4)) (10.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1->-r requirements.txt (line 8))\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0->-r requirements.txt (line 1)) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 3)) (2025.10.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.12/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (75.2.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4))\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 4)) (8.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=26599980eb11d2518ece5efe1e85d151d86cb7dcffe1a2d8226ed90eb05f6519\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=b34b9e6a5bc59d68614ce9e5595198c0c71dd725e1324e39565421b2d2073f1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=cb9a18dbad830d015bd543adfbe3f86edd6c0002832271fd83309aae5a1f3164\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, bitsandbytes, triton, tqdm, tcolorpy, pybind11, pyarrow-hotfix, portalocker, pathvalidate, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mbstrdecoder, jsonlines, fsspec, colorama, typepy, tqdm-multiprocess, sacrebleu, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, rouge-score, transformers, datasets, DataProperty, accelerate, tabledata, evaluate, pytablewriter, lm_eval\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.64.1 which is incompatible.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
      "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 accelerate-0.28.0 bitsandbytes-0.41.0 colorama-0.4.6 datasets-2.18.0 evaluate-0.4.6 fsspec-2024.2.0 jsonlines-4.0.0 lm_eval-0.4.2 mbstrdecoder-1.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pathvalidate-3.3.1 portalocker-3.2.0 pyarrow-hotfix-0.7 pybind11-3.0.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tokenizers-0.19.1 torch-2.2.2 tqdm-4.64.1 tqdm-multiprocess-0.0.11 transformers-4.40.1 triton-2.2.0 typepy-1.3.4 word2number-1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "17c81620dab043a387eba21eb4a5ffaa",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f687b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5145,
     "status": "ok",
     "timestamp": 1761643991069,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "a08f687b",
    "outputId": "ff22627e-2e8c-4858-c7a1-5e3f4382ee16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.28.0 in /usr/local/lib/python3.12/dist-packages (0.28.0)\n",
      "Requirement already satisfied: transformers==4.40.1 in /usr/local/lib/python3.12/dist-packages (4.40.1)\n",
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.28.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.28.0) (1.1.10)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.28.0) (12.6.85)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "Successfully installed peft-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.28.0 transformers==4.40.1 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e65bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1761644010651,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "21e65bee",
    "outputId": "554ef811-8010-48be-8982-5ddf61f04c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `newProject` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `newProject`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token #token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28035373",
   "metadata": {
    "id": "28035373",
    "outputId": "d3df788b-7994-4d79-bd36-74f82490c9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 3128.57 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = './output/research_experiments/mpq_adaptive_3bit/model'  # replace with your actual path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # size in MB\n",
    "\n",
    "model_size_mb = get_dir_size(model_dir)\n",
    "print(f\"Model size on disk: {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cbaad",
   "metadata": {
    "id": "ec2cbaad"
   },
   "outputs": [],
   "source": [
    "MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "SENSITIVITY_FILE=\"./sensitivity_results_llama_2_7b_hf.json\"\n",
    "TRAIN_SIZE=128\n",
    "VAL_SIZE=16\n",
    "BASE_OUTPUT=\"./output/research_experiments\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf4c4a",
   "metadata": {},
   "source": [
    "# 4 bit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae22fbe",
   "metadata": {},
   "source": [
    "# Uniform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cc6d9",
   "metadata": {
    "id": "652cc6d9",
    "outputId": "6ae250d1-a145-46f9-f0ad-ea1916e7b4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '4', '--group_size', '128', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--output_dir', './output/mistral_baseline', '--save_quant_dir', './output/mistral_baseline/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-12 06:46:14 root]\u001b[0m\u001b[33m(main_block_ap.py 115)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_baseline', save_quant_dir='./output/mistral_baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-12 06:46:14 root]\u001b[0m\u001b[33m(main_block_ap.py 119)\u001b[0m: INFO net is None, setting as Mistral-7B-Instruct-v0.2\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.77s/it]\n",
      "\u001b[32m[2025-10-12 06:46:20 root]\u001b[0m\u001b[33m(main_block_ap.py 138)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-12 06:46:20 root]\u001b[0m\u001b[33m(main_block_ap.py 145)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-12 06:46:20 root]\u001b[0m\u001b[33m(main_block_ap.py 147)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-12 06:46:20 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-12 06:46:21 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-12 06:46:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:46:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:1.853136240015374e-07 val_loss:1.2610242094979185e-07 quant_lr:1.540866429118164e-05 norm:0.00012317 max memory_allocated 7411.298828125 time 19.43341565132141 \n",
      "\u001b[32m[2025-10-12 06:47:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:1.1850710279759369e-07 val_loss:1.0511877235330758e-07 quant_lr:1.5e-06 norm:0.00009669 max memory_allocated 7475.173828125 time 19.31710147857666 \n",
      "\u001b[32m[2025-10-12 06:47:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:47:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:8.339868145412765e-06 val_loss:4.566254574456252e-06 quant_lr:1.540866429118164e-05 norm:0.00504526 max memory_allocated 7475.173828125 time 19.40117597579956 \n",
      "\u001b[32m[2025-10-12 06:47:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:2.543655000408762e-06 val_loss:4.098566932952963e-06 quant_lr:1.5e-06 norm:0.00257123 max memory_allocated 7475.923828125 time 19.409624576568604 \n",
      "\u001b[32m[2025-10-12 06:47:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:47:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:03 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:48:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:2.994241413034615e-06 val_loss:4.7449425437662285e-06 quant_lr:1.540866429118164e-05 norm:0.00009835 max memory_allocated 7475.923828125 time 19.44137692451477 \n",
      "\u001b[32m[2025-10-12 06:48:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:2.752758973656455e-06 val_loss:4.645276021619793e-06 quant_lr:1.5e-06 norm:0.00005750 max memory_allocated 7475.923828125 time 19.441293001174927 \n",
      "\u001b[32m[2025-10-12 06:48:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:48:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:49:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:4.246010576025583e-06 val_loss:5.993990271235816e-06 quant_lr:1.540866429118164e-05 norm:0.00018141 max memory_allocated 7475.923828125 time 19.450552463531494 \n",
      "\u001b[32m[2025-10-12 06:49:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:3.9744513742334675e-06 val_loss:5.903835699427873e-06 quant_lr:1.5e-06 norm:0.00006546 max memory_allocated 7475.923828125 time 19.450927734375 \n",
      "\u001b[32m[2025-10-12 06:49:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:49:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:50:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:6.087858764658449e-06 val_loss:7.872311471146531e-06 quant_lr:1.540866429118164e-05 norm:0.00014781 max memory_allocated 7475.923828125 time 19.527279138565063 \n",
      "\u001b[32m[2025-10-12 06:50:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:5.761609827459324e-06 val_loss:7.748199095658492e-06 quant_lr:1.5e-06 norm:0.00008564 max memory_allocated 7475.923828125 time 19.519132614135742 \n",
      "\u001b[32m[2025-10-12 06:50:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:50:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:51:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:9.224288987752516e-06 val_loss:1.1094814908574335e-05 quant_lr:1.540866429118164e-05 norm:0.00020845 max memory_allocated 7475.923828125 time 19.48434615135193 \n",
      "\u001b[32m[2025-10-12 06:51:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:8.74399938766146e-06 val_loss:1.0840560207725503e-05 quant_lr:1.5e-06 norm:0.00014332 max memory_allocated 7475.923828125 time 19.47970414161682 \n",
      "\u001b[32m[2025-10-12 06:51:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:51:31 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:51:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:1.3449735888571013e-05 val_loss:1.5403891666210257e-05 quant_lr:1.540866429118164e-05 norm:0.00033349 max memory_allocated 7475.923828125 time 19.716648817062378 \n",
      "\u001b[32m[2025-10-12 06:52:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:1.2649040399992373e-05 val_loss:1.5018092199170496e-05 quant_lr:1.5e-06 norm:0.00019912 max memory_allocated 7476.923828125 time 19.58583402633667 \n",
      "\u001b[32m[2025-10-12 06:52:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:52:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:52:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:53:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:53:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:53:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:53:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:53:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:54:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:1.9631508621387184e-05 val_loss:2.1740477677667513e-05 quant_lr:1.540866429118164e-05 norm:0.00046235 max memory_allocated 7476.923828125 time 19.469431400299072 \n",
      "\u001b[32m[2025-10-12 06:54:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:1.844229882408399e-05 val_loss:2.1096235286677256e-05 quant_lr:1.5e-06 norm:0.00039994 max memory_allocated 7476.923828125 time 19.495304346084595 \n",
      "\u001b[32m[2025-10-12 06:54:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:54:49 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:55:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:2.5689667381811887e-05 val_loss:2.82086093648104e-05 quant_lr:1.540866429118164e-05 norm:0.00060611 max memory_allocated 7476.923828125 time 19.50072455406189 \n",
      "\u001b[32m[2025-10-12 06:55:32 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:2.409123771940358e-05 val_loss:2.7440568374004215e-05 quant_lr:1.5e-06 norm:0.00036970 max memory_allocated 7476.923828125 time 19.5176362991333 \n",
      "\u001b[32m[2025-10-12 06:55:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:55:44 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:56:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:3.3701853681122884e-05 val_loss:3.722799738170579e-05 quant_lr:1.540866429118164e-05 norm:0.00048407 max memory_allocated 7476.923828125 time 19.556023836135864 \n",
      "\u001b[32m[2025-10-12 06:56:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:3.186734102200717e-05 val_loss:3.634797394624911e-05 quant_lr:1.5e-06 norm:0.00045256 max memory_allocated 7476.923828125 time 19.540929794311523 \n",
      "\u001b[32m[2025-10-12 06:56:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:56:43 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:57:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:4.321108281146735e-05 val_loss:4.772907050210051e-05 quant_lr:1.540866429118164e-05 norm:0.00074957 max memory_allocated 7476.923828125 time 19.629884719848633 \n",
      "\u001b[32m[2025-10-12 06:57:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:4.085907494300045e-05 val_loss:4.6496166760334745e-05 quant_lr:1.5e-06 norm:0.00051828 max memory_allocated 7477.048828125 time 19.54660677909851 \n",
      "\u001b[32m[2025-10-12 06:57:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:57:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:58:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:5.545938984141685e-05 val_loss:6.146547821117565e-05 quant_lr:1.540866429118164e-05 norm:0.00072931 max memory_allocated 7477.048828125 time 19.612022161483765 \n",
      "\u001b[32m[2025-10-12 06:58:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:5.2539406169671565e-05 val_loss:6.005986870150082e-05 quant_lr:1.5e-06 norm:0.00047381 max memory_allocated 7477.048828125 time 19.628254175186157 \n",
      "\u001b[32m[2025-10-12 06:58:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:58:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 06:59:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:7.129339792300016e-05 val_loss:7.943319360492751e-05 quant_lr:1.540866429118164e-05 norm:0.00096092 max memory_allocated 7477.048828125 time 19.7147855758667 \n",
      "\u001b[32m[2025-10-12 06:59:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:6.742372352164239e-05 val_loss:7.764437759760767e-05 quant_lr:1.5e-06 norm:0.00061775 max memory_allocated 7477.048828125 time 19.61581540107727 \n",
      "\u001b[32m[2025-10-12 06:59:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 06:59:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:00:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:8.922055712901056e-05 val_loss:0.00010031445708591491 quant_lr:1.540866429118164e-05 norm:0.00085902 max memory_allocated 7477.048828125 time 19.618518352508545 \n",
      "\u001b[32m[2025-10-12 07:00:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:8.500597323291004e-05 val_loss:9.840103302849457e-05 quant_lr:1.5e-06 norm:0.00055954 max memory_allocated 7477.048828125 time 19.633177757263184 \n",
      "\u001b[32m[2025-10-12 07:00:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:00:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:01:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.00010847307567019016 val_loss:0.0001226588647114113 quant_lr:1.540866429118164e-05 norm:0.00101435 max memory_allocated 7477.048828125 time 19.535847425460815 \n",
      "\u001b[32m[2025-10-12 07:01:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.00010285178723279387 val_loss:0.00012010482896585017 quant_lr:1.5e-06 norm:0.00071713 max memory_allocated 7477.048828125 time 19.538095951080322 \n",
      "\u001b[32m[2025-10-12 07:01:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:01:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:02:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.00014095181541051716 val_loss:0.00016075783059932292 quant_lr:1.540866429118164e-05 norm:0.00140457 max memory_allocated 7477.048828125 time 19.608792543411255 \n",
      "\u001b[32m[2025-10-12 07:02:20 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0001333936961600557 val_loss:0.00015730348241049796 quant_lr:1.5e-06 norm:0.00100621 max memory_allocated 7477.048828125 time 19.62373375892639 \n",
      "\u001b[32m[2025-10-12 07:02:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:02:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:02:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.0001763085019774735 val_loss:0.00020241881429683417 quant_lr:1.540866429118164e-05 norm:0.00133207 max memory_allocated 7477.048828125 time 19.55632209777832 \n",
      "\u001b[32m[2025-10-12 07:03:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.00016679779218975455 val_loss:0.00019832442922051996 quant_lr:1.5e-06 norm:0.00100688 max memory_allocated 7477.048828125 time 19.551137924194336 \n",
      "\u001b[32m[2025-10-12 07:03:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:03:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:03:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.00022080082271713763 val_loss:0.0002546669857110828 quant_lr:1.540866429118164e-05 norm:0.00177472 max memory_allocated 7477.048828125 time 19.559346437454224 \n",
      "\u001b[32m[2025-10-12 07:04:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.00020912630134262145 val_loss:0.00024954817490652204 quant_lr:1.5e-06 norm:0.00134049 max memory_allocated 7477.048828125 time 19.55518078804016 \n",
      "\u001b[32m[2025-10-12 07:04:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:04:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:04:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.00028215680504217744 val_loss:0.0003275977214798331 quant_lr:1.540866429118164e-05 norm:0.00263615 max memory_allocated 7477.048828125 time 19.56669330596924 \n",
      "\u001b[32m[2025-10-12 07:04:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.00026644233730621636 val_loss:0.00032034411560744047 quant_lr:1.5e-06 norm:0.00214644 max memory_allocated 7477.048828125 time 19.56925129890442 \n",
      "\u001b[32m[2025-10-12 07:05:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:05:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.0003770648909267038 val_loss:0.0004457348259165883 quant_lr:1.540866429118164e-05 norm:0.00446602 max memory_allocated 7477.048828125 time 19.67622685432434 \n",
      "\u001b[32m[2025-10-12 07:05:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.0003560650220606476 val_loss:0.0004362707259133458 quant_lr:1.5e-06 norm:0.00347350 max memory_allocated 7477.048828125 time 19.57157015800476 \n",
      "\u001b[32m[2025-10-12 07:05:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:05:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:06:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.00046480822493322194 val_loss:0.0005587784689851105 quant_lr:1.540866429118164e-05 norm:0.00265164 max memory_allocated 7477.048828125 time 19.567373991012573 \n",
      "\u001b[32m[2025-10-12 07:06:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.00044204408186487854 val_loss:0.0005494039505720139 quant_lr:1.5e-06 norm:0.00240791 max memory_allocated 7477.048828125 time 19.66046142578125 \n",
      "\u001b[32m[2025-10-12 07:06:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:06:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:07:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.0005620842566713691 val_loss:0.0006848008488304913 quant_lr:1.540866429118164e-05 norm:0.00343116 max memory_allocated 7477.048828125 time 19.578157663345337 \n",
      "\u001b[32m[2025-10-12 07:07:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.0005371240549720824 val_loss:0.0006744093261659145 quant_lr:1.5e-06 norm:0.00291938 max memory_allocated 7477.048828125 time 19.573941469192505 \n",
      "\u001b[32m[2025-10-12 07:07:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:07:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:08:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.0006753179477527738 val_loss:0.0008354744059033692 quant_lr:1.540866429118164e-05 norm:0.00337540 max memory_allocated 7477.048828125 time 19.572722911834717 \n",
      "\u001b[32m[2025-10-12 07:08:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.00064823281718418 val_loss:0.0008251789258792996 quant_lr:1.5e-06 norm:0.00281285 max memory_allocated 7477.048828125 time 19.576208353042603 \n",
      "\u001b[32m[2025-10-12 07:08:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:08:38 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:09:01 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.000821011490188539 val_loss:0.0010245722951367497 quant_lr:1.540866429118164e-05 norm:0.00373461 max memory_allocated 7477.048828125 time 19.595538854599 \n",
      "\u001b[32m[2025-10-12 07:09:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.0007893072324804962 val_loss:0.0010125117842108011 quant_lr:1.5e-06 norm:0.00331126 max memory_allocated 7477.048828125 time 19.593875408172607 \n",
      "\u001b[32m[2025-10-12 07:09:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:09:30 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:09:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.0009911359520629048 val_loss:0.0012508542276918888 quant_lr:1.540866429118164e-05 norm:0.00361275 max memory_allocated 7477.048828125 time 19.57843518257141 \n",
      "\u001b[32m[2025-10-12 07:10:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.0009547460358589888 val_loss:0.001237648306414485 quant_lr:1.5e-06 norm:0.00297946 max memory_allocated 7477.048828125 time 19.58118200302124 \n",
      "\u001b[32m[2025-10-12 07:10:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:10:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:10:46 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.0011956830276176333 val_loss:0.0015214314917102456 quant_lr:1.540866429118164e-05 norm:0.00443670 max memory_allocated 7477.048828125 time 19.583789825439453 \n",
      "\u001b[32m[2025-10-12 07:11:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.0011529125040397048 val_loss:0.001505854306742549 quant_lr:1.5e-06 norm:0.00331102 max memory_allocated 7477.048828125 time 19.585886478424072 \n",
      "\u001b[32m[2025-10-12 07:11:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:11:15 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:11:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.001445552334189415 val_loss:0.0018546429928392172 quant_lr:1.540866429118164e-05 norm:0.00482557 max memory_allocated 7477.048828125 time 19.582717895507812 \n",
      "\u001b[32m[2025-10-12 07:11:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.0013959184288978577 val_loss:0.001836725976318121 quant_lr:1.5e-06 norm:0.00369246 max memory_allocated 7477.048828125 time 19.57993459701538 \n",
      "\u001b[32m[2025-10-12 07:12:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:12:31 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.001753391930833459 val_loss:0.002273700898513198 quant_lr:1.540866429118164e-05 norm:0.00840548 max memory_allocated 7477.048828125 time 19.588807821273804 \n",
      "\u001b[32m[2025-10-12 07:12:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.0016968654235824943 val_loss:0.0022526956163346767 quant_lr:1.5e-06 norm:0.00716558 max memory_allocated 7477.048828125 time 19.59001398086548 \n",
      "\u001b[32m[2025-10-12 07:12:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:12:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:13:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.0022258430253714323 val_loss:0.002900542225688696 quant_lr:1.540866429118164e-05 norm:0.00719136 max memory_allocated 7477.048828125 time 19.668272972106934 \n",
      "\u001b[32m[2025-10-12 07:13:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.002147563034668565 val_loss:0.0028672334738075733 quant_lr:1.5e-06 norm:0.00561937 max memory_allocated 7477.048828125 time 19.67003321647644 \n",
      "\u001b[32m[2025-10-12 07:13:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:13:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:14:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.002935475669801235 val_loss:0.0038534391205757856 quant_lr:1.540866429118164e-05 norm:0.01197681 max memory_allocated 7477.048828125 time 19.655989170074463 \n",
      "\u001b[32m[2025-10-12 07:14:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.0028273866046220064 val_loss:0.00380377983674407 quant_lr:1.5e-06 norm:0.00943374 max memory_allocated 7477.048828125 time 19.672540426254272 \n",
      "\u001b[32m[2025-10-12 07:14:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:14:48 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:15:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.0038040350191295147 val_loss:0.004984958562999964 quant_lr:1.540866429118164e-05 norm:0.02310208 max memory_allocated 7477.048828125 time 19.605120420455933 \n",
      "\u001b[32m[2025-10-12 07:15:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.003650449449196458 val_loss:0.004911435768008232 quant_lr:1.5e-06 norm:0.01843562 max memory_allocated 7477.048828125 time 19.608087062835693 \n",
      "\u001b[32m[2025-10-12 07:15:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:15:40 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-12 07:16:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.00546701904386282 val_loss:0.007188883610069752 quant_lr:1.540866429118164e-05 norm:0.05760908 max memory_allocated 7477.048828125 time 19.68872332572937 \n",
      "\u001b[32m[2025-10-12 07:16:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.005226940382272005 val_loss:0.007067578379064798 quant_lr:1.5e-06 norm:0.05444971 max memory_allocated 7477.048828125 time 19.695026397705078 \n",
      "\u001b[32m[2025-10-12 07:16:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-12 07:16:34 root]\u001b[0m\u001b[33m(main_block_ap.py 166)\u001b[0m: INFO 1813.565012216568\n",
      "\u001b[32m[2025-10-12 07:16:34 root]\u001b[0m\u001b[33m(main_block_ap.py 169)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-12 07:16:37 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 163/163 [02:00<00:00,  1.35it/s]\n",
      "wikitext2:6.035289764404297\n",
      "\u001b[32m[2025-10-12 07:18:43 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 6.04\n",
      "\u001b[32m[2025-10-12 07:18:44 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 07:18:45 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-12 07:18:49 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 163307.31it/s]\n",
      "\u001b[32m[2025-10-12 07:18:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5192.78it/s]\n",
      "\u001b[32m[2025-10-12 07:18:58 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2250.02it/s]\n",
      "\u001b[32m[2025-10-12 07:18:59 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1754.76it/s]\n",
      "\u001b[32m[2025-10-12 07:19:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [28:14<00:00, 32.98it/s]\n",
      "\u001b[32m[2025-10-12 07:47:33 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7316|±  |0.0125|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6552|±  |0.0047|\n",
      "|          |       |none  |     0|acc_norm|0.8314|±  |0.0037|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8072|±  |0.0081|\n",
      "|          |       |none  |     0|acc_norm|0.7652|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.7992|±  |0.0093|\n",
      "|          |       |none  |     0|acc_norm|0.8047|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-12 07:47:33 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 74.83%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 4 \\\n",
    "       --group_size 128 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --output_dir ./output/mistral_baseline \\\n",
    "       --save_quant_dir ./output/mistral_baseline/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cc316",
   "metadata": {},
   "source": [
    "# MPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761a5ad",
   "metadata": {
    "id": "f761a5ad",
    "outputId": "0f1047a3-c105-4a68-ccee-777e66238479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2.json', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '4.0', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_mpq', '--save_quant_dir', './output/mistral_mpq/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq', save_quant_dir='./output/mistral_mpq/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-12 07:52:26 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.38s/it]\n",
      "\u001b[32m[2025-10-12 07:52:34 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-12 07:52:34 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-12 07:52:34 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-12 07:52:34 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 1.0000 to 1.0000\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 529)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 530)\u001b[0m: INFO   Bit-widths: [8, 8, 6, 8, 6, 6, 6, 6, 3, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 531)\u001b[0m: INFO   Actual Avg: 3.97 bits\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 532)\u001b[0m: INFO   Compression: 4.03x vs FP16\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-12 07:52:35 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 128\n",
      "\u001b[32m[2025-10-12 07:52:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.3s\n",
      "\u001b[32m[2025-10-12 07:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.3s\n",
      "\u001b[32m[2025-10-12 07:53:26 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-12 07:53:26 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 64\n",
      "\u001b[32m[2025-10-12 07:53:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000010 | Val Loss: 0.000003 | Grad Norm: 0.07 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:54:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:54:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-12 07:54:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 07:54:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000003 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:54:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:55:08 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-12 07:55:08 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 128\n",
      "\u001b[32m[2025-10-12 07:55:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:55:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:56:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-12 07:56:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 07:56:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000003 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 07:56:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:56:51 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-12 07:56:51 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 07:57:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:57:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:57:44 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-12 07:57:44 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 07:58:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:58:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:58:37 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-12 07:58:37 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 07:59:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000005 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 07:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000004 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.4s\n",
      "\u001b[32m[2025-10-12 07:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-12 07:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 07:59:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000034 | Val Loss: 0.000029 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:00:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000028 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:00:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-12 08:00:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 08:00:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000029 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.7s\n",
      "\u001b[32m[2025-10-12 08:01:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000027 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:01:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-12 08:01:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 08:01:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000029 | Val Loss: 0.000029 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:01:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000027 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:02:07 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-12 08:02:07 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:02:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000083 | Val Loss: 0.000075 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:02:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000071 | Val Loss: 0.000073 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:03:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-12 08:03:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:03:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000139 | Val Loss: 0.000130 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:03:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000123 | Val Loss: 0.000126 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-12 08:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:04:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000201 | Val Loss: 0.000191 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:04:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000183 | Val Loss: 0.000188 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:04:44 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-12 08:04:44 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:05:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000277 | Val Loss: 0.000265 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:05:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000251 | Val Loss: 0.000259 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:05:37 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-12 08:05:37 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:06:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000390 | Val Loss: 0.000377 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:06:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000355 | Val Loss: 0.000370 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:06:29 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-12 08:06:29 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:06:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000522 | Val Loss: 0.000510 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:07:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000478 | Val Loss: 0.000501 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:07:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-12 08:07:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:07:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000689 | Val Loss: 0.000680 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:08:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000636 | Val Loss: 0.000669 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:08:14 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-12 08:08:14 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:08:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000922 | Val Loss: 0.000917 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:08:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000849 | Val Loss: 0.000902 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:09:07 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-12 08:09:07 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:09:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001274 | Val Loss: 0.001287 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:09:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001177 | Val Loss: 0.001268 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:10:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-12 08:10:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:10:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001610 | Val Loss: 0.001660 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:10:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001508 | Val Loss: 0.001640 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:10:52 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-12 08:10:52 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:11:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.001979 | Val Loss: 0.002069 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:11:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.001869 | Val Loss: 0.002049 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:11:45 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-12 08:11:45 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:12:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002408 | Val Loss: 0.002559 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:12:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002289 | Val Loss: 0.002539 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-12 08:12:38 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:13:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.002961 | Val Loss: 0.003164 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:13:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.002820 | Val Loss: 0.003141 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:13:31 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-12 08:13:31 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:13:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.003608 | Val Loss: 0.003894 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:14:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.003446 | Val Loss: 0.003866 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-12 08:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:14:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.004386 | Val Loss: 0.004765 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:15:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.004201 | Val Loss: 0.004733 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:15:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-12 08:15:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 08:15:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.005342 | Val Loss: 0.005843 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:16:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.005125 | Val Loss: 0.005803 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:16:10 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-12 08:16:10 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 08:16:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.010318 | Val Loss: 0.010096 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:16:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.009006 | Val Loss: 0.009785 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-12 08:17:03 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 08:17:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.016401 | Val Loss: 0.016101 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:17:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.014605 | Val Loss: 0.015654 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:17:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-12 08:17:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 08:18:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.026035 | Val Loss: 0.025584 | Grad Norm: 0.05 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:18:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.023464 | Val Loss: 0.024980 | Grad Norm: 0.03 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:18:47 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-12 08:18:47 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 08:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.039206 | Val Loss: 0.038488 | Grad Norm: 0.07 | LR: 1.54e-05 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:19:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.035426 | Val Loss: 0.037573 | Grad Norm: 0.05 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:19:39 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-12 08:19:39 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 08:20:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.064109 | Val Loss: 0.061889 | Grad Norm: 0.17 | LR: 1.54e-05 | Time: 19.6s\n",
      "\u001b[32m[2025-10-12 08:20:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.056877 | Val Loss: 0.060480 | Grad Norm: 0.10 | LR: 1.50e-06 | Time: 19.5s\n",
      "\u001b[32m[2025-10-12 08:20:31 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 08:20:31 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 08:20:31 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 1677.05s (27.95min)\n",
      "\u001b[32m[2025-10-12 08:20:31 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-12 08:20:35 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq/model\n",
      "\u001b[32m[2025-10-12 08:20:35 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 163/163 [02:02<00:00,  1.33it/s]\n",
      "wikitext2:7.201603889465332\n",
      "\u001b[32m[2025-10-12 08:22:42 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.20\n",
      "\u001b[32m[2025-10-12 08:22:43 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 08:22:43 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-12 08:22:47 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 169592.57it/s]\n",
      "\u001b[32m[2025-10-12 08:22:54 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5434.23it/s]\n",
      "\u001b[32m[2025-10-12 08:22:57 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2328.02it/s]\n",
      "\u001b[32m[2025-10-12 08:22:58 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1809.42it/s]\n",
      "\u001b[32m[2025-10-12 08:22:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [28:06<00:00, 33.12it/s]\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7206|±  |0.0126|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6212|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8001|±  |0.0040|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7984|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7538|±  |0.0088|\n",
      "|piqa      |      1|none  |     0|acc     |0.7927|±  |0.0095|\n",
      "|          |       |none  |     0|acc_norm|0.8014|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 73.32%\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq/results.json\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-12 08:51:24 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2.json \\\n",
    "       --use_mixed_precision \\\n",
    "       --mpq_strategy adaptive \\\n",
    "       --target_avg_bits 4.0 \\\n",
    "       --train_size 128 --val_size 16 \\\n",
    "       --quant_lr 3e-5 --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_mpq \\\n",
    "       --save_quant_dir ./output/mistral_mpq/model \\\n",
    "       --eval_ppl --eval_tasks piqa,arc_easy,hellaswag,winogrande\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yqrdrQtvMlqf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4983092,
     "status": "ok",
     "timestamp": 1761632812258,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "yqrdrQtvMlqf",
    "outputId": "f6d0ccdb-3f22-47e9-edae-b201efaad260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'conservative', '--target_avg_bits', '4.0', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_combined', '--save_quant_dir', './output/mistral_combined/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_combined', save_quant_dir='./output/mistral_combined/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=4.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 05:03:53 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.13s/it]\n",
      "\u001b[32m[2025-10-28 05:03:58 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 05:03:58 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 05:03:58 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 05:03:58 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 05:04:00 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [5, 6, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4]\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 05:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-28 05:04:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 25.8s\n",
      "\u001b[32m[2025-10-28 05:04:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 25.9s\n",
      "\u001b[32m[2025-10-28 05:05:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 05:05:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-28 05:05:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000018 | Val Loss: 0.000006 | Grad Norm: 0.06 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:06:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000005 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:06:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 05:06:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:06:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 25.9s\n",
      "\u001b[32m[2025-10-28 05:07:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:07:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 05:07:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-28 05:08:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:08:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000004 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:08:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 05:08:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:09:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000006 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:09:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000006 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:10:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 05:10:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:10:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000009 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:11:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000009 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:11:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 05:11:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:11:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000014 | Val Loss: 0.000017 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:12:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000013 | Val Loss: 0.000016 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:12:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 05:12:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:13:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000021 | Val Loss: 0.000024 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:13:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000020 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:13:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 05:13:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:14:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000028 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:14:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000026 | Val Loss: 0.000030 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:15:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 05:15:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000037 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:16:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000035 | Val Loss: 0.000040 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:16:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 05:16:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:16:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000048 | Val Loss: 0.000052 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:17:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000045 | Val Loss: 0.000051 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:17:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 05:17:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:18:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000062 | Val Loss: 0.000068 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:18:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000059 | Val Loss: 0.000066 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:18:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 05:18:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:19:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000080 | Val Loss: 0.000088 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:19:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000076 | Val Loss: 0.000086 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:20:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 05:20:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:20:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000100 | Val Loss: 0.000111 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:21:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000096 | Val Loss: 0.000109 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:21:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 05:21:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000122 | Val Loss: 0.000136 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:22:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000116 | Val Loss: 0.000133 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:22:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 05:22:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:23:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000159 | Val Loss: 0.000177 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:23:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000151 | Val Loss: 0.000174 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:23:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 05:23:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:24:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000200 | Val Loss: 0.000224 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 05:24:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000190 | Val Loss: 0.000220 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:25:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 05:25:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:25:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000252 | Val Loss: 0.000282 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:26:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000239 | Val Loss: 0.000277 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:26:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 05:26:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:27:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000322 | Val Loss: 0.000364 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:27:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000305 | Val Loss: 0.000357 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:27:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 05:27:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:28:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.000430 | Val Loss: 0.000494 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:28:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000408 | Val Loss: 0.000484 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:29:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 05:29:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:29:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.000532 | Val Loss: 0.000619 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:29:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.000507 | Val Loss: 0.000610 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:30:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 05:30:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:30:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.000643 | Val Loss: 0.000756 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:31:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.000616 | Val Loss: 0.000746 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:31:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 05:31:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:32:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.000773 | Val Loss: 0.000922 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:32:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.000744 | Val Loss: 0.000913 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:32:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 05:32:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 05:33:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.001286 | Val Loss: 0.001413 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:33:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.001189 | Val Loss: 0.001396 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:34:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 05:34:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:34:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.001428 | Val Loss: 0.001665 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:35:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.001375 | Val Loss: 0.001650 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:35:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 05:35:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 05:35:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.002109 | Val Loss: 0.002339 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:36:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.001973 | Val Loss: 0.002314 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:36:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 05:36:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 05:37:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.002863 | Val Loss: 0.003156 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:37:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.002688 | Val Loss: 0.003125 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:37:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 05:37:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:38:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.003197 | Val Loss: 0.003707 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:38:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.003100 | Val Loss: 0.003680 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:39:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 05:39:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:39:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.003865 | Val Loss: 0.004556 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:40:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.003748 | Val Loss: 0.004520 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:40:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 05:40:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:40:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.004905 | Val Loss: 0.005878 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:41:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.004756 | Val Loss: 0.005831 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:41:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 05:41:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:42:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.006145 | Val Loss: 0.007426 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 05:42:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.005948 | Val Loss: 0.007345 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:42:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 05:42:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 05:43:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.008567 | Val Loss: 0.010438 | Grad Norm: 0.06 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 05:43:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.008249 | Val Loss: 0.010308 | Grad Norm: 0.05 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 05:44:09 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_combined/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 05:44:09 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_combined/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 05:44:10 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2411.88s (40.20min)\n",
      "\u001b[32m[2025-10-28 05:44:10 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 05:44:22 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_combined/model\n",
      "\u001b[32m[2025-10-28 05:44:22 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:46<00:00,  1.02s/it]\n",
      "wikitext2:6.073157787322998\n",
      "\u001b[32m[2025-10-28 05:47:22 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.07\n",
      "2025-10-28 05:47:22.397828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761630442.418955   17932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761630442.425261   17932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761630442.446979   17932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761630442.447007   17932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761630442.447010   17932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761630442.447013   17932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 05:47:27 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 05:47:28 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 05:47:32 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126939.21it/s]\n",
      "\u001b[32m[2025-10-28 05:47:59 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2416.88it/s]\n",
      "\u001b[32m[2025-10-28 05:48:05 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1026.68it/s]\n",
      "\u001b[32m[2025-10-28 05:48:07 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1004.62it/s]\n",
      "\u001b[32m[2025-10-28 05:48:09 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:57<00:00, 24.54it/s]\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7285|±  |0.0125|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6521|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8277|±  |0.0038|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8152|±  |0.0080|\n",
      "|          |       |none  |     0|acc_norm|0.7664|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.7916|±  |0.0095|\n",
      "|          |       |none  |     0|acc_norm|0.8020|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 74.69%\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_combined/results.json\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 06:26:49 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "  --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy conservative \\\n",
    "  --target_avg_bits 4.0 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --quant_lr 3e-5 \\\n",
    "  --weight_lr 2e-6 \\\n",
    "  --real_quant \\\n",
    "  --output_dir ./output/mistral_combined \\\n",
    "  --save_quant_dir ./output/mistral_combined/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c766ce",
   "metadata": {},
   "source": [
    "# SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db76df2",
   "metadata": {
    "id": "9db76df2",
    "outputId": "cbb0f493-dbae-4ab8-fc91-b430f94913f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2.json', '--use_adaptive_training', '--wbits', '4', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_sgra', '--save_quant_dir', './output/mistral_sgra/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_sgra', save_quant_dir='./output/mistral_sgra/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-12 09:41:50 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.94s/it]\n",
      "\u001b[32m[2025-10-12 09:41:56 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-12 09:41:56 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-12 09:41:56 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-12 09:41:56 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-12 09:41:57 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 1.0000 to 1.0000\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 2-3\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 2.04e-05-3.00e-05\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:41:58 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.28e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 09:42:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.72e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:42:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.41e-06 | Time: 19.8s\n",
      "\u001b[32m[2025-10-12 09:43:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.14e-06 | Time: 19.8s\n",
      "\u001b[32m[2025-10-12 09:43:09 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-12 09:43:09 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:43:09 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.04e-05, Patience: 4\n",
      "\u001b[32m[2025-10-12 09:43:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/3 | Train Loss: 0.000009 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:43:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 5.72e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:44:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.02e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-12 09:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:44:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000003 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:45:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:45:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-12 09:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:45:34 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.28e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 09:45:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.72e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:46:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 6.41e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:46:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.14e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:46:48 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-12 09:46:48 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:46:48 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:47:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000006 | Val Loss: 0.000008 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:47:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000005 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:47:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000005 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:48:01 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-12 09:48:01 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:48:01 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:48:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000009 | Val Loss: 0.000011 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:48:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000008 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:49:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000008 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:49:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-12 09:49:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:49:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:49:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000013 | Val Loss: 0.000015 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:49:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000012 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 09:50:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000012 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:50:28 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-12 09:50:28 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:50:28 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:50:52 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000019 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:51:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000018 | Val Loss: 0.000020 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:51:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000017 | Val Loss: 0.000020 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:51:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-12 09:51:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:51:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000024 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 09:52:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000022 | Val Loss: 0.000026 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-12 09:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:52:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000032 | Val Loss: 0.000036 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:53:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000030 | Val Loss: 0.000035 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:53:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000029 | Val Loss: 0.000034 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:53:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-12 09:53:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:53:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:54:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.000040 | Val Loss: 0.000045 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:54:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000038 | Val Loss: 0.000044 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:54:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000037 | Val Loss: 0.000043 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:55:02 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-12 09:55:02 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:55:02 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:55:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000051 | Val Loss: 0.000058 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.2s\n",
      "\u001b[32m[2025-10-12 09:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000048 | Val Loss: 0.000057 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:55:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-12 09:55:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:55:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:56:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000066 | Val Loss: 0.000075 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:56:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000062 | Val Loss: 0.000073 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:56:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-12 09:56:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:56:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:57:12 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000083 | Val Loss: 0.000095 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:57:32 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000078 | Val Loss: 0.000093 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-12 09:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:57:42 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:58:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000101 | Val Loss: 0.000117 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:58:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000094 | Val Loss: 0.000114 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-12 09:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:58:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000131 | Val Loss: 0.000156 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000123 | Val Loss: 0.000149 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 09:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-12 09:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 09:59:30 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 09:59:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000164 | Val Loss: 0.000193 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:00:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000154 | Val Loss: 0.000188 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:00:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-12 10:00:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:00:24 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:00:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000205 | Val Loss: 0.000243 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:01:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000192 | Val Loss: 0.000237 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:01:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-12 10:01:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:01:17 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:01:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000262 | Val Loss: 0.000313 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:02:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000245 | Val Loss: 0.000305 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-12 10:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:02:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.000350 | Val Loss: 0.000427 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000328 | Val Loss: 0.000417 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:03:03 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-12 10:03:03 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:03:03 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:03:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.000431 | Val Loss: 0.000539 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:03:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.000406 | Val Loss: 0.000526 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:03:56 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-12 10:03:56 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:03:56 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:04:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.000521 | Val Loss: 0.000659 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.2s\n",
      "\u001b[32m[2025-10-12 10:04:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.000494 | Val Loss: 0.000647 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:04:49 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-12 10:04:49 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:04:49 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:05:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.000624 | Val Loss: 0.000807 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:05:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.000594 | Val Loss: 0.000794 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:05:43 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-12 10:05:43 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:05:43 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:06:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.000756 | Val Loss: 0.000989 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:06:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.000721 | Val Loss: 0.000976 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-12 10:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:06:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:07:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.000911 | Val Loss: 0.001209 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:07:20 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.000871 | Val Loss: 0.001193 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:07:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-12 10:07:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:07:30 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:07:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.001097 | Val Loss: 0.001471 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:08:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.001050 | Val Loss: 0.001453 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:08:23 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-12 10:08:23 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:08:23 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:08:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.001325 | Val Loss: 0.001795 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:09:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.001269 | Val Loss: 0.001774 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:09:16 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-12 10:09:16 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:09:16 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:09:40 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.001604 | Val Loss: 0.002200 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:10:00 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.001541 | Val Loss: 0.002177 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:10:10 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-12 10:10:10 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:10:10 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:10:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.002035 | Val Loss: 0.002805 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:10:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.001947 | Val Loss: 0.002770 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-12 10:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:11:04 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:11:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.002685 | Val Loss: 0.003723 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:11:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.002564 | Val Loss: 0.003669 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:11:57 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-12 10:11:57 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:11:57 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:12:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.003481 | Val Loss: 0.004811 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:12:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.003314 | Val Loss: 0.004733 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:12:51 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-12 10:12:51 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 10:12:51 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:13:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.005011 | Val Loss: 0.006947 | Grad Norm: 0.06 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:13:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.004767 | Val Loss: 0.006812 | Grad Norm: 0.06 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 10:13:45 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_sgra/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 10:13:45 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_sgra/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 10:13:45 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 1908.70s (31.81min)\n",
      "\u001b[32m[2025-10-12 10:13:45 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-12 10:13:49 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_sgra/model\n",
      "\u001b[32m[2025-10-12 10:13:49 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 163/163 [02:00<00:00,  1.35it/s]\n",
      "wikitext2:6.031783580780029\n",
      "\u001b[32m[2025-10-12 10:15:54 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.03\n",
      "\u001b[32m[2025-10-12 10:15:56 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 10:15:56 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-12 10:16:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 169495.21it/s]\n",
      "\u001b[32m[2025-10-12 10:16:07 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5360.04it/s]\n",
      "\u001b[32m[2025-10-12 10:16:10 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2317.67it/s]\n",
      "\u001b[32m[2025-10-12 10:16:11 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1821.77it/s]\n",
      "\u001b[32m[2025-10-12 10:16:12 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [28:09<00:00, 33.07it/s]\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7364|±  |0.0124|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6538|±  |0.0047|\n",
      "|          |       |none  |     0|acc_norm|0.8308|±  |0.0037|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8081|±  |0.0081|\n",
      "|          |       |none  |     0|acc_norm|0.7647|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.7965|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8063|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 74.87%\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_sgra/results.json\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-12 10:44:40 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2.json \\\n",
    "       --use_adaptive_training \\\n",
    "       --wbits 4 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_sgra \\\n",
    "       --save_quant_dir ./output/mistral_sgra/model \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd211839",
   "metadata": {},
   "source": [
    "# combined methord "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d9625",
   "metadata": {
    "id": "3d4d9625",
    "outputId": "422957b4-11db-492f-ccd4-67bee8c1aaa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2.json', '--use_mixed_precision', '--use_adaptive_training', '--mpq_strategy', 'adaptive', '--target_avg_bits', '4.0', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_combined', '--save_quant_dir', './output/mistral_combined/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_combined', save_quant_dir='./output/mistral_combined/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-12 10:47:09 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.22s/it]\n",
      "\u001b[32m[2025-10-12 10:47:17 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-12 10:47:17 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-12 10:47:17 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-12 10:47:17 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 1.0000 to 1.0000\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 529)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 530)\u001b[0m: INFO   Bit-widths: [8, 8, 6, 8, 6, 6, 6, 6, 3, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 531)\u001b[0m: INFO   Actual Avg: 3.97 bits\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 532)\u001b[0m: INFO   Compression: 4.03x vs FP16\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 2-3\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 2.04e-05-3.00e-05\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 128\n",
      "\u001b[32m[2025-10-12 10:47:19 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.28e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 10:47:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.72e-05 | Time: 19.8s\n",
      "\u001b[32m[2025-10-12 10:48:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.41e-06 | Time: 19.8s\n",
      "\u001b[32m[2025-10-12 10:48:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.14e-06 | Time: 19.8s\n",
      "\u001b[32m[2025-10-12 10:48:31 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-12 10:48:31 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 64\n",
      "\u001b[32m[2025-10-12 10:48:31 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.04e-05, Patience: 4\n",
      "\u001b[32m[2025-10-12 10:48:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/3 | Train Loss: 0.000009 | Val Loss: 0.000005 | Grad Norm: 0.05 | LR: 1.54e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:49:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 5.72e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:49:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 1.02e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:49:43 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-12 10:49:43 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:49:43 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:50:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:50:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:50:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:50:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-12 10:50:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 128\n",
      "\u001b[32m[2025-10-12 10:50:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.28e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 10:51:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.72e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:51:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 6.41e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:51:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.14e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-12 10:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:52:08 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:52:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:52:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:53:21 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-12 10:53:21 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:53:21 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:54:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:54:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:54:34 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-12 10:54:34 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:54:34 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:54:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:55:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:55:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-12 10:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:56:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/3 | Train Loss: 0.000004 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/3 | Train Loss: 0.000004 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:56:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:57:00 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-12 10:57:00 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 10:57:00 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:57:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000033 | Val Loss: 0.000029 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:57:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000027 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 10:57:52 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-12 10:57:52 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:57:52 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:58:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/3 | Train Loss: 0.000028 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/3 | Train Loss: 0.000025 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:58:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/3 | Train Loss: 0.000024 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:59:06 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-12 10:59:06 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 256\n",
      "\u001b[32m[2025-10-12 10:59:06 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.59e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 10:59:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/3 | Train Loss: 0.000027 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 1.96e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 10:59:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/3 | Train Loss: 0.000025 | Val Loss: 0.000028 | Grad Norm: 0.00 | LR: 7.28e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:00:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/3 | Train Loss: 0.000024 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 1.30e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:00:19 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-12 11:00:19 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:00:19 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:00:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000080 | Val Loss: 0.000074 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:01:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000067 | Val Loss: 0.000071 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:01:13 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-12 11:01:13 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:01:13 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:01:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000134 | Val Loss: 0.000126 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:01:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000117 | Val Loss: 0.000122 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:02:06 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-12 11:02:06 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:02:06 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:02:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000193 | Val Loss: 0.000186 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 11:02:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000174 | Val Loss: 0.000181 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:02:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-12 11:02:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:02:59 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:03:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000266 | Val Loss: 0.000257 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:03:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000239 | Val Loss: 0.000251 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:03:52 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-12 11:03:52 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:03:52 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:04:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000374 | Val Loss: 0.000366 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:04:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000338 | Val Loss: 0.000358 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 11:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-12 11:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:05:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000501 | Val Loss: 0.000495 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 11:05:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000454 | Val Loss: 0.000485 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-12 11:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:05:39 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:06:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000659 | Val Loss: 0.000659 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:06:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000603 | Val Loss: 0.000646 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:06:32 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-12 11:06:32 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:06:32 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:06:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000882 | Val Loss: 0.000888 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:07:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000805 | Val Loss: 0.000871 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-12 11:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:07:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001216 | Val Loss: 0.001248 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:08:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001115 | Val Loss: 0.001226 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:08:18 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-12 11:08:18 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:08:18 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:08:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001535 | Val Loss: 0.001608 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:09:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001425 | Val Loss: 0.001586 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:09:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-12 11:09:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:09:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:09:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.001882 | Val Loss: 0.002004 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:09:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.001764 | Val Loss: 0.001982 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:10:04 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-12 11:10:04 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:10:04 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:10:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002285 | Val Loss: 0.002479 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:10:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002158 | Val Loss: 0.002456 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:10:58 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-12 11:10:58 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:10:58 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:11:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.002805 | Val Loss: 0.003066 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:11:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.002656 | Val Loss: 0.003042 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 11:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-12 11:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:12:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.003413 | Val Loss: 0.003778 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:12:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.003242 | Val Loss: 0.003747 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-12 11:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:12:44 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:13:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.004145 | Val Loss: 0.004624 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.1s\n",
      "\u001b[32m[2025-10-12 11:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.003948 | Val Loss: 0.004589 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:13:37 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-12 11:13:37 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-12 11:13:37 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:14:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.005043 | Val Loss: 0.005668 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:14:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.004813 | Val Loss: 0.005627 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:14:30 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-12 11:14:30 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 11:14:30 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:14:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.009823 | Val Loss: 0.009813 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 11:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.008549 | Val Loss: 0.009508 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 11:15:24 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-12 11:15:24 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 11:15:24 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:15:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.015682 | Val Loss: 0.015629 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 11:16:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.013911 | Val Loss: 0.015196 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-12 11:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 11:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:16:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.024891 | Val Loss: 0.024810 | Grad Norm: 0.04 | LR: 1.54e-05 | Time: 19.9s\n",
      "\u001b[32m[2025-10-12 11:17:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.022371 | Val Loss: 0.024217 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-12 11:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 11:17:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:17:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.037310 | Val Loss: 0.037118 | Grad Norm: 0.07 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:17:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.033655 | Val Loss: 0.036219 | Grad Norm: 0.04 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:18:04 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-12 11:18:04 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 11:18:04 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 11:18:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.060402 | Val Loss: 0.059230 | Grad Norm: 0.16 | LR: 1.54e-05 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:18:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.053670 | Val Loss: 0.057799 | Grad Norm: 0.10 | LR: 1.50e-06 | Time: 20.0s\n",
      "\u001b[32m[2025-10-12 11:18:57 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_combined/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 11:18:57 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_combined/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 11:18:57 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 1900.66s (31.68min)\n",
      "\u001b[32m[2025-10-12 11:18:57 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-12 11:19:01 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_combined/model\n",
      "\u001b[32m[2025-10-12 11:19:01 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 163/163 [02:01<00:00,  1.34it/s]\n",
      "wikitext2:7.2043232917785645\n",
      "\u001b[32m[2025-10-12 11:21:07 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.20\n",
      "\u001b[32m[2025-10-12 11:21:09 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 11:21:09 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-12 11:21:13 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 164668.54it/s]\n",
      "\u001b[32m[2025-10-12 11:21:20 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5327.57it/s]\n",
      "\u001b[32m[2025-10-12 11:21:22 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2343.54it/s]\n",
      "\u001b[32m[2025-10-12 11:21:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1806.47it/s]\n",
      "\u001b[32m[2025-10-12 11:21:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [28:07<00:00, 33.12it/s]\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7222|±  |0.0126|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6234|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8052|±  |0.0040|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7984|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7521|±  |0.0089|\n",
      "|piqa      |      1|none  |     0|acc     |0.7938|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.7987|±  |0.0094|\n",
      "\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 73.44%\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_combined/results.json\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-12 11:49:50 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "  --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2.json \\\n",
    "  --use_mixed_precision \\\n",
    "  --use_adaptive_training \\\n",
    "  --mpq_strategy adaptive \\\n",
    "  --target_avg_bits 4.0 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --quant_lr 3e-5 \\\n",
    "  --weight_lr 2e-6 \\\n",
    "  --real_quant \\\n",
    "  --output_dir ./output/mistral_combined \\\n",
    "  --save_quant_dir ./output/mistral_combined/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37101192",
   "metadata": {},
   "source": [
    "# 3 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61572a33",
   "metadata": {},
   "source": [
    "# uniform methord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87XEmItXg0CO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4928326,
     "status": "ok",
     "timestamp": 1761643193258,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "87XEmItXg0CO",
    "outputId": "b1ed264f-d119-4422-b878-ddce3a08212e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '3', '--group_size', '128', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--output_dir', './output/mistral_baseline', '--save_quant_dir', './output/mistral_baseline/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 07:57:49 root]\u001b[0m\u001b[33m(main_block_ap.py 115)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_baseline', save_quant_dir='./output/mistral_baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-28 07:57:49 root]\u001b[0m\u001b[33m(main_block_ap.py 119)\u001b[0m: INFO net is None, setting as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.19s/it]\n",
      "\u001b[32m[2025-10-28 07:57:54 root]\u001b[0m\u001b[33m(main_block_ap.py 138)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-28 07:57:54 root]\u001b[0m\u001b[33m(main_block_ap.py 145)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 07:57:54 root]\u001b[0m\u001b[33m(main_block_ap.py 147)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 07:57:54 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-28 07:57:55 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-28 07:57:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 07:58:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:5.420471325123799e-07 val_loss:4.329485250309517e-07 quant_lr:1.540866429118164e-05 norm:0.00012092 max memory_allocated 7411.298828125 time 25.311845302581787 \n",
      "\u001b[32m[2025-10-28 07:58:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:4.271377918030339e-07 val_loss:4.159781497037329e-07 quant_lr:1.5e-06 norm:0.00009249 max memory_allocated 7475.173828125 time 25.315911531448364 \n",
      "\u001b[32m[2025-10-28 07:58:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 07:58:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 07:58:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 07:58:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 07:59:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 07:59:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 07:59:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 07:59:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 07:59:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:8.78701321198605e-05 val_loss:6.384497100953013e-05 quant_lr:1.540866429118164e-05 norm:0.01395050 max memory_allocated 7475.173828125 time 25.527530908584595 \n",
      "\u001b[32m[2025-10-28 08:00:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:6.26147011644207e-05 val_loss:5.7567376643419266e-05 quant_lr:1.5e-06 norm:0.00994261 max memory_allocated 7475.173828125 time 25.42186665534973 \n",
      "\u001b[32m[2025-10-28 08:00:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:00:20 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:00:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:7.149751763790846e-05 val_loss:6.535198190249503e-05 quant_lr:1.540866429118164e-05 norm:0.00023624 max memory_allocated 7475.173828125 time 25.410339832305908 \n",
      "\u001b[32m[2025-10-28 08:01:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:6.886625487823039e-05 val_loss:6.451186345657334e-05 quant_lr:1.5e-06 norm:0.00018916 max memory_allocated 7475.173828125 time 25.52352285385132 \n",
      "\u001b[32m[2025-10-28 08:01:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:01:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:02:05 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:7.369968807324767e-05 val_loss:6.820958515163511e-05 quant_lr:1.540866429118164e-05 norm:0.00021842 max memory_allocated 7475.173828125 time 25.47369360923767 \n",
      "\u001b[32m[2025-10-28 08:02:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:7.151463796617463e-05 val_loss:6.739320087945089e-05 quant_lr:1.5e-06 norm:0.00017336 max memory_allocated 7475.173828125 time 25.545681476593018 \n",
      "\u001b[32m[2025-10-28 08:02:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:02:49 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:03:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:7.941576768644154e-05 val_loss:7.34836285118945e-05 quant_lr:1.540866429118164e-05 norm:0.00026315 max memory_allocated 7475.173828125 time 25.468159198760986 \n",
      "\u001b[32m[2025-10-28 08:03:45 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:7.653979264432564e-05 val_loss:7.26742873666808e-05 quant_lr:1.5e-06 norm:0.00019923 max memory_allocated 7475.173828125 time 25.587985515594482 \n",
      "\u001b[32m[2025-10-28 08:03:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:03:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:03:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:03:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:03:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:03:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:04:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:04:03 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:04:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:8.967665780801326e-05 val_loss:8.423512190347537e-05 quant_lr:1.540866429118164e-05 norm:0.00031218 max memory_allocated 7475.173828125 time 25.497182369232178 \n",
      "\u001b[32m[2025-10-28 08:04:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:8.642753527965397e-05 val_loss:8.32606092444621e-05 quant_lr:1.5e-06 norm:0.00024925 max memory_allocated 7475.173828125 time 25.58370614051819 \n",
      "\u001b[32m[2025-10-28 08:05:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:05:18 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:05:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.00010505899990675971 val_loss:9.947388753062114e-05 quant_lr:1.540866429118164e-05 norm:0.00043123 max memory_allocated 7475.173828125 time 25.529966354370117 \n",
      "\u001b[32m[2025-10-28 08:06:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.00010056849714601412 val_loss:9.854486415861174e-05 quant_lr:1.5e-06 norm:0.00029534 max memory_allocated 7475.173828125 time 25.605611085891724 \n",
      "\u001b[32m[2025-10-28 08:06:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:06:32 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:07:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.00012719150981865823 val_loss:0.00012171602429589257 quant_lr:1.540866429118164e-05 norm:0.00055253 max memory_allocated 7475.173828125 time 25.503196477890015 \n",
      "\u001b[32m[2025-10-28 08:07:29 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0001206818560604006 val_loss:0.00012009096099063754 quant_lr:1.5e-06 norm:0.00040749 max memory_allocated 7475.173828125 time 25.597187995910645 \n",
      "\u001b[32m[2025-10-28 08:07:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:07:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:08:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.00014901052054483443 val_loss:0.00014521887351293117 quant_lr:1.540866429118164e-05 norm:0.00064834 max memory_allocated 7475.173828125 time 25.520000457763672 \n",
      "\u001b[32m[2025-10-28 08:08:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.00014149461640045047 val_loss:0.00014366941468324512 quant_lr:1.5e-06 norm:0.00046437 max memory_allocated 7476.923828125 time 25.605426788330078 \n",
      "\u001b[32m[2025-10-28 08:08:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:08:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:08:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:08:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:08:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:08:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:09:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:09:02 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:09:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.00017935970390681177 val_loss:0.00017826291150413454 quant_lr:1.540866429118164e-05 norm:0.00059353 max memory_allocated 7476.923828125 time 25.532273054122925 \n",
      "\u001b[32m[2025-10-28 08:09:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.00017001516243908554 val_loss:0.00017636512347962707 quant_lr:1.5e-06 norm:0.00044051 max memory_allocated 7476.923828125 time 25.639978885650635 \n",
      "\u001b[32m[2025-10-28 08:10:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:10:17 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:10:47 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.00021572124387603253 val_loss:0.00021702289814129472 quant_lr:1.540866429118164e-05 norm:0.00094053 max memory_allocated 7476.923828125 time 25.519726514816284 \n",
      "\u001b[32m[2025-10-28 08:11:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.00020448386203497648 val_loss:0.0002143952442565933 quant_lr:1.5e-06 norm:0.00066778 max memory_allocated 7476.923828125 time 25.62514901161194 \n",
      "\u001b[32m[2025-10-28 08:11:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:11:31 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:12:02 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.00026304437778890133 val_loss:0.00026874145260080695 quant_lr:1.540866429118164e-05 norm:0.00106331 max memory_allocated 7476.923828125 time 25.54578971862793 \n",
      "\u001b[32m[2025-10-28 08:12:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.00025001331232488155 val_loss:0.00026597914984449744 quant_lr:1.5e-06 norm:0.00075138 max memory_allocated 7476.923828125 time 25.62019658088684 \n",
      "\u001b[32m[2025-10-28 08:12:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:38 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:12:45 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:13:16 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.0003239643410779536 val_loss:0.0003346576704643667 quant_lr:1.540866429118164e-05 norm:0.00133918 max memory_allocated 7476.923828125 time 25.538751125335693 \n",
      "\u001b[32m[2025-10-28 08:13:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.00030657617026008666 val_loss:0.00033095531398430467 quant_lr:1.5e-06 norm:0.00089739 max memory_allocated 7476.923828125 time 25.62240719795227 \n",
      "\u001b[32m[2025-10-28 08:13:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:13:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:13:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:13:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:13:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:13:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:14:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:14:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:14:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.0003929610247723758 val_loss:0.0004114285111427307 quant_lr:1.540866429118164e-05 norm:0.00120232 max memory_allocated 7476.923828125 time 25.528748273849487 \n",
      "\u001b[32m[2025-10-28 08:14:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.00037426763447001576 val_loss:0.00040761413401924074 quant_lr:1.5e-06 norm:0.00088924 max memory_allocated 7476.923828125 time 25.644399642944336 \n",
      "\u001b[32m[2025-10-28 08:15:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:15:14 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:15:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.0004668257024604827 val_loss:0.0004909519338980317 quant_lr:1.540866429118164e-05 norm:0.00154382 max memory_allocated 7476.923828125 time 25.529276609420776 \n",
      "\u001b[32m[2025-10-28 08:16:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.00044146893196739256 val_loss:0.0004855795996263623 quant_lr:1.5e-06 norm:0.00113999 max memory_allocated 7476.923828125 time 25.630584955215454 \n",
      "\u001b[32m[2025-10-28 08:16:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:16:28 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:16:59 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.0005913854693062603 val_loss:0.0006286742864176631 quant_lr:1.540866429118164e-05 norm:0.00188359 max memory_allocated 7476.923828125 time 25.537405252456665 \n",
      "\u001b[32m[2025-10-28 08:17:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0005585142062045634 val_loss:0.0006216952460817993 quant_lr:1.5e-06 norm:0.00142721 max memory_allocated 7476.923828125 time 25.64664888381958 \n",
      "\u001b[32m[2025-10-28 08:17:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:17:43 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:18:14 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.0007296125404536724 val_loss:0.0007825908833183348 quant_lr:1.540866429118164e-05 norm:0.00199279 max memory_allocated 7476.923828125 time 25.554535388946533 \n",
      "\u001b[32m[2025-10-28 08:18:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.0006888299831189215 val_loss:0.0007742246380075812 quant_lr:1.5e-06 norm:0.00153107 max memory_allocated 7476.923828125 time 25.614100456237793 \n",
      "\u001b[32m[2025-10-28 08:18:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:18:58 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:19:28 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.0009049901855178177 val_loss:0.0009784483117982745 quant_lr:1.540866429118164e-05 norm:0.00245508 max memory_allocated 7476.923828125 time 25.555853843688965 \n",
      "\u001b[32m[2025-10-28 08:19:54 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.0008570646750740707 val_loss:0.0009682547533884645 quant_lr:1.5e-06 norm:0.00178063 max memory_allocated 7476.923828125 time 25.63901376724243 \n",
      "\u001b[32m[2025-10-28 08:20:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:20:12 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:20:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.0011489329626783729 val_loss:0.0012537575094029307 quant_lr:1.540866429118164e-05 norm:0.00360878 max memory_allocated 7476.923828125 time 25.53958535194397 \n",
      "\u001b[32m[2025-10-28 08:21:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.001083903363905847 val_loss:0.0012384233996272087 quant_lr:1.5e-06 norm:0.00277305 max memory_allocated 7476.923828125 time 25.685549020767212 \n",
      "\u001b[32m[2025-10-28 08:21:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:21:27 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:21:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.0015305215492844582 val_loss:0.0016994636971503496 quant_lr:1.540866429118164e-05 norm:0.00449680 max memory_allocated 7476.923828125 time 25.54970622062683 \n",
      "\u001b[32m[2025-10-28 08:22:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.0014438551152125 val_loss:0.001681614201515913 quant_lr:1.5e-06 norm:0.00366103 max memory_allocated 7476.923828125 time 25.64761757850647 \n",
      "\u001b[32m[2025-10-28 08:22:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:22:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:23:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.0018901423318311572 val_loss:0.002141282893717289 quant_lr:1.540866429118164e-05 norm:0.00405929 max memory_allocated 7476.923828125 time 25.549565076828003 \n",
      "\u001b[32m[2025-10-28 08:23:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.0017980101983994246 val_loss:0.0021231938153505325 quant_lr:1.5e-06 norm:0.00315612 max memory_allocated 7476.923828125 time 25.645495414733887 \n",
      "\u001b[32m[2025-10-28 08:23:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:23:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:24:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.002287915674969554 val_loss:0.002626679604873061 quant_lr:1.540866429118164e-05 norm:0.00440464 max memory_allocated 7476.923828125 time 25.565685272216797 \n",
      "\u001b[32m[2025-10-28 08:24:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.0021881305146962404 val_loss:0.0026078778319060802 quant_lr:1.5e-06 norm:0.00357286 max memory_allocated 7476.923828125 time 25.634850025177002 \n",
      "\u001b[32m[2025-10-28 08:24:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:24:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:24:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:25:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:25:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:25:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:25:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:25:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:25:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.002744761761277914 val_loss:0.0031994362361729145 quant_lr:1.540866429118164e-05 norm:0.00469795 max memory_allocated 7476.923828125 time 25.568747997283936 \n",
      "\u001b[32m[2025-10-28 08:26:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.0026368428952991962 val_loss:0.0031808512285351753 quant_lr:1.5e-06 norm:0.00370626 max memory_allocated 7476.923828125 time 25.659085750579834 \n",
      "\u001b[32m[2025-10-28 08:26:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:26:26 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:26:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.003329102648422122 val_loss:0.003912698943167925 quant_lr:1.540866429118164e-05 norm:0.00556894 max memory_allocated 7476.923828125 time 25.561594009399414 \n",
      "\u001b[32m[2025-10-28 08:27:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.003202715888619423 val_loss:0.003891914151608944 quant_lr:1.5e-06 norm:0.00443347 max memory_allocated 7476.923828125 time 25.687522888183594 \n",
      "\u001b[32m[2025-10-28 08:27:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:27:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:28:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.004006627481430769 val_loss:0.004755929112434387 quant_lr:1.540866429118164e-05 norm:0.00552941 max memory_allocated 7476.923828125 time 25.585292100906372 \n",
      "\u001b[32m[2025-10-28 08:28:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.003862638957798481 val_loss:0.004730673041194677 quant_lr:1.5e-06 norm:0.00446652 max memory_allocated 7476.923828125 time 25.686386823654175 \n",
      "\u001b[32m[2025-10-28 08:28:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:28:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:29:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.0048152063973248005 val_loss:0.005749405827373266 quant_lr:1.540866429118164e-05 norm:0.00602881 max memory_allocated 7476.923828125 time 25.60989761352539 \n",
      "\u001b[32m[2025-10-28 08:29:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.004647649358958006 val_loss:0.005720492452383041 quant_lr:1.5e-06 norm:0.00479994 max memory_allocated 7476.923828125 time 25.652992010116577 \n",
      "\u001b[32m[2025-10-28 08:29:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:29:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:29:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:29:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:30:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:30:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:30:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:30:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:30:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.0057970080524683 val_loss:0.006966814864426851 quant_lr:1.540866429118164e-05 norm:0.00672379 max memory_allocated 7476.923828125 time 25.584001064300537 \n",
      "\u001b[32m[2025-10-28 08:31:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.005603359080851078 val_loss:0.006931359879672527 quant_lr:1.5e-06 norm:0.00532634 max memory_allocated 7476.923828125 time 25.673928022384644 \n",
      "\u001b[32m[2025-10-28 08:31:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:31:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:31:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.0070106578059494495 val_loss:0.008481133729219437 quant_lr:1.540866429118164e-05 norm:0.00906535 max memory_allocated 7476.923828125 time 25.58427405357361 \n",
      "\u001b[32m[2025-10-28 08:32:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.006787677761167288 val_loss:0.008441026322543621 quant_lr:1.5e-06 norm:0.00712229 max memory_allocated 7476.923828125 time 25.68506407737732 \n",
      "\u001b[32m[2025-10-28 08:32:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:32:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:33:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.008852408267557621 val_loss:0.010718334466218948 quant_lr:1.540866429118164e-05 norm:0.01080934 max memory_allocated 7476.923828125 time 25.59510588645935 \n",
      "\u001b[32m[2025-10-28 08:33:36 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.008540988899767399 val_loss:0.010654963552951813 quant_lr:1.5e-06 norm:0.00847987 max memory_allocated 7476.923828125 time 25.705085515975952 \n",
      "\u001b[32m[2025-10-28 08:33:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:33:54 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:34:25 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.011670044623315334 val_loss:0.014174737967550755 quant_lr:1.540866429118164e-05 norm:0.01844213 max memory_allocated 7476.923828125 time 25.635626554489136 \n",
      "\u001b[32m[2025-10-28 08:34:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.011225619353353977 val_loss:0.01407629530876875 quant_lr:1.5e-06 norm:0.01419005 max memory_allocated 7476.923828125 time 25.702056407928467 \n",
      "\u001b[32m[2025-10-28 08:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:34:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:34:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:34:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:35:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:35:05 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:35:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:35:09 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:35:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.015100164338946342 val_loss:0.018285539001226425 quant_lr:1.540866429118164e-05 norm:0.03020092 max memory_allocated 7476.923828125 time 25.62277054786682 \n",
      "\u001b[32m[2025-10-28 08:36:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.014443651773035526 val_loss:0.018117215484380722 quant_lr:1.5e-06 norm:0.02204430 max memory_allocated 7476.923828125 time 25.703186750411987 \n",
      "\u001b[32m[2025-10-28 08:36:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:36:25 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 08:36:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.021734926849603653 val_loss:0.02641361951828003 quant_lr:1.540866429118164e-05 norm:0.07223671 max memory_allocated 7476.923828125 time 25.623539209365845 \n",
      "\u001b[32m[2025-10-28 08:37:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.02066369354724884 val_loss:0.02611415833234787 quant_lr:1.5e-06 norm:0.05294119 max memory_allocated 7476.923828125 time 25.717583179473877 \n",
      "\u001b[32m[2025-10-28 08:37:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 08:37:40 root]\u001b[0m\u001b[33m(main_block_ap.py 166)\u001b[0m: INFO 2386.1359062194824\n",
      "\u001b[32m[2025-10-28 08:37:40 root]\u001b[0m\u001b[33m(main_block_ap.py 169)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-28 08:37:51 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 163/163 [02:44<00:00,  1.01s/it]\n",
      "wikitext2:6.384588718414307\n",
      "\u001b[32m[2025-10-28 08:40:49 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 6.38\n",
      "2025-10-28 08:40:49.825770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761640849.846632   61464 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761640849.852782   61464 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761640849.874385   61464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761640849.874417   61464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761640849.874420   61464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761640849.874424   61464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 08:40:54 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 08:40:55 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 08:40:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 123896.84it/s]\n",
      "\u001b[32m[2025-10-28 08:41:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2398.70it/s]\n",
      "\u001b[32m[2025-10-28 08:41:31 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1056.50it/s]\n",
      "\u001b[32m[2025-10-28 08:41:34 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1012.32it/s]\n",
      "\u001b[32m[2025-10-28 08:41:36 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:32<00:00, 24.80it/s]\n",
      "\u001b[32m[2025-10-28 09:19:50 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6803|±  |0.0131|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6335|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8139|±  |0.0039|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8081|±  |0.0081|\n",
      "|          |       |none  |     0|acc_norm|0.7609|±  |0.0088|\n",
      "|piqa      |      1|none  |     0|acc     |0.7976|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8003|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-28 09:19:50 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 72.99%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 3 \\\n",
    "       --group_size 128 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --output_dir ./output/mistral_baseline \\\n",
    "       --save_quant_dir ./output/mistral_baseline/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd3643",
   "metadata": {},
   "source": [
    "# SGRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qKirQXcV2EJe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4436762,
     "status": "ok",
     "timestamp": 1761683874658,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "qKirQXcV2EJe",
    "outputId": "fc3cda74-27a8-4046-cdf9-eeb33733016a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_adaptive_training', '--wbits', '3', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_sgra_2', '--save_quant_dir', './output/mistral_sgra_2/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_sgra_2', save_quant_dir='./output/mistral_sgra_2/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=3, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 19:10:33 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.30s/it]\n",
      "\u001b[32m[2025-10-28 19:10:38 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 19:10:38 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 19:10:38 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 19:10:38 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 19:10:39 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 2.00e-05-3.00e-05\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:10:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.33e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 19:11:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.76e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 19:11:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.55e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 19:12:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.16e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 19:12:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 19:12:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:12:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 2.00e-05, Patience: 5\n",
      "\u001b[32m[2025-10-28 19:12:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000086 | Val Loss: 0.000068 | Grad Norm: 0.01 | LR: 1.71e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:13:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000055 | Val Loss: 0.000040 | Grad Norm: 0.01 | LR: 1.04e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:13:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000036 | Val Loss: 0.000027 | Grad Norm: 0.01 | LR: 3.70e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:14:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000028 | Val Loss: 0.000026 | Grad Norm: 0.01 | LR: 1.00e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:14:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 19:14:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:14:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.55e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 19:15:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000036 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 1.93e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:15:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000033 | Val Loss: 0.000030 | Grad Norm: 0.00 | LR: 7.17e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:15:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000033 | Val Loss: 0.000030 | Grad Norm: 0.00 | LR: 1.28e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:16:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 19:16:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:16:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.44e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 19:16:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000039 | Val Loss: 0.000036 | Grad Norm: 0.00 | LR: 1.85e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:17:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000037 | Val Loss: 0.000035 | Grad Norm: 0.00 | LR: 6.86e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:17:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000036 | Val Loss: 0.000034 | Grad Norm: 0.00 | LR: 1.22e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:17:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 19:17:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:17:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.62e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:18:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000045 | Val Loss: 0.000042 | Grad Norm: 0.00 | LR: 1.98e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:18:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000043 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 7.36e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:19:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000042 | Val Loss: 0.000041 | Grad Norm: 0.00 | LR: 1.31e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:19:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 19:19:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:19:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.66e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:20:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000056 | Val Loss: 0.000053 | Grad Norm: 0.00 | LR: 2.01e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:20:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000053 | Val Loss: 0.000052 | Grad Norm: 0.00 | LR: 7.47e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:21:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000052 | Val Loss: 0.000052 | Grad Norm: 0.00 | LR: 1.33e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 19:21:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 19:21:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:21:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.70e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:21:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000072 | Val Loss: 0.000070 | Grad Norm: 0.00 | LR: 1.39e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:22:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000068 | Val Loss: 0.000069 | Grad Norm: 0.00 | LR: 1.35e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:22:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 19:22:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:22:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:23:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000095 | Val Loss: 0.000093 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:23:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000089 | Val Loss: 0.000092 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:23:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 19:23:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:23:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:24:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000117 | Val Loss: 0.000118 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:24:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000110 | Val Loss: 0.000116 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:25:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 19:25:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:25:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:25:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000148 | Val Loss: 0.000152 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:26:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000139 | Val Loss: 0.000150 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:26:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 19:26:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:26:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.75e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:26:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000184 | Val Loss: 0.000191 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:27:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000173 | Val Loss: 0.000188 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:27:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 19:27:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:27:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:28:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000230 | Val Loss: 0.000242 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:28:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000217 | Val Loss: 0.000239 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:28:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 19:28:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:28:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:29:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000288 | Val Loss: 0.000308 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:29:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000271 | Val Loss: 0.000304 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:30:15 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 19:30:15 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:30:15 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.80e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:30:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000356 | Val Loss: 0.000384 | Grad Norm: 0.00 | LR: 1.44e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:31:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000337 | Val Loss: 0.000381 | Grad Norm: 0.00 | LR: 1.40e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:31:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 19:31:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:31:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.79e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:32:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000428 | Val Loss: 0.000464 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:32:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000402 | Val Loss: 0.000459 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:32:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 19:32:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:32:48 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:33:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000548 | Val Loss: 0.000602 | Grad Norm: 0.00 | LR: 1.46e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:33:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000515 | Val Loss: 0.000594 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:34:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 19:34:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:34:05 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.91e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:34:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000682 | Val Loss: 0.000755 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:35:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000639 | Val Loss: 0.000746 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:35:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 19:35:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:35:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:35:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000851 | Val Loss: 0.000949 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:36:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000800 | Val Loss: 0.000937 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:36:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 19:36:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:36:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.78e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:37:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.001086 | Val Loss: 0.001221 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:37:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.001018 | Val Loss: 0.001206 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:37:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 19:37:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:37:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.82e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:38:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001454 | Val Loss: 0.001669 | Grad Norm: 0.00 | LR: 1.45e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:38:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001363 | Val Loss: 0.001649 | Grad Norm: 0.00 | LR: 1.41e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:39:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 19:39:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:39:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:39:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001798 | Val Loss: 0.002107 | Grad Norm: 0.00 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:40:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001699 | Val Loss: 0.002088 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 19:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:40:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.002176 | Val Loss: 0.002591 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:41:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.002071 | Val Loss: 0.002571 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:41:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 19:41:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:41:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:42:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002610 | Val Loss: 0.003158 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:42:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002495 | Val Loss: 0.003137 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:43:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 19:43:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:43:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:43:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.003164 | Val Loss: 0.003862 | Grad Norm: 0.01 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:43:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.003031 | Val Loss: 0.003839 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:44:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 19:44:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:44:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.88e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:44:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.003807 | Val Loss: 0.004697 | Grad Norm: 0.01 | LR: 1.48e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:45:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.003655 | Val Loss: 0.004671 | Grad Norm: 0.00 | LR: 1.44e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:45:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 19:45:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:45:33 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:46:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.004571 | Val Loss: 0.005677 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:46:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.004393 | Val Loss: 0.005644 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:46:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 19:46:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:46:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.93e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:47:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.005500 | Val Loss: 0.006873 | Grad Norm: 0.01 | LR: 1.51e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:47:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.005294 | Val Loss: 0.006838 | Grad Norm: 0.01 | LR: 1.47e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 19:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:48:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:48:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.006648 | Val Loss: 0.008368 | Grad Norm: 0.01 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 19:49:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.006411 | Val Loss: 0.008324 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:49:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 19:49:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:49:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:49:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.008397 | Val Loss: 0.010574 | Grad Norm: 0.01 | LR: 1.47e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:50:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.008068 | Val Loss: 0.010510 | Grad Norm: 0.01 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:50:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 19:50:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:50:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:51:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.011068 | Val Loss: 0.013989 | Grad Norm: 0.02 | LR: 1.49e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.010606 | Val Loss: 0.013888 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:51:56 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 19:51:56 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:51:56 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.83e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:52:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.014319 | Val Loss: 0.018046 | Grad Norm: 0.03 | LR: 1.45e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:52:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.013652 | Val Loss: 0.017874 | Grad Norm: 0.02 | LR: 1.42e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:53:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 19:53:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 19:53:13 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 19:53:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.020540 | Val Loss: 0.025933 | Grad Norm: 0.07 | LR: 1.47e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 19:54:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.019461 | Val Loss: 0.025643 | Grad Norm: 0.06 | LR: 1.43e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 19:54:29 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_sgra_2/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 19:54:29 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_sgra_2/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 19:54:30 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2631.87s (43.86min)\n",
      "\u001b[32m[2025-10-28 19:54:30 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 19:54:41 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_sgra_2/model\n",
      "\u001b[32m[2025-10-28 19:54:41 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:50<00:00,  1.05s/it]\n",
      "wikitext2:6.380879878997803\n",
      "\u001b[32m[2025-10-28 19:57:44 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.38\n",
      "2025-10-28 19:57:44.682498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761681464.703715  150371 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761681464.710215  150371 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761681464.731737  150371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761681464.731762  150371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761681464.731765  150371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761681464.731768  150371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 19:57:49 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 19:57:50 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 19:57:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126416.80it/s]\n",
      "\u001b[32m[2025-10-28 19:58:21 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2413.79it/s]\n",
      "\u001b[32m[2025-10-28 19:58:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1054.47it/s]\n",
      "\u001b[32m[2025-10-28 19:58:29 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1017.79it/s]\n",
      "\u001b[32m[2025-10-28 19:58:31 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [38:40<00:00, 24.08it/s]\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6685|±  |0.0132|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6334|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8097|±  |0.0039|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8005|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7647|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.8014|±  |0.0093|\n",
      "|          |       |none  |     0|acc_norm|0.8009|±  |0.0093|\n",
      "\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 72.60%\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_sgra_2/results.json\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 20:37:51 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "       --use_adaptive_training \\\n",
    "       --wbits 3 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_sgra_2 \\\n",
    "       --save_quant_dir ./output/mistral_sgra_2/model \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74178199",
   "metadata": {},
   "source": [
    "# MPQ + SGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YvQ2hsUY3SDg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5252768,
     "status": "ok",
     "timestamp": 1761671968002,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "YvQ2hsUY3SDg",
    "outputId": "e816eef5-129f-44b4-e390-79c9e316d800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '3.0', '--real_quant', '--use_adaptive_training', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--output_dir', './output/mistral_mpq_3bit', '--save_quant_dir', './output/mistral_mpq_3bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq_3bit', save_quant_dir='./output/mistral_mpq_3bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=3.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 15:51:59 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.14s/it]\n",
      "\u001b[32m[2025-10-28 15:52:04 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 15:52:04 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 15:52:04 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 15:52:04 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 15:52:05 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 2.00e-05-3.00e-05\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 15:52:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.33e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 15:52:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.76e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 15:53:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.55e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:53:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.16e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:53:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 15:53:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-28 15:53:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 2.00e-05, Patience: 5\n",
      "\u001b[32m[2025-10-28 15:54:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000011 | Val Loss: 0.000012 | Grad Norm: 0.01 | LR: 1.71e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 15:54:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000004 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.04e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 15:55:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000003 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 3.70e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 15:55:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000003 | Val Loss: 0.000007 | Grad Norm: 0.00 | LR: 1.00e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 15:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 15:55:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.55e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 15:56:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000007 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 1.93e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:56:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000006 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 7.17e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 15:57:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000005 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 1.28e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:57:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 15:57:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 15:57:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.44e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 15:58:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000011 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 1.85e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:58:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000010 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 6.86e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 15:59:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000010 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 1.22e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 15:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 15:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 15:59:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.62e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 15:59:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000019 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 1.98e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 16:00:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000017 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 7.36e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:00:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000016 | Val Loss: 0.000021 | Grad Norm: 0.00 | LR: 1.31e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 16:01:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 16:01:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:01:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.66e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:01:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000030 | Val Loss: 0.000033 | Grad Norm: 0.00 | LR: 2.01e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 16:02:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000028 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 7.47e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:02:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000027 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 1.33e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 16:02:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 16:02:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:02:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.70e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:03:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000047 | Val Loss: 0.000049 | Grad Norm: 0.00 | LR: 1.39e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:03:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000043 | Val Loss: 0.000048 | Grad Norm: 0.00 | LR: 1.35e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:04:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 16:04:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:04:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:04:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000072 | Val Loss: 0.000073 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:05:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000065 | Val Loss: 0.000072 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:05:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 16:05:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:05:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:05:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000096 | Val Loss: 0.000098 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:06:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000087 | Val Loss: 0.000096 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 16:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:07:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000129 | Val Loss: 0.000133 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:07:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000118 | Val Loss: 0.000131 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:07:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 16:07:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:07:51 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.75e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:08:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000168 | Val Loss: 0.000173 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:08:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000154 | Val Loss: 0.000170 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:09:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 16:09:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:09:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:09:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000219 | Val Loss: 0.000226 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:10:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000202 | Val Loss: 0.000222 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 16:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:10:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:10:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000281 | Val Loss: 0.000292 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:11:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000260 | Val Loss: 0.000288 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:11:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 16:11:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:11:42 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.80e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:12:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000354 | Val Loss: 0.000371 | Grad Norm: 0.00 | LR: 1.44e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:12:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000331 | Val Loss: 0.000366 | Grad Norm: 0.00 | LR: 1.40e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:12:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 16:12:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:12:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.79e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:13:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000434 | Val Loss: 0.000455 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:13:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000403 | Val Loss: 0.000448 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:14:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 16:14:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:14:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:14:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000565 | Val Loss: 0.000595 | Grad Norm: 0.00 | LR: 1.46e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000524 | Val Loss: 0.000587 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:15:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 16:15:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:15:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.91e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:16:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000711 | Val Loss: 0.000755 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:16:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000660 | Val Loss: 0.000744 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:16:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 16:16:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:16:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:17:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000897 | Val Loss: 0.000956 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:17:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000836 | Val Loss: 0.000943 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:18:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 16:18:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:18:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.78e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:18:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.001156 | Val Loss: 0.001238 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:19:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.001073 | Val Loss: 0.001220 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:19:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 16:19:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:19:23 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.82e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:19:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001552 | Val Loss: 0.001691 | Grad Norm: 0.00 | LR: 1.45e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:20:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001444 | Val Loss: 0.001668 | Grad Norm: 0.00 | LR: 1.41e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:20:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 16:20:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:20:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:21:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.001922 | Val Loss: 0.002133 | Grad Norm: 0.00 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:21:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001807 | Val Loss: 0.002111 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 16:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:21:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:22:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.002327 | Val Loss: 0.002617 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:22:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.002203 | Val Loss: 0.002593 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:23:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 16:23:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:23:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:23:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002795 | Val Loss: 0.003194 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:24:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002661 | Val Loss: 0.003171 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:24:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 16:24:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:24:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:25:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.003395 | Val Loss: 0.003909 | Grad Norm: 0.01 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:25:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.003238 | Val Loss: 0.003884 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:25:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 16:25:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:25:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.88e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:26:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.004094 | Val Loss: 0.004763 | Grad Norm: 0.01 | LR: 1.48e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:26:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.003915 | Val Loss: 0.004732 | Grad Norm: 0.00 | LR: 1.44e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:27:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 16:27:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:27:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:27:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.004924 | Val Loss: 0.005770 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:28:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.004718 | Val Loss: 0.005734 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:28:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 16:28:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:28:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.93e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:28:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.005941 | Val Loss: 0.007007 | Grad Norm: 0.01 | LR: 1.51e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:29:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.005701 | Val Loss: 0.006961 | Grad Norm: 0.01 | LR: 1.47e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:29:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 16:29:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:29:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:30:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.007195 | Val Loss: 0.008552 | Grad Norm: 0.01 | LR: 1.49e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:30:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.006920 | Val Loss: 0.008496 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:30:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 16:30:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:30:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:31:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.009126 | Val Loss: 0.010844 | Grad Norm: 0.01 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 16:31:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.008739 | Val Loss: 0.010765 | Grad Norm: 0.01 | LR: 1.43e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 16:32:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 16:32:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:32:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:32:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.012063 | Val Loss: 0.014375 | Grad Norm: 0.02 | LR: 1.49e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:33:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.011520 | Val Loss: 0.014255 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 16:33:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 16:33:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:33:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.83e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:34:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.015698 | Val Loss: 0.018660 | Grad Norm: 0.03 | LR: 1.45e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:34:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.014904 | Val Loss: 0.018447 | Grad Norm: 0.02 | LR: 1.42e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 16:34:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 16:34:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 16:34:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 16:35:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.022707 | Val Loss: 0.026904 | Grad Norm: 0.08 | LR: 1.47e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 16:35:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.021350 | Val Loss: 0.026530 | Grad Norm: 0.06 | LR: 1.43e-06 | Time: 26.9s\n",
      "\u001b[32m[2025-10-28 16:36:02 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq_3bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 16:36:02 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq_3bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 16:36:02 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2638.16s (43.97min)\n",
      "\u001b[32m[2025-10-28 16:36:02 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 16:36:14 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq_3bit/model\n",
      "\u001b[32m[2025-10-28 16:36:14 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:51<00:00,  1.05s/it]\n",
      "wikitext2:6.417377471923828\n",
      "\u001b[32m[2025-10-28 16:39:18 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.42\n",
      "2025-10-28 16:39:18.971950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761669558.992638   99762 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761669558.999021   99762 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761669559.020318   99762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761669559.020344   99762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761669559.020353   99762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761669559.020361   99762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 16:39:23 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 16:39:24 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 16:39:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 125328.60it/s]\n",
      "\u001b[32m[2025-10-28 16:39:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2433.85it/s]\n",
      "\u001b[32m[2025-10-28 16:40:00 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1052.32it/s]\n",
      "\u001b[32m[2025-10-28 16:40:03 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1018.43it/s]\n",
      "\u001b[32m[2025-10-28 16:40:05 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [38:38<00:00, 24.10it/s]\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7159|±  |0.0127|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6328|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8109|±  |0.0039|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8035|±  |0.0082|\n",
      "|          |       |none  |     0|acc_norm|0.7555|±  |0.0088|\n",
      "|piqa      |      1|none  |     0|acc     |0.7987|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8074|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 73.77%\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq_3bit/results.json\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 17:19:25 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python main_research.py \\\n",
    "    --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "    --sensitivity_file \"./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\" \\\n",
    "    --use_mixed_precision \\\n",
    "    --mpq_strategy adaptive \\\n",
    "    --target_avg_bits 3.0 \\\n",
    "    --real_quant \\\n",
    "    --use_adaptive_training \\\n",
    "    --train_size 128 \\\n",
    "    --val_size 16 \\\n",
    "    --quant_lr 3e-5 \\\n",
    "    --weight_lr 2e-6 \\\n",
    "    --output_dir \"./output/mistral_mpq_3bit\" \\\n",
    "    --save_quant_dir \"./output/mistral_mpq_3bit/model\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f759cb",
   "metadata": {},
   "source": [
    "# MPQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d774a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5112513,
     "status": "ok",
     "timestamp": 1761585082874,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "12d774a8",
    "outputId": "cac9c607-b22c-4ce3-ff11-ad8cf12cd580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '3.0', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_mpq_3bit', '--save_quant_dir', './output/mistral_mpq/model_3bit', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 3.0\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq_3bit', save_quant_dir='./output/mistral_mpq/model_3bit', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=3.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-27 15:46:17 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100% 596/596 [00:00<00:00, 3.79MB/s]\n",
      "tokenizer_config.json: 2.10kB [00:00, 4.48MB/s]\n",
      "tokenizer.model: 100% 493k/493k [00:01<00:00, 412kB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.19MB/s]\n",
      "tokenizer.json: 1.80MB [00:00, 13.9MB/s]\n",
      "model.safetensors.index.json: 25.1kB [00:00, 80.8MB/s]\n",
      "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
      "model-00001-of-00003.safetensors:   0% 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   0% 711k/4.94G [00:01<2:10:24, 632kB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   2% 99.2M/4.94G [00:01<01:03, 75.7MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   8% 411M/4.94G [00:02<00:20, 219MB/s]  \u001b[A\n",
      "model-00001-of-00003.safetensors:  12% 606M/4.94G [00:05<00:35, 123MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  16% 809M/4.94G [00:05<00:21, 189MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  19% 943M/4.94G [00:05<00:16, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  22% 1.08G/4.94G [00:05<00:13, 287MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  25% 1.21G/4.94G [00:05<00:10, 363MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  27% 1.33G/4.94G [00:09<00:35, 102MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  31% 1.53G/4.94G [00:09<00:21, 161MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  34% 1.66G/4.94G [00:09<00:16, 205MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  36% 1.79G/4.94G [00:09<00:12, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  39% 1.93G/4.94G [00:10<00:13, 231MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  40% 2.00G/4.94G [00:10<00:14, 210MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  42% 2.06G/4.94G [00:11<00:14, 195MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  43% 2.13G/4.94G [00:13<00:29, 96.1MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48% 2.35G/4.94G [00:13<00:14, 183MB/s] \u001b[A\n",
      "model-00001-of-00003.safetensors:  50% 2.48G/4.94G [00:13<00:10, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  53% 2.62G/4.94G [00:14<00:11, 202MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  54% 2.69G/4.94G [00:14<00:10, 216MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  56% 2.75G/4.94G [00:14<00:08, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  57% 2.82G/4.94G [00:17<00:25, 84.5MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  61% 3.02G/4.94G [00:17<00:12, 158MB/s] \u001b[A\n",
      "model-00001-of-00003.safetensors:  64% 3.15G/4.94G [00:17<00:08, 220MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  67% 3.29G/4.94G [00:17<00:06, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69% 3.40G/4.94G [00:18<00:06, 248MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  70% 3.47G/4.94G [00:18<00:05, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  72% 3.54G/4.94G [00:19<00:06, 207MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73% 3.60G/4.94G [00:21<00:15, 85.1MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  77% 3.81G/4.94G [00:21<00:06, 163MB/s] \u001b[A\n",
      "model-00001-of-00003.safetensors:  80% 3.94G/4.94G [00:21<00:04, 226MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  82% 4.07G/4.94G [00:22<00:03, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85% 4.21G/4.94G [00:22<00:03, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  86% 4.27G/4.94G [00:23<00:02, 242MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  88% 4.34G/4.94G [00:23<00:02, 261MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  89% 4.41G/4.94G [00:23<00:01, 279MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  91% 4.47G/4.94G [00:23<00:01, 311MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  92% 4.54G/4.94G [00:25<00:04, 95.2MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  96% 4.74G/4.94G [00:25<00:01, 191MB/s] \u001b[A\n",
      "model-00001-of-00003.safetensors: 100% 4.94G/4.94G [00:25<00:00, 191MB/s]\n",
      "Downloading shards:  33% 1/3 [00:26<00:52, 26.33s/it]\n",
      "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   0% 754k/5.00G [00:02<4:01:17, 345kB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   5% 236M/5.00G [00:02<00:47, 101MB/s]  \u001b[A\n",
      "model-00002-of-00003.safetensors:   8% 412M/5.00G [00:03<00:23, 196MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  11% 564M/5.00G [00:05<00:40, 109MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  16% 781M/5.00G [00:05<00:22, 186MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  19% 928M/5.00G [00:05<00:16, 250MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  21% 1.05G/5.00G [00:05<00:12, 305MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  23% 1.15G/5.00G [00:11<01:01, 62.8MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  24% 1.22G/5.00G [00:11<00:50, 74.8MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  26% 1.29G/5.00G [00:11<00:40, 90.9MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  27% 1.35G/5.00G [00:12<00:34, 106MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:  28% 1.42G/5.00G [00:12<00:27, 131MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  30% 1.49G/5.00G [00:12<00:29, 121MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  34% 1.72G/5.00G [00:13<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  36% 1.81G/5.00G [00:13<00:11, 277MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  38% 1.92G/5.00G [00:13<00:10, 303MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  40% 1.99G/5.00G [00:16<00:32, 92.1MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  44% 2.20G/5.00G [00:16<00:16, 168MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:  48% 2.39G/5.00G [00:16<00:10, 258MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  51% 2.56G/5.00G [00:16<00:06, 355MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  55% 2.76G/5.00G [00:16<00:04, 454MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  58% 2.91G/5.00G [00:17<00:04, 484MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  61% 3.05G/5.00G [00:17<00:03, 551MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64% 3.18G/5.00G [00:17<00:03, 558MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  66% 3.30G/5.00G [00:19<00:10, 162MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  68% 3.39G/5.00G [00:19<00:08, 197MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  69% 3.47G/5.00G [00:23<00:23, 65.9MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73% 3.64G/5.00G [00:24<00:12, 108MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:  76% 3.79G/5.00G [00:24<00:07, 153MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  78% 3.91G/5.00G [00:24<00:06, 180MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  80% 3.99G/5.00G [00:24<00:04, 210MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  81% 4.06G/5.00G [00:24<00:03, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  84% 4.19G/5.00G [00:24<00:02, 323MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87% 4.33G/5.00G [00:25<00:01, 422MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  88% 4.42G/5.00G [00:25<00:01, 476MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91% 4.54G/5.00G [00:25<00:01, 444MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92% 4.62G/5.00G [00:25<00:00, 428MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  95% 4.73G/5.00G [00:26<00:00, 383MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  96% 4.80G/5.00G [00:26<00:00, 408MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  97% 4.87G/5.00G [00:26<00:00, 368MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  99% 4.93G/5.00G [00:26<00:00, 358MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:26<00:00, 186MB/s]\n",
      "Downloading shards:  67% 2/3 [00:53<00:26, 26.94s/it]\n",
      "model-00003-of-00003.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   0% 816k/4.54G [00:01<1:46:44, 709kB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   3% 135M/4.54G [00:02<01:04, 68.5MB/s] \u001b[A\n",
      "model-00003-of-00003.safetensors:   6% 269M/4.54G [00:04<01:10, 60.8MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   7% 336M/4.54G [00:05<01:01, 68.1MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  21% 941M/4.54G [00:05<00:11, 308MB/s] \u001b[A\n",
      "model-00003-of-00003.safetensors:  25% 1.14G/4.54G [00:08<00:23, 147MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30% 1.35G/4.54G [00:08<00:16, 198MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  33% 1.48G/4.54G [00:09<00:13, 221MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36% 1.61G/4.54G [00:09<00:13, 218MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39% 1.75G/4.54G [00:10<00:13, 210MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40% 1.82G/4.54G [00:11<00:16, 161MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41% 1.88G/4.54G [00:12<00:22, 117MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  46% 2.08G/4.54G [00:13<00:12, 196MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  49% 2.22G/4.54G [00:13<00:09, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52% 2.35G/4.54G [00:13<00:09, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53% 2.42G/4.54G [00:14<00:08, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55% 2.48G/4.54G [00:14<00:08, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56% 2.55G/4.54G [00:14<00:10, 197MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  58% 2.62G/4.54G [00:17<00:22, 86.4MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62% 2.82G/4.54G [00:17<00:10, 167MB/s] \u001b[A\n",
      "model-00003-of-00003.safetensors:  66% 3.00G/4.54G [00:17<00:06, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69% 3.13G/4.54G [00:17<00:05, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  71% 3.20G/4.54G [00:18<00:06, 218MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  72% 3.27G/4.54G [00:18<00:05, 229MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73% 3.34G/4.54G [00:19<00:05, 213MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75% 3.40G/4.54G [00:21<00:12, 88.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79% 3.60G/4.54G [00:21<00:05, 171MB/s] \u001b[A\n",
      "model-00003-of-00003.safetensors:  82% 3.74G/4.54G [00:21<00:03, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85% 3.87G/4.54G [00:21<00:02, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88% 4.01G/4.54G [00:22<00:01, 280MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  90% 4.07G/4.54G [00:22<00:01, 285MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91% 4.14G/4.54G [00:22<00:01, 294MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93% 4.21G/4.54G [00:22<00:00, 337MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94% 4.27G/4.54G [00:22<00:00, 356MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96% 4.34G/4.54G [00:23<00:00, 367MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  97% 4.41G/4.54G [00:23<00:00, 356MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  99% 4.47G/4.54G [00:23<00:00, 318MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100% 4.54G/4.54G [00:23<00:00, 191MB/s]\n",
      "Downloading shards: 100% 3/3 [01:17<00:00, 25.98s/it]\n",
      "Loading checkpoint shards: 100% 3/3 [00:14<00:00,  4.69s/it]\n",
      "generation_config.json: 100% 111/111 [00:00<00:00, 880kB/s]\n",
      "\u001b[32m[2025-10-27 15:47:57 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "Downloading readme: 10.5kB [00:00, 27.0MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 1.39MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:00<00:00, 9.62MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 1.35MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 76232.08 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 683816.67 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 603982.35 examples/s]\n",
      "Downloading readme: 10.5kB [00:00, 40.5MB/s]\n",
      "\u001b[32m[2025-10-27 15:48:39 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-27 15:48:40 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 3.0 bits\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 3.03 bits\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.28x vs FP16\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-27 15:48:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-27 15:49:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 25.9s\n",
      "\u001b[32m[2025-10-27 15:49:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 25.7s\n",
      "\u001b[32m[2025-10-27 15:49:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-27 15:49:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 64\n",
      "\u001b[32m[2025-10-27 15:50:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000009 | Val Loss: 0.000013 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:50:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000004 | Val Loss: 0.000008 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 15:51:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-27 15:51:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000007 | Val Loss: 0.000011 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:52:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000006 | Val Loss: 0.000011 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 15:52:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-27 15:52:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:52:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000012 | Val Loss: 0.000016 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:53:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000010 | Val Loss: 0.000015 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:53:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-27 15:53:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:54:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000019 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:54:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000018 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 15:54:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-27 15:54:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:55:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000032 | Val Loss: 0.000035 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:55:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000029 | Val Loss: 0.000035 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 15:56:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-27 15:56:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:56:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000050 | Val Loss: 0.000052 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:57:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000045 | Val Loss: 0.000051 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:57:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-27 15:57:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:57:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000075 | Val Loss: 0.000077 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:58:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000068 | Val Loss: 0.000075 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 15:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-27 15:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 15:59:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000100 | Val Loss: 0.000102 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 15:59:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000092 | Val Loss: 0.000101 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 15:59:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-27 15:59:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:00:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000134 | Val Loss: 0.000138 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:00:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000124 | Val Loss: 0.000136 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:01:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-27 16:01:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:01:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000174 | Val Loss: 0.000179 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000161 | Val Loss: 0.000176 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:02:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-27 16:02:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000227 | Val Loss: 0.000234 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:03:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000211 | Val Loss: 0.000231 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:03:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-27 16:03:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000292 | Val Loss: 0.000302 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:04:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000272 | Val Loss: 0.000298 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:04:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-27 16:04:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:05:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000369 | Val Loss: 0.000384 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:05:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000346 | Val Loss: 0.000380 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:06:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-27 16:06:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:06:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000452 | Val Loss: 0.000470 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:07:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000422 | Val Loss: 0.000464 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:07:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-27 16:07:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:07:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000588 | Val Loss: 0.000614 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 16:08:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000550 | Val Loss: 0.000607 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:08:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-27 16:08:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:09:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000743 | Val Loss: 0.000778 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:09:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000695 | Val Loss: 0.000769 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:09:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-27 16:09:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:10:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000937 | Val Loss: 0.000986 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:10:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000880 | Val Loss: 0.000975 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:11:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-27 16:11:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:11:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.001209 | Val Loss: 0.001277 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:12:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.001131 | Val Loss: 0.001260 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:12:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-27 16:12:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:13:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.001625 | Val Loss: 0.001739 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:13:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.001522 | Val Loss: 0.001719 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:13:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-27 16:13:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:14:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.002017 | Val Loss: 0.002197 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:14:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.001908 | Val Loss: 0.002176 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:15:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-27 16:15:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:15:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.002447 | Val Loss: 0.002692 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:15:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.002330 | Val Loss: 0.002671 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:16:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-27 16:16:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:16:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.002944 | Val Loss: 0.003285 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:17:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.002819 | Val Loss: 0.003264 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:17:34 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-27 16:17:34 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:18:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.003580 | Val Loss: 0.004018 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:18:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.003434 | Val Loss: 0.003995 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:18:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-27 16:18:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:19:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.004323 | Val Loss: 0.004894 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:19:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.004154 | Val Loss: 0.004865 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:20:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-27 16:20:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:20:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.005208 | Val Loss: 0.005926 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:21:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.005015 | Val Loss: 0.005893 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:21:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-27 16:21:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:21:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.006292 | Val Loss: 0.007197 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:22:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.006065 | Val Loss: 0.007156 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:22:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-27 16:22:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:23:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.007625 | Val Loss: 0.008779 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:23:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.007366 | Val Loss: 0.008732 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:23:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-27 16:23:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:24:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.009669 | Val Loss: 0.011129 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:24:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.009301 | Val Loss: 0.011058 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:25:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-27 16:25:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:25:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.012782 | Val Loss: 0.014745 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:26:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.012261 | Val Loss: 0.014631 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:26:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-27 16:26:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:26:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.016639 | Val Loss: 0.019116 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 16:27:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.015853 | Val Loss: 0.018916 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-27 16:27:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-27 16:27:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 16:28:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.024130 | Val Loss: 0.027594 | Grad Norm: 0.09 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:28:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.022733 | Val Loss: 0.027246 | Grad Norm: 0.05 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-27 16:28:59 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq_3bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-27 16:28:59 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq/model_3bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-27 16:28:59 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2462.10s (41.04min)\n",
      "\u001b[32m[2025-10-27 16:28:59 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-27 16:29:10 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq/model_3bit\n",
      "\u001b[32m[2025-10-27 16:29:10 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:47<00:00,  1.03s/it]\n",
      "wikitext2:6.427535533905029\n",
      "\u001b[32m[2025-10-27 16:32:08 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.43\n",
      "2025-10-27 16:32:10.295819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761582730.602743    3289 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761582730.684208    3289 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761582731.325074    3289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761582731.325114    3289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761582731.325129    3289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761582731.325136    3289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 17.0MB/s]\n",
      "\u001b[32m[2025-10-27 16:32:18 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-27 16:32:19 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-27 16:32:23 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 21.6MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 21.4MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:01<00:00, 1.65MB/s]\n",
      "Downloading data: 815kB [00:00, 78.5MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 34467.42 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37756.30 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 34844.00 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 27.6MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.24MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.39MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 302kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 373767.95 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 431414.13 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 247797.81 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 25.0MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 34.6MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 11.5MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 12.0MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 250926.81 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 245455.00 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 247789.16 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 6190.67 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6518.41 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 35.1MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 4.06MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 247kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 170kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1108127.06 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 626486.49 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 469243.55 examples/s]\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 130112.46it/s]\n",
      "\u001b[32m[2025-10-27 16:33:18 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2415.58it/s]\n",
      "\u001b[32m[2025-10-27 16:33:23 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1050.01it/s]\n",
      "\u001b[32m[2025-10-27 16:33:26 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1006.29it/s]\n",
      "\u001b[32m[2025-10-27 16:33:28 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:11<00:00, 25.04it/s]\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7103|±  |0.0127|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6323|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8105|±  |0.0039|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8056|±  |0.0081|\n",
      "|          |       |none  |     0|acc_norm|0.7609|±  |0.0088|\n",
      "|piqa      |      1|none  |     0|acc     |0.7949|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8052|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 73.58%\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq_3bit/results.json\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-27 17:11:21 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "       --use_mixed_precision \\\n",
    "       --mpq_strategy adaptive \\\n",
    "       --target_avg_bits 3.0 \\\n",
    "       --train_size 128 --val_size 16 \\\n",
    "       --quant_lr 3e-5 --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_mpq_3bit \\\n",
    "       --save_quant_dir ./output/mistral_mpq/model_3bit \\\n",
    "       --eval_ppl --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b01950",
   "metadata": {},
   "source": [
    "# 2 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a00086",
   "metadata": {},
   "source": [
    "##  2.3 bit combined  methord "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a7780",
   "metadata": {
    "id": "9c0a7780",
    "outputId": "14c789f0-c6c3-48e1-e598-9d136011b538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2.json', '--use_mixed_precision', '--mpq_strategy', 'aggressive', '--target_avg_bits', '2.5', '--real_quant', '--use_adaptive_training', '--epochs', '3', '--train_size', '256', '--val_size', '32', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--output_dir', './output/mistral_mpq_2.5bit', '--save_quant_dir', './output/mistral_mpq_2.5bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: aggressive\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.5\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq_2.5bit', save_quant_dir='./output/mistral_mpq_2.5bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=256, val_size=32, training_seqlen=2048, batch_size=2, epochs=3, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2.json', use_mixed_precision=True, mpq_strategy='aggressive', target_avg_bits=2.5, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-12 12:11:41 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:08<00:00,  2.86s/it]\n",
      "\u001b[32m[2025-10-12 12:11:50 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "get_wikitext2\n",
      "\u001b[32m[2025-10-12 12:12:07 root]\u001b[0m\u001b[33m(block_ap_research.py 393)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-12 12:12:08 root]\u001b[0m\u001b[33m(block_ap_research.py 478)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 514)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2.json\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 518)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 519)\u001b[0m: INFO [RESEARCH] Sensitivity range: 1.0000 to 1.0000\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 529)\u001b[0m: INFO   Strategy: aggressive, Target Avg: 2.5 bits\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 530)\u001b[0m: INFO   Bit-widths: [4, 8, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 531)\u001b[0m: INFO   Actual Avg: 2.31 bits\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 532)\u001b[0m: INFO   Compression: 6.92x vs FP16\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Epoch range: 3-5\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   LR range: 6.79e-05-1.00e-04\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 12:12:09 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 5, LR: 7.60e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 12:12:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/5 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.90e-05 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:13:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/5 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:14:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/5 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 2.86e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:14:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 3/5 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.06e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:15:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 4/5 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:15:44 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-12 12:15:44 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 8, Group: 64\n",
      "\u001b[32m[2025-10-12 12:15:44 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 5, LR: 6.79e-05, Patience: 4\n",
      "\u001b[32m[2025-10-12 12:16:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/5 | Train Loss: 0.000022 | Val Loss: 0.000006 | Grad Norm: 0.04 | LR: 6.16e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:17:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/5 | Train Loss: 0.000008 | Val Loss: 0.000005 | Grad Norm: 0.02 | LR: 4.54e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:17:50 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/5 | Train Loss: 0.000005 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 2.55e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:18:30 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/5 | Train Loss: 0.000004 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 9.46e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:19:10 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 4/5 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 3.39e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:19:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-12 12:19:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:19:22 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:20:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/4 | Train Loss: 0.000013 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/4 | Train Loss: 0.000011 | Val Loss: 0.000011 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:21:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/4 | Train Loss: 0.000010 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:22:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 3/4 | Train Loss: 0.000010 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.7s\n",
      "\u001b[32m[2025-10-12 12:22:18 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-12 12:22:18 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 4, Group: 128\n",
      "\u001b[32m[2025-10-12 12:22:18 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 5, LR: 7.60e-05, Patience: 3\n",
      "\u001b[32m[2025-10-12 12:23:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/5 | Train Loss: 0.000013 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 6.90e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:23:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/5 | Train Loss: 0.000013 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 5.09e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:24:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/5 | Train Loss: 0.000012 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 2.86e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 3/5 | Train Loss: 0.000012 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 1.06e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:25:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 4/5 | Train Loss: 0.000012 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 3.80e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-12 12:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:26:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/4 | Train Loss: 0.000034 | Val Loss: 0.000034 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:27:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/4 | Train Loss: 0.000031 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:28:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/4 | Train Loss: 0.000030 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:28:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 3/4 | Train Loss: 0.000029 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-12 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:29:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/4 | Train Loss: 0.000066 | Val Loss: 0.000067 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:30:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/4 | Train Loss: 0.000060 | Val Loss: 0.000064 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:31:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/4 | Train Loss: 0.000058 | Val Loss: 0.000062 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:31:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 3/4 | Train Loss: 0.000056 | Val Loss: 0.000061 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:31:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-12 12:31:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:31:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:32:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/4 | Train Loss: 0.000106 | Val Loss: 0.000111 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:33:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/4 | Train Loss: 0.000097 | Val Loss: 0.000106 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:34:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 2/4 | Train Loss: 0.000093 | Val Loss: 0.000103 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:34:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 3/4 | Train Loss: 0.000091 | Val Loss: 0.000102 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:34:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-12 12:34:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:34:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:35:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/4 | Train Loss: 0.000165 | Val Loss: 0.000175 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/4 | Train Loss: 0.000151 | Val Loss: 0.000167 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:37:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 2/4 | Train Loss: 0.000144 | Val Loss: 0.000163 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:37:41 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 3/4 | Train Loss: 0.000141 | Val Loss: 0.000162 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-12 12:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:38:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/3 | Train Loss: 0.000218 | Val Loss: 0.000238 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:39:21 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/3 | Train Loss: 0.000201 | Val Loss: 0.000228 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:40:01 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 2/3 | Train Loss: 0.000195 | Val Loss: 0.000226 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-12 12:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:40:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:41:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/4 | Train Loss: 0.000297 | Val Loss: 0.000328 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:41:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/4 | Train Loss: 0.000276 | Val Loss: 0.000316 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:42:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 2/4 | Train Loss: 0.000266 | Val Loss: 0.000310 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:43:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 3/4 | Train Loss: 0.000261 | Val Loss: 0.000308 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:43:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-12 12:43:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:43:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 8.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/4 | Train Loss: 0.000381 | Val Loss: 0.000428 | Grad Norm: 0.00 | LR: 7.42e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:44:42 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/4 | Train Loss: 0.000355 | Val Loss: 0.000413 | Grad Norm: 0.00 | LR: 4.51e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:45:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 2/4 | Train Loss: 0.000343 | Val Loss: 0.000406 | Grad Norm: 0.00 | LR: 1.62e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:46:02 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 3/4 | Train Loss: 0.000337 | Val Loss: 0.000404 | Grad Norm: 0.00 | LR: 4.32e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:46:15 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-12 12:46:15 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:46:15 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:47:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/3 | Train Loss: 0.000488 | Val Loss: 0.000554 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:47:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/3 | Train Loss: 0.000457 | Val Loss: 0.000537 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:48:22 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 2/3 | Train Loss: 0.000445 | Val Loss: 0.000531 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:48:36 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-12 12:48:36 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:48:36 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:49:23 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/3 | Train Loss: 0.000629 | Val Loss: 0.000718 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.8s\n",
      "\u001b[32m[2025-10-12 12:50:03 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/3 | Train Loss: 0.000589 | Val Loss: 0.000696 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:50:43 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 2/3 | Train Loss: 0.000573 | Val Loss: 0.000689 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:50:56 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-12 12:50:56 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:50:56 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:51:44 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/3 | Train Loss: 0.000796 | Val Loss: 0.000915 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:52:24 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/3 | Train Loss: 0.000750 | Val Loss: 0.000891 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:53:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 2/3 | Train Loss: 0.000731 | Val Loss: 0.000883 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-12 12:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:53:17 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:54:04 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/3 | Train Loss: 0.000973 | Val Loss: 0.001119 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:54:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/3 | Train Loss: 0.000912 | Val Loss: 0.001088 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 12:55:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 2/3 | Train Loss: 0.000887 | Val Loss: 0.001077 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 12:55:38 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-12 12:55:38 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:55:38 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:56:25 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/3 | Train Loss: 0.001271 | Val Loss: 0.001476 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:57:05 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/3 | Train Loss: 0.001188 | Val Loss: 0.001434 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 2/3 | Train Loss: 0.001154 | Val Loss: 0.001421 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 12:57:59 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-12 12:57:59 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 12:57:59 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 12:58:46 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/3 | Train Loss: 0.001621 | Val Loss: 0.001894 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 12:59:26 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/3 | Train Loss: 0.001513 | Val Loss: 0.001841 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:00:06 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 2/3 | Train Loss: 0.001467 | Val Loss: 0.001824 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-12 13:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:01:07 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/3 | Train Loss: 0.002050 | Val Loss: 0.002406 | Grad Norm: 0.00 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:01:47 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/3 | Train Loss: 0.001915 | Val Loss: 0.002338 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:02:27 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 2/3 | Train Loss: 0.001859 | Val Loss: 0.002315 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:02:41 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-12 13:02:41 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:02:41 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:03:28 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/3 | Train Loss: 0.002670 | Val Loss: 0.003170 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:04:08 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/3 | Train Loss: 0.002485 | Val Loss: 0.003077 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:04:48 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 2/3 | Train Loss: 0.002409 | Val Loss: 0.003048 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:05:01 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-12 13:05:01 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:05:01 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:05:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/3 | Train Loss: 0.003657 | Val Loss: 0.004428 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:06:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/3 | Train Loss: 0.003403 | Val Loss: 0.004302 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:07:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 2/3 | Train Loss: 0.003297 | Val Loss: 0.004255 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:07:22 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-12 13:07:22 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:07:22 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:08:09 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/3 | Train Loss: 0.004582 | Val Loss: 0.005700 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:08:49 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/3 | Train Loss: 0.004290 | Val Loss: 0.005570 | Grad Norm: 0.00 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:09:29 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 2/3 | Train Loss: 0.004168 | Val Loss: 0.005530 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:09:44 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-12 13:09:44 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:09:44 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:10:31 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/3 | Train Loss: 0.005569 | Val Loss: 0.007102 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/3 | Train Loss: 0.005259 | Val Loss: 0.006956 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 2/3 | Train Loss: 0.005125 | Val Loss: 0.006916 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-12 13:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:12:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/3 | Train Loss: 0.006694 | Val Loss: 0.008679 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:13:33 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/3 | Train Loss: 0.006361 | Val Loss: 0.008537 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:14:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 2/3 | Train Loss: 0.006219 | Val Loss: 0.008499 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:14:26 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-12 13:14:26 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:14:26 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:15:13 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/3 | Train Loss: 0.008080 | Val Loss: 0.010732 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:15:53 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/3 | Train Loss: 0.007706 | Val Loss: 0.010571 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:16:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 2/3 | Train Loss: 0.007546 | Val Loss: 0.010523 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:16:47 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-12 13:16:47 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:16:47 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:17:34 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/3 | Train Loss: 0.009678 | Val Loss: 0.013104 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:18:14 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/3 | Train Loss: 0.009260 | Val Loss: 0.012934 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:18:54 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 2/3 | Train Loss: 0.009081 | Val Loss: 0.012886 | Grad Norm: 0.00 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:19:08 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-12 13:19:08 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:19:08 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:19:55 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/3 | Train Loss: 0.011550 | Val Loss: 0.015897 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 39.9s\n",
      "\u001b[32m[2025-10-12 13:20:35 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/3 | Train Loss: 0.011084 | Val Loss: 0.015703 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:21:15 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 2/3 | Train Loss: 0.010885 | Val Loss: 0.015646 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:21:29 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-12 13:21:29 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:21:29 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:22:16 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/3 | Train Loss: 0.013822 | Val Loss: 0.019189 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:22:56 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/3 | Train Loss: 0.013293 | Val Loss: 0.018973 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:23:36 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 2/3 | Train Loss: 0.013069 | Val Loss: 0.018913 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:23:50 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-12 13:23:50 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:23:50 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:24:37 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/3 | Train Loss: 0.016602 | Val Loss: 0.023192 | Grad Norm: 0.01 | LR: 7.59e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:25:17 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/3 | Train Loss: 0.016004 | Val Loss: 0.022942 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 2/3 | Train Loss: 0.015748 | Val Loss: 0.022871 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:26:11 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-12 13:26:11 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:26:11 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:26:58 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/3 | Train Loss: 0.020891 | Val Loss: 0.029068 | Grad Norm: 0.02 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:27:38 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/3 | Train Loss: 0.020072 | Val Loss: 0.028724 | Grad Norm: 0.01 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:28:18 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 2/3 | Train Loss: 0.019734 | Val Loss: 0.028623 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-12 13:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:28:32 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:29:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/3 | Train Loss: 0.027441 | Val Loss: 0.038316 | Grad Norm: 0.03 | LR: 7.59e-05 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:29:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/3 | Train Loss: 0.026277 | Val Loss: 0.037777 | Grad Norm: 0.02 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:30:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 2/3 | Train Loss: 0.025815 | Val Loss: 0.037649 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:30:53 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-12 13:30:53 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:30:53 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:31:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/3 | Train Loss: 0.035126 | Val Loss: 0.048672 | Grad Norm: 0.04 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:32:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/3 | Train Loss: 0.033568 | Val Loss: 0.048000 | Grad Norm: 0.03 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:32:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 2/3 | Train Loss: 0.032973 | Val Loss: 0.047761 | Grad Norm: 0.02 | LR: 5.00e-06 | Time: 40.1s\n",
      "\u001b[32m[2025-10-12 13:33:12 root]\u001b[0m\u001b[33m(block_ap_research.py 568)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-12 13:33:12 root]\u001b[0m\u001b[33m(block_ap_research.py 569)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-12 13:33:12 root]\u001b[0m\u001b[33m(block_ap_research.py 572)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-12 13:33:59 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/3 | Train Loss: 0.050396 | Val Loss: 0.068509 | Grad Norm: 0.15 | LR: 7.59e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:34:39 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/3 | Train Loss: 0.047574 | Val Loss: 0.067127 | Grad Norm: 0.10 | LR: 2.84e-05 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:35:19 root]\u001b[0m\u001b[33m(block_ap_research.py 704)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 2/3 | Train Loss: 0.046543 | Val Loss: 0.066718 | Grad Norm: 0.06 | LR: 5.00e-06 | Time: 40.0s\n",
      "\u001b[32m[2025-10-12 13:35:33 root]\u001b[0m\u001b[33m(block_ap_research.py 775)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq_2.5bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 13:35:33 root]\u001b[0m\u001b[33m(block_ap_research.py 783)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq_2.5bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-12 13:35:33 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 5023.40s (83.72min)\n",
      "\u001b[32m[2025-10-12 13:35:33 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-12 13:35:36 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq_2.5bit/model\n",
      "\u001b[32m[2025-10-12 13:35:36 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100%|█████████████████████████████████████████| 163/163 [01:59<00:00,  1.36it/s]\n",
      "wikitext2:9.15273666381836\n",
      "\u001b[32m[2025-10-12 13:37:39 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 9.15\n",
      "\u001b[32m[2025-10-12 13:37:40 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/root/PR/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 13:37:40 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-12 13:37:45 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/PR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 167947.13it/s]\n",
      "\u001b[32m[2025-10-12 13:37:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:01<00:00, 5362.81it/s]\n",
      "\u001b[32m[2025-10-12 13:37:55 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 2328.66it/s]\n",
      "\u001b[32m[2025-10-12 13:37:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:00<00:00, 2252.63it/s]\n",
      "\u001b[32m[2025-10-12 13:37:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 55879/55879 [27:38<00:00, 33.69it/s]\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6393|±  |0.0135|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5129|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.6746|±  |0.0047|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.6877|±  |0.0095|\n",
      "|          |       |none  |     0|acc_norm|0.6309|±  |0.0099|\n",
      "|piqa      |      1|none  |     0|acc     |0.7372|±  |0.0103|\n",
      "|          |       |none  |     0|acc_norm|0.7508|±  |0.0101|\n",
      "\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 64.43%\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq_2.5bit/results.json\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-12 14:05:53 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "    --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "    --sensitivity_file \"./sensitivity_results_mistral_7b_instruct_v0.2.json\" \\\n",
    "    --use_mixed_precision \\\n",
    "    --mpq_strategy aggressive \\\n",
    "    --target_avg_bits 2.5 \\\n",
    "    --real_quant \\\n",
    "    --use_adaptive_training \\\n",
    "    --epochs 3 \\\n",
    "    --train_size 256 \\\n",
    "    --val_size 32 \\\n",
    "    --quant_lr 1e-4 \\\n",
    "    --weight_lr 2e-5 \\\n",
    "    --output_dir \"./output/mistral_mpq_2.5bit\" \\\n",
    "    --save_quant_dir \"./output/mistral_mpq_2.5bit/model\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ab193",
   "metadata": {},
   "source": [
    "# MPQ 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AtS5Dv7tWMVS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4850464,
     "status": "ok",
     "timestamp": 1761590595071,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "AtS5Dv7tWMVS",
    "outputId": "3b24decf-3122-488c-d64c-c2d726a22000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'aggressive', '--target_avg_bits', '2', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_mpq_2bit', '--save_quant_dir', './output/mistral_mpq/model_2bit', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: aggressive\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.0\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq_2bit', save_quant_dir='./output/mistral_mpq/model_2bit', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='aggressive', target_avg_bits=2.0, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-27 17:22:31 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.14s/it]\n",
      "\u001b[32m[2025-10-27 17:22:36 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-27 17:22:36 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-27 17:22:36 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-27 17:22:36 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: aggressive, Target Avg: 2.0 bits\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.09 bits\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 7.64x vs FP16\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-27 17:22:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-27 17:23:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 25.9s\n",
      "\u001b[32m[2025-10-27 17:23:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:23:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-27 17:23:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-27 17:24:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000030 | Val Loss: 0.000023 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:24:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000012 | Val Loss: 0.000016 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-27 17:25:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:25:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000029 | Val Loss: 0.000031 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:26:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000024 | Val Loss: 0.000030 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:26:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-27 17:26:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-27 17:26:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000034 | Val Loss: 0.000039 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:27:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000032 | Val Loss: 0.000039 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:27:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-27 17:27:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:28:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000076 | Val Loss: 0.000075 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:28:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000066 | Val Loss: 0.000072 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:28:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-27 17:28:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:29:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000139 | Val Loss: 0.000133 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:29:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000123 | Val Loss: 0.000129 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:30:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-27 17:30:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:30:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000228 | Val Loss: 0.000217 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:30:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000202 | Val Loss: 0.000209 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:31:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-27 17:31:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:31:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000357 | Val Loss: 0.000337 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:32:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000318 | Val Loss: 0.000327 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:32:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-27 17:32:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:33:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000488 | Val Loss: 0.000466 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:33:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000439 | Val Loss: 0.000453 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:33:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-27 17:33:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:34:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000660 | Val Loss: 0.000636 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:34:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000601 | Val Loss: 0.000620 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:35:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-27 17:35:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:35:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000863 | Val Loss: 0.000830 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:36:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000787 | Val Loss: 0.000810 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:36:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-27 17:36:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:36:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.001114 | Val Loss: 0.001079 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:37:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.001026 | Val Loss: 0.001056 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:37:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-27 17:37:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:38:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.001427 | Val Loss: 0.001387 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:38:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.001317 | Val Loss: 0.001357 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-27 17:38:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:39:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.001812 | Val Loss: 0.001763 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:39:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.001684 | Val Loss: 0.001729 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-27 17:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:40:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.002227 | Val Loss: 0.002168 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:41:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.002064 | Val Loss: 0.002126 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:41:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-27 17:41:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:41:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.002909 | Val Loss: 0.002846 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:42:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.002707 | Val Loss: 0.002793 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:42:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-27 17:42:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:43:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.003743 | Val Loss: 0.003664 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:43:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.003485 | Val Loss: 0.003600 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:43:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-27 17:43:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:44:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.004800 | Val Loss: 0.004693 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:44:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.004467 | Val Loss: 0.004606 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:45:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-27 17:45:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:45:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.006309 | Val Loss: 0.006199 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:46:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.005872 | Val Loss: 0.006086 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:46:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-27 17:46:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:46:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.008695 | Val Loss: 0.008669 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-27 17:47:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.008131 | Val Loss: 0.008527 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:47:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-27 17:47:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:48:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.011027 | Val Loss: 0.011157 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:48:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.010425 | Val Loss: 0.011010 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:48:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-27 17:48:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:49:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.013614 | Val Loss: 0.013885 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:49:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.012945 | Val Loss: 0.013726 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:50:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-27 17:50:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:50:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.016500 | Val Loss: 0.017022 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:51:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.015789 | Val Loss: 0.016862 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:51:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-27 17:51:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:51:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.020115 | Val Loss: 0.020795 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:52:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.019281 | Val Loss: 0.020606 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:52:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-27 17:52:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:53:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.024218 | Val Loss: 0.025227 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:53:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.023302 | Val Loss: 0.025009 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:53:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-27 17:53:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:54:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.029017 | Val Loss: 0.030257 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:54:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.027966 | Val Loss: 0.030011 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:55:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-27 17:55:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:55:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.034809 | Val Loss: 0.036362 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:56:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.033581 | Val Loss: 0.036071 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:56:24 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-27 17:56:24 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:56:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.041838 | Val Loss: 0.043835 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:57:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.040440 | Val Loss: 0.043505 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:57:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-27 17:57:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:58:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.052458 | Val Loss: 0.054772 | Grad Norm: 0.04 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:58:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.050513 | Val Loss: 0.054318 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 17:58:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-27 17:58:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 17:59:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.068697 | Val Loss: 0.071621 | Grad Norm: 0.06 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 17:59:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.065945 | Val Loss: 0.070996 | Grad Norm: 0.04 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 18:00:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-27 18:00:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 18:00:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.088381 | Val Loss: 0.091655 | Grad Norm: 0.09 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 18:01:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.084341 | Val Loss: 0.090755 | Grad Norm: 0.06 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 18:01:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-27 18:01:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-27 18:01:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.124473 | Val Loss: 0.127685 | Grad Norm: 0.21 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-27 18:02:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.116724 | Val Loss: 0.126134 | Grad Norm: 0.13 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-27 18:02:39 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq_2bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-27 18:02:39 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq/model_2bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-27 18:02:40 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2403.46s (40.06min)\n",
      "\u001b[32m[2025-10-27 18:02:40 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-27 18:02:46 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq/model_2bit\n",
      "\u001b[32m[2025-10-27 18:02:46 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:37<00:00,  1.04it/s]\n",
      "wikitext2:14.398239135742188\n",
      "\u001b[32m[2025-10-27 18:05:35 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 14.40\n",
      "2025-10-27 18:05:36.081710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761588336.102837   27854 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761588336.109067   27854 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761588336.129476   27854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761588336.129502   27854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761588336.129506   27854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761588336.129509   27854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 18.2MB/s]\n",
      "\u001b[32m[2025-10-27 18:05:41 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-27 18:05:42 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-27 18:05:46 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading readme: 8.41kB [00:00, 28.0MB/s]\n",
      "Downloading readme: 9.00kB [00:00, 33.5MB/s]\n",
      "Downloading readme: 7.02kB [00:00, 29.4MB/s]\n",
      "Downloading readme: 11.2kB [00:00, 38.3MB/s]\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 127674.20it/s]\n",
      "\u001b[32m[2025-10-27 18:06:14 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2402.23it/s]\n",
      "\u001b[32m[2025-10-27 18:06:20 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1041.60it/s]\n",
      "\u001b[32m[2025-10-27 18:06:22 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1015.59it/s]\n",
      "\u001b[32m[2025-10-27 18:06:24 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:10<00:00, 25.75it/s]\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5754|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4765|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.6203|±  |0.0048|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.5829|±  |0.0101|\n",
      "|          |       |none  |     0|acc_norm|0.5282|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6921|±  |0.0108|\n",
      "|          |       |none  |     0|acc_norm|0.7067|±  |0.0106|\n",
      "\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 58.17%\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq_2bit/results.json\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-27 18:43:15 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "       --use_mixed_precision \\\n",
    "       --mpq_strategy aggressive \\\n",
    "       --target_avg_bits 2 \\\n",
    "       --train_size 128 --val_size 16 \\\n",
    "       --quant_lr 3e-5 --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_mpq_2bit \\\n",
    "       --save_quant_dir ./output/mistral_mpq/model_2bit \\\n",
    "       --eval_ppl --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53b25d",
   "metadata": {},
   "source": [
    "# uniform 2bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8k-rSRBVrgCI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4884438,
     "status": "ok",
     "timestamp": 1761666275071,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "8k-rSRBVrgCI",
    "outputId": "d5dea922-33b2-4dac-d4d9-463c40bd24d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_block_ap.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--calib_dataset', 'wikitext2', '--train_size', '128', '--val_size', '16', '--wbits', '2', '--group_size', '128', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--output_dir', './output/mistral_baseline', '--save_quant_dir', './output/mistral_baseline/model', '--real_quant', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 14:23:14 root]\u001b[0m\u001b[33m(main_block_ap.py 135)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_baseline', save_quant_dir='./output/mistral_baseline/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)\n",
      "\u001b[32m[2025-10-28 14:23:14 root]\u001b[0m\u001b[33m(main_block_ap.py 139)\u001b[0m: INFO net is None, setting as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.16s/it]\n",
      "\u001b[32m[2025-10-28 14:23:19 root]\u001b[0m\u001b[33m(main_block_ap.py 163)\u001b[0m: INFO === start quantization ===\n",
      "\u001b[32m[2025-10-28 14:23:19 root]\u001b[0m\u001b[33m(main_block_ap.py 170)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 14:23:19 root]\u001b[0m\u001b[33m(main_block_ap.py 172)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 14:23:19 root]\u001b[0m\u001b[33m(block_ap.py 40)\u001b[0m: INFO Starting ...\n",
      "\u001b[32m[2025-10-28 14:23:22 root]\u001b[0m\u001b[33m(block_ap.py 129)\u001b[0m: INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.\n",
      "\u001b[32m[2025-10-28 14:23:23 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 0===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:23:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 0 recon_loss:3.136423174510128e-06 val_loss:2.4123983166646212e-06 quant_lr:1.540866429118164e-05 norm:0.00049932 max memory_allocated 7411.298828125 time 25.402329683303833 \n",
      "\u001b[32m[2025-10-28 14:24:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 0 epoch 1 recon_loss:2.2775132038077572e-06 val_loss:2.2031942989997333e-06 quant_lr:1.5e-06 norm:0.00019568 max memory_allocated 7475.298828125 time 25.384603023529053 \n",
      "\u001b[32m[2025-10-28 14:24:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:23 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:24:34 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 1===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:25:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 0 recon_loss:0.0006688449066132307 val_loss:0.0004969249130226672 quant_lr:1.540866429118164e-05 norm:0.03673201 max memory_allocated 7475.298828125 time 25.53079080581665 \n",
      "\u001b[32m[2025-10-28 14:25:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 1 epoch 1 recon_loss:0.00048106489703059196 val_loss:0.00047265581088140607 quant_lr:1.5e-06 norm:0.02136149 max memory_allocated 7475.298828125 time 25.545469284057617 \n",
      "\u001b[32m[2025-10-28 14:25:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:25:47 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 2===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:26:17 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 0 recon_loss:0.000491014332510531 val_loss:0.0004949821741320193 quant_lr:1.540866429118164e-05 norm:0.00046031 max memory_allocated 7475.298828125 time 25.52174687385559 \n",
      "\u001b[32m[2025-10-28 14:26:43 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 2 epoch 1 recon_loss:0.00048580364091321826 val_loss:0.000493692874442786 quant_lr:1.5e-06 norm:0.00031737 max memory_allocated 7475.298828125 time 25.56408405303955 \n",
      "\u001b[32m[2025-10-28 14:26:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:26:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:26:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:26:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:26:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:26:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:27:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:27:00 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 3===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:27:30 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 0 recon_loss:0.0005173158715479076 val_loss:0.0005215047858655453 quant_lr:1.540866429118164e-05 norm:0.00036558 max memory_allocated 7475.298828125 time 25.534059047698975 \n",
      "\u001b[32m[2025-10-28 14:27:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 3 epoch 1 recon_loss:0.0005115545354783535 val_loss:0.0005198261351324618 quant_lr:1.5e-06 norm:0.00029602 max memory_allocated 7475.923828125 time 25.599255323410034 \n",
      "\u001b[32m[2025-10-28 14:28:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:28:13 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 4===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:28:44 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 0 recon_loss:0.0005582089070230722 val_loss:0.0005601467564702034 quant_lr:1.540866429118164e-05 norm:0.00049075 max memory_allocated 7475.923828125 time 25.56941318511963 \n",
      "\u001b[32m[2025-10-28 14:29:10 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 4 epoch 1 recon_loss:0.0005489002796821296 val_loss:0.0005576780531555414 quant_lr:1.5e-06 norm:0.00038552 max memory_allocated 7475.923828125 time 25.61639952659607 \n",
      "\u001b[32m[2025-10-28 14:29:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:29:28 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 5===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:29:58 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 0 recon_loss:0.0006229499122127891 val_loss:0.0006220234790816903 quant_lr:1.540866429118164e-05 norm:0.00063370 max memory_allocated 7475.923828125 time 25.57908010482788 \n",
      "\u001b[32m[2025-10-28 14:30:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 5 epoch 1 recon_loss:0.0006077276775613427 val_loss:0.0006179217016324401 quant_lr:1.5e-06 norm:0.00047326 max memory_allocated 7475.923828125 time 25.673032760620117 \n",
      "\u001b[32m[2025-10-28 14:30:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:30:42 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 6===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:31:13 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 0 recon_loss:0.0007110176957212389 val_loss:0.0007071052677929401 quant_lr:1.540866429118164e-05 norm:0.00089550 max memory_allocated 7475.923828125 time 25.65145206451416 \n",
      "\u001b[32m[2025-10-28 14:31:39 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 6 epoch 1 recon_loss:0.0006872458616271615 val_loss:0.0007008578395470977 quant_lr:1.5e-06 norm:0.00058550 max memory_allocated 7475.923828125 time 25.7248797416687 \n",
      "\u001b[32m[2025-10-28 14:31:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:50 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:31:57 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 7===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:32:27 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 0 recon_loss:0.000832567922770977 val_loss:0.0008247832302004099 quant_lr:1.540866429118164e-05 norm:0.00130854 max memory_allocated 7475.923828125 time 25.636743307113647 \n",
      "\u001b[32m[2025-10-28 14:32:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 7 epoch 1 recon_loss:0.0007978049688972533 val_loss:0.0008158657583408058 quant_lr:1.5e-06 norm:0.00086561 max memory_allocated 7475.923828125 time 25.682978630065918 \n",
      "\u001b[32m[2025-10-28 14:32:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:33:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 8===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:33:42 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 0 recon_loss:0.0009539226884953678 val_loss:0.0009466790361329913 quant_lr:1.540866429118164e-05 norm:0.00145372 max memory_allocated 7475.923828125 time 25.627615690231323 \n",
      "\u001b[32m[2025-10-28 14:34:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 8 epoch 1 recon_loss:0.0009105742210522294 val_loss:0.0009362718556076288 quant_lr:1.5e-06 norm:0.00097679 max memory_allocated 7475.923828125 time 25.702110528945923 \n",
      "\u001b[32m[2025-10-28 14:34:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:34:26 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 9===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:34:57 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 0 recon_loss:0.0011105455923825502 val_loss:0.00110874033998698 quant_lr:1.540866429118164e-05 norm:0.00154965 max memory_allocated 7475.923828125 time 25.64984965324402 \n",
      "\u001b[32m[2025-10-28 14:35:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 9 epoch 1 recon_loss:0.0010576839558780193 val_loss:0.0010951790027320385 quant_lr:1.5e-06 norm:0.00098502 max memory_allocated 7475.923828125 time 25.731278896331787 \n",
      "\u001b[32m[2025-10-28 14:35:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:31 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:34 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:35:41 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 10===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:36:12 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 0 recon_loss:0.001302632037550211 val_loss:0.0012953870464116335 quant_lr:1.540866429118164e-05 norm:0.00241726 max memory_allocated 7475.923828125 time 25.6417179107666 \n",
      "\u001b[32m[2025-10-28 14:36:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 10 epoch 1 recon_loss:0.0012336550280451775 val_loss:0.0012784174177795649 quant_lr:1.5e-06 norm:0.00146568 max memory_allocated 7475.923828125 time 25.726993799209595 \n",
      "\u001b[32m[2025-10-28 14:36:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:36:56 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 11===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:37:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 0 recon_loss:0.0015280051156878471 val_loss:0.0015263730892911553 quant_lr:1.540866429118164e-05 norm:0.00248856 max memory_allocated 7475.923828125 time 25.687916040420532 \n",
      "\u001b[32m[2025-10-28 14:37:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 11 epoch 1 recon_loss:0.0014511689078062773 val_loss:0.0015068063512444496 quant_lr:1.5e-06 norm:0.00159949 max memory_allocated 7475.923828125 time 25.740883111953735 \n",
      "\u001b[32m[2025-10-28 14:37:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:37:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:37:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:38:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:38:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:38:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:38:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:38:11 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 12===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:38:41 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 0 recon_loss:0.0018167974194511771 val_loss:0.0018159844912588596 quant_lr:1.540866429118164e-05 norm:0.00300786 max memory_allocated 7475.923828125 time 25.644734144210815 \n",
      "\u001b[32m[2025-10-28 14:39:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 12 epoch 1 recon_loss:0.0017219488508999348 val_loss:0.0017914820928126574 quant_lr:1.5e-06 norm:0.00196731 max memory_allocated 7475.923828125 time 25.734721422195435 \n",
      "\u001b[32m[2025-10-28 14:39:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:39:25 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 13===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:39:56 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 0 recon_loss:0.002173011191189289 val_loss:0.0021676630713045597 quant_lr:1.540866429118164e-05 norm:0.00303721 max memory_allocated 7475.923828125 time 25.64930295944214 \n",
      "\u001b[32m[2025-10-28 14:40:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 13 epoch 1 recon_loss:0.002060686470940709 val_loss:0.002139926189556718 quant_lr:1.5e-06 norm:0.00201826 max memory_allocated 7475.923828125 time 25.774309396743774 \n",
      "\u001b[32m[2025-10-28 14:40:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:40:40 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 14===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:41:11 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 0 recon_loss:0.0025292704813182354 val_loss:0.00253122067078948 quant_lr:1.540866429118164e-05 norm:0.00465933 max memory_allocated 7475.923828125 time 25.6571843624115 \n",
      "\u001b[32m[2025-10-28 14:41:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 14 epoch 1 recon_loss:0.0023834581952542067 val_loss:0.002496188273653388 quant_lr:1.5e-06 norm:0.00282586 max memory_allocated 7475.923828125 time 25.727012634277344 \n",
      "\u001b[32m[2025-10-28 14:41:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:41:55 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 15===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:42:26 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 0 recon_loss:0.0031495443545281887 val_loss:0.0031660855747759342 quant_lr:1.540866429118164e-05 norm:0.00481147 max memory_allocated 7475.923828125 time 25.679659128189087 \n",
      "\u001b[32m[2025-10-28 14:42:51 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 15 epoch 1 recon_loss:0.0029739616438746452 val_loss:0.003122154390439391 quant_lr:1.5e-06 norm:0.00333880 max memory_allocated 7475.923828125 time 25.74117350578308 \n",
      "\u001b[32m[2025-10-28 14:42:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:42:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:42:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:43:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:43:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:43:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:43:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:43:10 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 16===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:43:40 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 0 recon_loss:0.003899301402270794 val_loss:0.003918302245438099 quant_lr:1.540866429118164e-05 norm:0.00528508 max memory_allocated 7475.923828125 time 25.69118618965149 \n",
      "\u001b[32m[2025-10-28 14:44:06 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 16 epoch 1 recon_loss:0.0036750666331499815 val_loss:0.0038651758804917336 quant_lr:1.5e-06 norm:0.00377133 max memory_allocated 7475.923828125 time 25.739209175109863 \n",
      "\u001b[32m[2025-10-28 14:44:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:13 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:17 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:20 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:44:24 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 17===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:44:55 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 0 recon_loss:0.004859359003603458 val_loss:0.004866436589509249 quant_lr:1.540866429118164e-05 norm:0.00692490 max memory_allocated 7475.923828125 time 25.68199586868286 \n",
      "\u001b[32m[2025-10-28 14:45:21 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 17 epoch 1 recon_loss:0.004567014053463936 val_loss:0.004794356878846884 quant_lr:1.5e-06 norm:0.00450553 max memory_allocated 7475.923828125 time 25.785629272460938 \n",
      "\u001b[32m[2025-10-28 14:45:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:28 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:32 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:35 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:45:39 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 18===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:46:09 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 0 recon_loss:0.006211248226463795 val_loss:0.006251784041523933 quant_lr:1.540866429118164e-05 norm:0.00964531 max memory_allocated 7475.923828125 time 25.692474365234375 \n",
      "\u001b[32m[2025-10-28 14:46:35 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 18 epoch 1 recon_loss:0.005832240916788578 val_loss:0.006160699296742678 quant_lr:1.5e-06 norm:0.00670567 max memory_allocated 7475.923828125 time 25.805261373519897 \n",
      "\u001b[32m[2025-10-28 14:46:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:43 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:46 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:49 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:53 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:46:53 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 19===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:47:24 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 0 recon_loss:0.008392971940338612 val_loss:0.008564401417970657 quant_lr:1.540866429118164e-05 norm:0.01240992 max memory_allocated 7475.923828125 time 25.696749687194824 \n",
      "\u001b[32m[2025-10-28 14:47:50 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 19 epoch 1 recon_loss:0.007893701083958149 val_loss:0.008449705317616463 quant_lr:1.5e-06 norm:0.00878835 max memory_allocated 7476.048828125 time 25.764915466308594 \n",
      "\u001b[32m[2025-10-28 14:47:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:47:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:47:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:47:58 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:48:01 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:48:04 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:48:08 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:48:08 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 20===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:48:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 0 recon_loss:0.010496614500880241 val_loss:0.010866818949580193 quant_lr:1.540866429118164e-05 norm:0.01130174 max memory_allocated 7476.048828125 time 25.732831478118896 \n",
      "\u001b[32m[2025-10-28 14:49:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 20 epoch 1 recon_loss:0.009965366683900356 val_loss:0.010742348618805408 quant_lr:1.5e-06 norm:0.00818150 max memory_allocated 7476.923828125 time 25.783523321151733 \n",
      "\u001b[32m[2025-10-28 14:49:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:16 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:19 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:49:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 21===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:49:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 0 recon_loss:0.01281658373773098 val_loss:0.013401863165199757 quant_lr:1.540866429118164e-05 norm:0.01287760 max memory_allocated 7476.923828125 time 25.69341206550598 \n",
      "\u001b[32m[2025-10-28 14:50:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 21 epoch 1 recon_loss:0.012242300435900688 val_loss:0.013275504112243652 quant_lr:1.5e-06 norm:0.00882184 max memory_allocated 7476.923828125 time 25.767879962921143 \n",
      "\u001b[32m[2025-10-28 14:50:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:50:37 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 22===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:51:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 0 recon_loss:0.015414601191878319 val_loss:0.016317911446094513 quant_lr:1.540866429118164e-05 norm:0.01288463 max memory_allocated 7476.923828125 time 25.71384334564209 \n",
      "\u001b[32m[2025-10-28 14:51:34 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 22 epoch 1 recon_loss:0.014797168783843517 val_loss:0.01618475280702114 quant_lr:1.5e-06 norm:0.00935116 max memory_allocated 7476.923828125 time 25.82449436187744 \n",
      "\u001b[32m[2025-10-28 14:51:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:51:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 23===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:52:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 0 recon_loss:0.01866319589316845 val_loss:0.019818447530269623 quant_lr:1.540866429118164e-05 norm:0.01517390 max memory_allocated 7476.923828125 time 25.706485748291016 \n",
      "\u001b[32m[2025-10-28 14:52:49 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 23 epoch 1 recon_loss:0.01793876849114895 val_loss:0.019664950668811798 quant_lr:1.5e-06 norm:0.01075658 max memory_allocated 7476.923828125 time 25.884451866149902 \n",
      "\u001b[32m[2025-10-28 14:52:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:52:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:52:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:52:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:53:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:53:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:53:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:53:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 24===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:53:38 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 0 recon_loss:0.02235661819577217 val_loss:0.023914560675621033 quant_lr:1.540866429118164e-05 norm:0.01550343 max memory_allocated 7476.923828125 time 25.69486141204834 \n",
      "\u001b[32m[2025-10-28 14:54:04 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 24 epoch 1 recon_loss:0.02155229076743126 val_loss:0.023735646158456802 quant_lr:1.5e-06 norm:0.01166141 max memory_allocated 7476.923828125 time 25.789507389068604 \n",
      "\u001b[32m[2025-10-28 14:54:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:12 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:15 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:22 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:54:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 25===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:54:53 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 0 recon_loss:0.026656145229935646 val_loss:0.02855071611702442 quant_lr:1.540866429118164e-05 norm:0.01686069 max memory_allocated 7476.923828125 time 25.710556268692017 \n",
      "\u001b[32m[2025-10-28 14:55:19 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 25 epoch 1 recon_loss:0.0257343128323555 val_loss:0.028352415189146996 quant_lr:1.5e-06 norm:0.01231158 max memory_allocated 7476.923828125 time 25.754931688308716 \n",
      "\u001b[32m[2025-10-28 14:55:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:27 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:30 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:37 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:55:37 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 26===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:56:08 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 0 recon_loss:0.031848713755607605 val_loss:0.0341719426214695 quant_lr:1.540866429118164e-05 norm:0.01883207 max memory_allocated 7476.923828125 time 25.720133066177368 \n",
      "\u001b[32m[2025-10-28 14:56:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 26 epoch 1 recon_loss:0.03077426739037037 val_loss:0.03393658995628357 quant_lr:1.5e-06 norm:0.01357616 max memory_allocated 7476.923828125 time 25.811476469039917 \n",
      "\u001b[32m[2025-10-28 14:56:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:42 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:45 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:48 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:52 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:56:52 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 27===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:57:23 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 0 recon_loss:0.03811631724238396 val_loss:0.041038986295461655 quant_lr:1.540866429118164e-05 norm:0.02429280 max memory_allocated 7476.923828125 time 25.71034836769104 \n",
      "\u001b[32m[2025-10-28 14:57:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 27 epoch 1 recon_loss:0.036906879395246506 val_loss:0.040765728801488876 quant_lr:1.5e-06 norm:0.01669190 max memory_allocated 7476.923828125 time 25.838112592697144 \n",
      "\u001b[32m[2025-10-28 14:57:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:57:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:57:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:57:57 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:58:00 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:58:03 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:58:07 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:58:07 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 28===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:58:37 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 0 recon_loss:0.047627829015254974 val_loss:0.05109136924147606 quant_lr:1.540866429118164e-05 norm:0.03378984 max memory_allocated 7476.923828125 time 25.717599630355835 \n",
      "\u001b[32m[2025-10-28 14:59:03 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 28 epoch 1 recon_loss:0.045944638550281525 val_loss:0.050718292593955994 quant_lr:1.5e-06 norm:0.02200302 max memory_allocated 7476.923828125 time 25.803441286087036 \n",
      "\u001b[32m[2025-10-28 14:59:09 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:10 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:11 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:14 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:18 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:21 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 14:59:22 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 29===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 14:59:52 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 0 recon_loss:0.06213292479515076 val_loss:0.06664849072694778 quant_lr:1.540866429118164e-05 norm:0.05080481 max memory_allocated 7476.923828125 time 25.73236632347107 \n",
      "\u001b[32m[2025-10-28 15:00:18 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 29 epoch 1 recon_loss:0.05978496000170708 val_loss:0.06613367795944214 quant_lr:1.5e-06 norm:0.03426832 max memory_allocated 7476.923828125 time 25.801496744155884 \n",
      "\u001b[32m[2025-10-28 15:00:24 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:25 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:26 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:29 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:33 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:36 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 15:00:36 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 30===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 15:01:07 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 0 recon_loss:0.0795903429389 val_loss:0.08487866818904877 quant_lr:1.540866429118164e-05 norm:0.08780856 max memory_allocated 7476.923828125 time 25.75078558921814 \n",
      "\u001b[32m[2025-10-28 15:01:33 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 30 epoch 1 recon_loss:0.07613086700439453 val_loss:0.0840831845998764 quant_lr:1.5e-06 norm:0.05679744 max memory_allocated 7476.923828125 time 25.820194005966187 \n",
      "\u001b[32m[2025-10-28 15:01:39 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:40 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:41 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:44 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:47 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:51 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 15:01:51 root]\u001b[0m\u001b[33m(block_ap.py 166)\u001b[0m: INFO === Start quantize blocks 31===\n",
      "trainable parameter number: 221.519872M\n",
      "\u001b[32m[2025-10-28 15:02:22 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 0 recon_loss:0.11217313259840012 val_loss:0.11849859356880188 quant_lr:1.540866429118164e-05 norm:0.18989678 max memory_allocated 7476.923828125 time 25.73183536529541 \n",
      "\u001b[32m[2025-10-28 15:02:48 root]\u001b[0m\u001b[33m(block_ap.py 266)\u001b[0m: INFO blocks 31 epoch 1 recon_loss:0.10590959340333939 val_loss:0.11723268777132034 quant_lr:1.5e-06 norm:0.12150288 max memory_allocated 7476.923828125 time 25.823623180389404 \n",
      "\u001b[32m[2025-10-28 15:02:54 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.q_proj finished\n",
      "\u001b[32m[2025-10-28 15:02:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.k_proj finished\n",
      "\u001b[32m[2025-10-28 15:02:55 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.v_proj finished\n",
      "\u001b[32m[2025-10-28 15:02:56 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized self_attn.o_proj finished\n",
      "\u001b[32m[2025-10-28 15:02:59 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.gate_proj finished\n",
      "\u001b[32m[2025-10-28 15:03:02 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.up_proj finished\n",
      "\u001b[32m[2025-10-28 15:03:06 root]\u001b[0m\u001b[33m(block_ap.py 300)\u001b[0m: INFO pack quantized mlp.down_proj finished\n",
      "\u001b[32m[2025-10-28 15:03:07 root]\u001b[0m\u001b[33m(main_block_ap.py 191)\u001b[0m: INFO 2387.2462208271027\n",
      "\u001b[32m[2025-10-28 15:03:07 root]\u001b[0m\u001b[33m(main_block_ap.py 194)\u001b[0m: INFO start saving model\n",
      "\u001b[32m[2025-10-28 15:03:13 root]\u001b[0m\u001b[33m(main_block_ap.py 197)\u001b[0m: INFO save model success\n",
      "get_wikitext2\n",
      "100% 163/163 [02:41<00:00,  1.01it/s]\n",
      "wikitext2:13.02121353149414\n",
      "\u001b[32m[2025-10-28 15:06:07 root]\u001b[0m\u001b[33m(main_block_ap.py 39)\u001b[0m: INFO wikitext2 perplexity: 13.02\n",
      "2025-10-28 15:06:08.132136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761663968.152875   77742 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761663968.159228   77742 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761663968.180418   77742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761663968.180442   77742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761663968.180445   77742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761663968.180449   77742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 15:06:13 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 15:06:13 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 15:06:17 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 128120.53it/s]\n",
      "\u001b[32m[2025-10-28 15:06:44 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2403.79it/s]\n",
      "\u001b[32m[2025-10-28 15:06:49 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1049.34it/s]\n",
      "\u001b[32m[2025-10-28 15:06:52 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1015.66it/s]\n",
      "\u001b[32m[2025-10-28 15:06:54 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:56<00:00, 25.21it/s]\n",
      "\u001b[32m[2025-10-28 15:44:32 root]\u001b[0m\u001b[33m(main_block_ap.py 54)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5848|±  |0.0138|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4743|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.6160|±  |0.0049|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.6208|±  |0.0100|\n",
      "|          |       |none  |     0|acc_norm|0.5711|±  |0.0102|\n",
      "|piqa      |      1|none  |     0|acc     |0.6986|±  |0.0107|\n",
      "|          |       |none  |     0|acc_norm|0.7067|±  |0.0106|\n",
      "\n",
      "\u001b[32m[2025-10-28 15:44:32 root]\u001b[0m\u001b[33m(main_block_ap.py 58)\u001b[0m: INFO Average Acc: 59.46%\n"
     ]
    }
   ],
   "source": [
    "!python main_block_ap.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --calib_dataset wikitext2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --wbits 2 \\\n",
    "       --group_size 128 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --output_dir ./output/mistral_baseline \\\n",
    "       --save_quant_dir ./output/mistral_baseline/model \\\n",
    "       --real_quant \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hoe8rHFSMmzD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5136000,
     "status": "ok",
     "timestamp": 1761638195141,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "hoe8rHFSMmzD",
    "outputId": "9576a872-9543-4087-c96c-7a0f53d51ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--use_adaptive_training', '--mpq_strategy', 'conservative', '--target_avg_bits', '4.0', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_combined', '--save_quant_dir', './output/mistral_combined/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: conservative\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 4.0\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_combined', save_quant_dir='./output/mistral_combined/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='conservative', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 06:31:03 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.24s/it]\n",
      "\u001b[32m[2025-10-28 06:31:08 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 06:31:08 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 06:31:08 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 06:31:08 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 06:31:09 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: conservative, Target Avg: 4.0 bits\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [5, 6, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4]\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 4.03 bits\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 3.97x vs FP16\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 2.00e-05-3.00e-05\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-28 06:31:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.33e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 06:31:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.76e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 06:32:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 6.55e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 06:32:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.16e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:32:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 06:32:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-28 06:32:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 2.00e-05, Patience: 5\n",
      "\u001b[32m[2025-10-28 06:33:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000019 | Val Loss: 0.000004 | Grad Norm: 0.06 | LR: 1.71e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:33:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000003 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 1.04e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:34:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.01 | LR: 3.70e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:34:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000002 | Val Loss: 0.000003 | Grad Norm: 0.00 | LR: 1.00e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:34:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 06:34:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:34:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.55e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 06:35:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000004 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.93e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:35:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 7.17e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000003 | Val Loss: 0.000004 | Grad Norm: 0.00 | LR: 1.28e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 06:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-28 06:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.44e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 06:37:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000004 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 1.85e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:37:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000004 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 6.86e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:38:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000004 | Val Loss: 0.000005 | Grad Norm: 0.00 | LR: 1.22e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:38:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 06:38:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:38:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.62e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:38:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000006 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.98e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:39:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000005 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 7.36e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:39:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000005 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.31e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 06:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:40:05 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.66e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:40:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000009 | Val Loss: 0.000010 | Grad Norm: 0.00 | LR: 2.01e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:41:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000008 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 7.47e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:41:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000008 | Val Loss: 0.000009 | Grad Norm: 0.00 | LR: 1.33e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:41:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 06:41:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:41:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.70e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:42:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000013 | Val Loss: 0.000014 | Grad Norm: 0.00 | LR: 1.39e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:42:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000012 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 1.35e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:43:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 06:43:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:43:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:43:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000020 | Val Loss: 0.000020 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:44:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000018 | Val Loss: 0.000020 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 06:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:44:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:44:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000026 | Val Loss: 0.000027 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:45:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000024 | Val Loss: 0.000026 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 06:45:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 06:45:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:45:36 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:46:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000035 | Val Loss: 0.000037 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:46:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000033 | Val Loss: 0.000036 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:46:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 06:46:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:46:52 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.75e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:47:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000045 | Val Loss: 0.000048 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:47:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000042 | Val Loss: 0.000046 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:48:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 06:48:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:48:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:48:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000058 | Val Loss: 0.000062 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:49:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000055 | Val Loss: 0.000061 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:49:24 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 06:49:24 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:49:24 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:49:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000075 | Val Loss: 0.000081 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:50:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000070 | Val Loss: 0.000079 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:50:40 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 06:50:40 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:50:40 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.80e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:51:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000094 | Val Loss: 0.000103 | Grad Norm: 0.00 | LR: 1.44e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:51:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000089 | Val Loss: 0.000101 | Grad Norm: 0.00 | LR: 1.40e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:51:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 06:51:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:51:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.79e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:52:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000115 | Val Loss: 0.000127 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:52:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000108 | Val Loss: 0.000124 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 06:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:53:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:53:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000150 | Val Loss: 0.000167 | Grad Norm: 0.00 | LR: 1.46e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:54:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000141 | Val Loss: 0.000163 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:54:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 06:54:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:54:28 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.91e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:55:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.000188 | Val Loss: 0.000211 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:55:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.000176 | Val Loss: 0.000206 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 06:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:55:44 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:56:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.000236 | Val Loss: 0.000266 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:56:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.000222 | Val Loss: 0.000261 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 06:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.78e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:57:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.000301 | Val Loss: 0.000344 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:57:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.000283 | Val Loss: 0.000336 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:58:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 06:58:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:58:17 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.82e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 06:58:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.000402 | Val Loss: 0.000469 | Grad Norm: 0.00 | LR: 1.45e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 06:59:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.000378 | Val Loss: 0.000459 | Grad Norm: 0.00 | LR: 1.41e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 06:59:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 06:59:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 06:59:33 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:00:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.000496 | Val Loss: 0.000588 | Grad Norm: 0.00 | LR: 1.50e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:00:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.000468 | Val Loss: 0.000578 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:00:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 07:00:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:00:49 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:01:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.000598 | Val Loss: 0.000720 | Grad Norm: 0.00 | LR: 1.47e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:01:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.000569 | Val Loss: 0.000710 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 07:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:02:05 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:02:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.000717 | Val Loss: 0.000880 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:03:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.000685 | Val Loss: 0.000869 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:03:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 07:03:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 07:03:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:03:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.001216 | Val Loss: 0.001361 | Grad Norm: 0.00 | LR: 1.50e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:04:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.001113 | Val Loss: 0.001340 | Grad Norm: 0.00 | LR: 1.46e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:04:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 07:04:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:04:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.88e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:05:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.001337 | Val Loss: 0.001598 | Grad Norm: 0.00 | LR: 1.48e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:05:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.001277 | Val Loss: 0.001582 | Grad Norm: 0.00 | LR: 1.44e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:05:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 07:05:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 07:05:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:06:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.001990 | Val Loss: 0.002254 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:06:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.001846 | Val Loss: 0.002226 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:07:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 07:07:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 07:07:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.93e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:07:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.002709 | Val Loss: 0.003051 | Grad Norm: 0.01 | LR: 1.51e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:08:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.002522 | Val Loss: 0.003014 | Grad Norm: 0.00 | LR: 1.47e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:08:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 07:08:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:08:27 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:08:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.003000 | Val Loss: 0.003573 | Grad Norm: 0.01 | LR: 1.49e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 07:09:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.002889 | Val Loss: 0.003542 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:09:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 07:09:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:09:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:10:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.003608 | Val Loss: 0.004388 | Grad Norm: 0.01 | LR: 1.47e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:10:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.003477 | Val Loss: 0.004350 | Grad Norm: 0.01 | LR: 1.43e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:11:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 07:11:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:11:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:11:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.004565 | Val Loss: 0.005668 | Grad Norm: 0.01 | LR: 1.49e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:11:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.004398 | Val Loss: 0.005615 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:12:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 07:12:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:12:16 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.83e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:12:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.005705 | Val Loss: 0.007182 | Grad Norm: 0.02 | LR: 1.45e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:13:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.005492 | Val Loss: 0.007085 | Grad Norm: 0.02 | LR: 1.42e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:13:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 07:13:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 07:13:32 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 07:14:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.007955 | Val Loss: 0.010112 | Grad Norm: 0.06 | LR: 1.47e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:14:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.007613 | Val Loss: 0.009972 | Grad Norm: 0.05 | LR: 1.43e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 07:14:48 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_combined/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 07:14:48 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_combined/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 07:14:48 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2620.30s (43.67min)\n",
      "\u001b[32m[2025-10-28 07:14:48 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 07:15:01 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_combined/model\n",
      "\u001b[32m[2025-10-28 07:15:01 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:41<00:00,  1.01it/s]\n",
      "wikitext2:6.069447994232178\n",
      "\u001b[32m[2025-10-28 07:17:55 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 6.07\n",
      "2025-10-28 07:17:55.394494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761635875.415486   39710 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761635875.421846   39710 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761635875.443471   39710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761635875.443498   39710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761635875.443501   39710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761635875.443504   39710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 07:18:00 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 07:18:00 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 07:18:04 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 07:18:30 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 07:18:30 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 07:18:30 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 07:18:30 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 07:18:30 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126558.30it/s]\n",
      "\u001b[32m[2025-10-28 07:18:31 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2394.03it/s]\n",
      "\u001b[32m[2025-10-28 07:18:36 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1056.20it/s]\n",
      "\u001b[32m[2025-10-28 07:18:39 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1018.32it/s]\n",
      "\u001b[32m[2025-10-28 07:18:41 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:09<00:00, 25.06it/s]\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.7316|±  |0.0125|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6509|±  |0.0048|\n",
      "|          |       |none  |     0|acc_norm|0.8275|±  |0.0038|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.8068|±  |0.0081|\n",
      "|          |       |none  |     0|acc_norm|0.7673|±  |0.0087|\n",
      "|piqa      |      1|none  |     0|acc     |0.7954|±  |0.0094|\n",
      "|          |       |none  |     0|acc_norm|0.8096|±  |0.0092|\n",
      "\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 74.62%\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_combined/results.json\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 07:56:32 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "  --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "  --use_mixed_precision \\\n",
    "  --use_adaptive_training \\\n",
    "  --mpq_strategy conservative \\\n",
    "  --target_avg_bits 4.0 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --quant_lr 3e-5 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c647b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b84f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "  --weight_lr 2e-6 \\\n",
    "  --real_quant \\\n",
    "  --output_dir ./output/mistral_combined \\\n",
    "  --save_quant_dir ./output/mistral_combined/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b08ec3",
   "metadata": {},
   "source": [
    "# MPQ 2.5 (2.69 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tPhBwPar0q8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5019703,
     "status": "ok",
     "timestamp": 1761650024191,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "tPhBwPar0q8f",
    "outputId": "b708f12b-1cee-42d5-af4a-08c09343f5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '2.5', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_combined', '--save_quant_dir', './output/mistral_combined/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.5\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_combined', save_quant_dir='./output/mistral_combined/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=2.5, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 09:50:08 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.14s/it]\n",
      "\u001b[32m[2025-10-28 09:50:13 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 09:50:13 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 09:50:13 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 09:50:13 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 2.5 bits\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3]\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.69 bits\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 5.95x vs FP16\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 09:50:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 3, Group: 128\n",
      "\u001b[32m[2025-10-28 09:50:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000001 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 25.8s\n",
      "\u001b[32m[2025-10-28 09:51:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 25.8s\n",
      "\u001b[32m[2025-10-28 09:51:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 09:51:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 3, Group: 64\n",
      "\u001b[32m[2025-10-28 09:52:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000030 | Val Loss: 0.000023 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:52:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000012 | Val Loss: 0.000016 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:52:44 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 09:52:44 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:53:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000014 | Val Loss: 0.000020 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 09:53:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000013 | Val Loss: 0.000019 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 09:53:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 09:53:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:54:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000020 | Val Loss: 0.000025 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 09:54:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000018 | Val Loss: 0.000024 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:55:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 09:55:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000028 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 09:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000026 | Val Loss: 0.000032 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 09:56:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:57:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000040 | Val Loss: 0.000045 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 09:57:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000038 | Val Loss: 0.000044 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 09:57:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:58:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000059 | Val Loss: 0.000062 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:58:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000054 | Val Loss: 0.000061 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 09:59:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 09:59:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 09:59:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000084 | Val Loss: 0.000087 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 09:59:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000077 | Val Loss: 0.000086 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:00:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 10:00:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:00:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000109 | Val Loss: 0.000113 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:01:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000101 | Val Loss: 0.000112 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:01:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 10:01:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:02:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000144 | Val Loss: 0.000150 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:02:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000134 | Val Loss: 0.000148 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:02:49 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 10:02:49 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:03:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000185 | Val Loss: 0.000192 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:03:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000172 | Val Loss: 0.000189 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:04:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 10:04:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:04:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000239 | Val Loss: 0.000248 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:05:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000223 | Val Loss: 0.000245 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:05:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 10:05:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:05:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000600 | Val Loss: 0.000545 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 10:06:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000497 | Val Loss: 0.000517 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 10:06:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:07:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.000583 | Val Loss: 0.000593 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:07:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000552 | Val Loss: 0.000587 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:07:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 10:07:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:08:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.000650 | Val Loss: 0.000668 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:08:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.000614 | Val Loss: 0.000661 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 10:09:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:09:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.000793 | Val Loss: 0.000822 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:10:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.000749 | Val Loss: 0.000813 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:10:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 10:10:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:10:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.001704 | Val Loss: 0.001570 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 10:11:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.001446 | Val Loss: 0.001506 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:11:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 10:11:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:12:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.001670 | Val Loss: 0.001702 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:12:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.001583 | Val Loss: 0.001685 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:12:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 10:12:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:13:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.001936 | Val Loss: 0.002007 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:13:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.001836 | Val Loss: 0.001987 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:14:05 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 10:14:05 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:14:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.002418 | Val Loss: 0.002560 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:15:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.002296 | Val Loss: 0.002536 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 10:15:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 10:15:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:15:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.004616 | Val Loss: 0.004479 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:16:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.004052 | Val Loss: 0.004336 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:16:36 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 10:16:36 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:17:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.004648 | Val Loss: 0.004920 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:17:33 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.004454 | Val Loss: 0.004887 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:17:52 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 10:17:52 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:18:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.007226 | Val Loss: 0.007226 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:18:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.006534 | Val Loss: 0.007060 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:19:07 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 10:19:07 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:19:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.009820 | Val Loss: 0.009839 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:20:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.009007 | Val Loss: 0.009649 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:20:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 10:20:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:20:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.012787 | Val Loss: 0.012943 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:21:20 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.011891 | Val Loss: 0.012729 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:21:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 10:21:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:22:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.016344 | Val Loss: 0.016576 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:22:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.015296 | Val Loss: 0.016323 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:22:54 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 10:22:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:23:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.020646 | Val Loss: 0.021041 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:23:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.019447 | Val Loss: 0.020747 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:24:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 10:24:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:24:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.025940 | Val Loss: 0.026570 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:25:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.024557 | Val Loss: 0.026233 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:25:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 10:25:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:25:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.029168 | Val Loss: 0.030891 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:26:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.028381 | Val Loss: 0.030717 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 10:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 10:26:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 10:27:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.042498 | Val Loss: 0.043577 | Grad Norm: 0.05 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 10:27:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.039889 | Val Loss: 0.042980 | Grad Norm: 0.03 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:27:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 10:27:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:28:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.048785 | Val Loss: 0.051920 | Grad Norm: 0.05 | LR: 1.54e-05 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 10:28:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.047298 | Val Loss: 0.051540 | Grad Norm: 0.04 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 10:29:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 10:29:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 10:29:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.064070 | Val Loss: 0.069021 | Grad Norm: 0.10 | LR: 1.54e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 10:30:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.061961 | Val Loss: 0.068451 | Grad Norm: 0.08 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 10:30:29 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_combined/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 10:30:29 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_combined/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 10:30:30 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2416.30s (40.27min)\n",
      "\u001b[32m[2025-10-28 10:30:30 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 10:30:37 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_combined/model\n",
      "\u001b[32m[2025-10-28 10:30:37 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:54<00:00,  1.07s/it]\n",
      "wikitext2:7.825936317443848\n",
      "\u001b[32m[2025-10-28 10:33:44 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 7.83\n",
      "2025-10-28 10:33:46.451351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761647626.761922    7878 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761647626.843568    7878 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761647627.438512    7878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761647627.438549    7878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761647627.438552    7878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761647627.438555    7878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Downloading builder script: 5.67kB [00:00, 19.8MB/s]\n",
      "\u001b[32m[2025-10-28 10:33:54 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 10:33:54 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 10:33:59 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.36kB [00:00, 18.4MB/s]\n",
      "Downloading readme: 8.41kB [00:00, 27.9MB/s]\n",
      "Downloading data: 100% 1.82M/1.82M [00:01<00:00, 1.64MB/s]\n",
      "Downloading data: 815kB [00:00, 76.6MB/s]       \n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 33437.95 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 37249.85 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 34838.65 examples/s]\n",
      "Downloading readme: 9.00kB [00:00, 29.0MB/s]\n",
      "Downloading data: 100% 331k/331k [00:00<00:00, 1.25MB/s]\n",
      "Downloading data: 100% 346k/346k [00:00<00:00, 1.37MB/s]\n",
      "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 347kB/s]\n",
      "Generating train split: 100% 2251/2251 [00:00<00:00, 281437.34 examples/s]\n",
      "Generating test split: 100% 2376/2376 [00:00<00:00, 441779.69 examples/s]\n",
      "Generating validation split: 100% 570/570 [00:00<00:00, 254904.92 examples/s]\n",
      "Downloading readme: 7.02kB [00:00, 26.4MB/s]\n",
      "Downloading data: 100% 24.4M/24.4M [00:00<00:00, 41.8MB/s]\n",
      "Downloading data: 100% 6.11M/6.11M [00:00<00:00, 12.3MB/s]\n",
      "Downloading data: 100% 6.32M/6.32M [00:00<00:00, 12.4MB/s]\n",
      "Generating train split: 100% 39905/39905 [00:00<00:00, 246253.68 examples/s]\n",
      "Generating test split: 100% 10003/10003 [00:00<00:00, 246486.05 examples/s]\n",
      "Generating validation split: 100% 10042/10042 [00:00<00:00, 258987.89 examples/s]\n",
      "Map: 100% 39905/39905 [00:06<00:00, 6335.10 examples/s]\n",
      "Map: 100% 10042/10042 [00:01<00:00, 6656.65 examples/s]\n",
      "Downloading readme: 11.2kB [00:00, 42.6MB/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 3.01MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 225kB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 177kB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1232436.21 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 681001.12 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 555993.22 examples/s]\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 123752.58it/s]\n",
      "\u001b[32m[2025-10-28 10:34:50 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2379.71it/s]\n",
      "\u001b[32m[2025-10-28 10:34:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1041.22it/s]\n",
      "\u001b[32m[2025-10-28 10:34:58 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1002.59it/s]\n",
      "\u001b[32m[2025-10-28 10:35:00 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:59<00:00, 24.52it/s]\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6890|±  |0.0130|\n",
      "|hellaswag |      1|none  |     0|acc     |0.6064|±  |0.0049|\n",
      "|          |       |none  |     0|acc_norm|0.7762|±  |0.0042|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7790|±  |0.0085|\n",
      "|          |       |none  |     0|acc_norm|0.7370|±  |0.0090|\n",
      "|piqa      |      1|none  |     0|acc     |0.7666|±  |0.0099|\n",
      "|          |       |none  |     0|acc_norm|0.7818|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 71.03%\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_combined/results.json\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 11:13:41 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "  --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy adaptive \\\n",
    "  --target_avg_bits 2.5 \\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --quant_lr 3e-5 \\\n",
    "  --weight_lr 2e-6 \\\n",
    "  --real_quant \\\n",
    "  --output_dir ./output/mistral_combined \\\n",
    "  --save_quant_dir ./output/mistral_combined/model \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03e6ec",
   "metadata": {},
   "source": [
    "## 2.4 bits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MNz9OdqrhfNj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 847392,
     "status": "ok",
     "timestamp": 1761655656625,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "MNz9OdqrhfNj",
    "outputId": "fe451521-0d67-4795-fdf1-82fabe3bb2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'adaptive', '--target_avg_bits', '2.4', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_combined1', '--save_quant_dir', './output/mistral_combined/model1', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: adaptive\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.4\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: False\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_combined1', save_quant_dir='./output/mistral_combined/model1', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='adaptive', target_avg_bits=2.4, use_adaptive_training=False, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 12:03:53 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.27s/it]\n",
      "\u001b[32m[2025-10-28 12:03:59 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 12:03:59 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 12:03:59 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 12:03:59 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 12:04:00 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: adaptive, Target Avg: 2.4 bits\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [5, 5, 4, 5, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.44 bits\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 6.56x vs FP16\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 12:04:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 5, Group: 128\n",
      "\u001b[32m[2025-10-28 12:04:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 12:04:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/2 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.0s\n",
      "\u001b[32m[2025-10-28 12:05:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 12:05:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 5, Group: 64\n",
      "\u001b[32m[2025-10-28 12:05:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/2 | Train Loss: 0.000014 | Val Loss: 0.000006 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 12:06:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000005 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.4s\n",
      "\u001b[32m[2025-10-28 12:06:28 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 12:06:28 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 12:06:59 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/2 | Train Loss: 0.000003 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:07:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/2 | Train Loss: 0.000003 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:07:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 12:07:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-28 12:08:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/2 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:08:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/2 | Train Loss: 0.000004 | Val Loss: 0.000006 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:08:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 12:08:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 12:09:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/2 | Train Loss: 0.000011 | Val Loss: 0.000013 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:09:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/2 | Train Loss: 0.000010 | Val Loss: 0.000012 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:10:13 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 12:10:13 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 12:10:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/2 | Train Loss: 0.000023 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:11:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/2 | Train Loss: 0.000020 | Val Loss: 0.000023 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:11:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 12:11:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 12:12:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000039 | Val Loss: 0.000039 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:12:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000035 | Val Loss: 0.000038 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:12:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 12:12:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:13:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000179 | Val Loss: 0.000153 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:13:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000142 | Val Loss: 0.000143 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:14:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 12:14:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:14:30 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000302 | Val Loss: 0.000270 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:14:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000256 | Val Loss: 0.000258 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 12:15:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:15:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000458 | Val Loss: 0.000420 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:16:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000400 | Val Loss: 0.000405 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:16:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 12:16:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:17:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000637 | Val Loss: 0.000591 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:17:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000566 | Val Loss: 0.000572 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:17:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 12:17:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:18:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000865 | Val Loss: 0.000814 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:18:43 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000780 | Val Loss: 0.000791 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:19:01 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 12:19:01 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:19:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.001152 | Val Loss: 0.001090 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:19:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.001045 | Val Loss: 0.001062 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:20:16 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 12:20:16 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.001499 | Val Loss: 0.001427 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:21:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.001375 | Val Loss: 0.001394 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 12:21:32 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:22:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.001884 | Val Loss: 0.001798 | Grad Norm: 0.00 | LR: 1.54e-05 | Time: 26.1s\n",
      "\u001b[32m[2025-10-28 12:22:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.001727 | Val Loss: 0.001757 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:22:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 12:22:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.002509 | Val Loss: 0.002408 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:23:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.002310 | Val Loss: 0.002357 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:24:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 12:24:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:24:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.003278 | Val Loss: 0.003156 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:25:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.003024 | Val Loss: 0.003090 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:25:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 12:25:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:25:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.004251 | Val Loss: 0.004095 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:26:16 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.003924 | Val Loss: 0.004006 | Grad Norm: 0.00 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:26:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 12:26:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:27:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.005633 | Val Loss: 0.005457 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:27:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.005204 | Val Loss: 0.005345 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:27:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 12:27:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:28:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.007805 | Val Loss: 0.007673 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:28:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.007250 | Val Loss: 0.007530 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:29:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 12:29:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:29:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.009941 | Val Loss: 0.009924 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:30:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.009346 | Val Loss: 0.009774 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:30:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 12:30:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:30:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.012314 | Val Loss: 0.012394 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:31:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.011653 | Val Loss: 0.012238 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:31:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 12:31:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:32:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.014980 | Val Loss: 0.015266 | Grad Norm: 0.01 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:32:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.014275 | Val Loss: 0.015099 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:32:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 12:32:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:33:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.018341 | Val Loss: 0.018732 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:33:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.017511 | Val Loss: 0.018539 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:34:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 12:34:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:34:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.022168 | Val Loss: 0.022829 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:35:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.021255 | Val Loss: 0.022610 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:35:23 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 12:35:23 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:35:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.026679 | Val Loss: 0.027526 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:36:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.025626 | Val Loss: 0.027279 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 12:36:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:37:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.032134 | Val Loss: 0.033246 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:37:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.030910 | Val Loss: 0.032953 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 12:37:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:38:26 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.038774 | Val Loss: 0.040276 | Grad Norm: 0.03 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:38:53 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.037382 | Val Loss: 0.039937 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:39:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 12:39:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:39:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.048818 | Val Loss: 0.050570 | Grad Norm: 0.04 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:40:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.046894 | Val Loss: 0.050113 | Grad Norm: 0.02 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 12:40:27 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:40:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.064227 | Val Loss: 0.066474 | Grad Norm: 0.06 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:41:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.061513 | Val Loss: 0.065865 | Grad Norm: 0.04 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:41:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 12:41:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:42:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.083095 | Val Loss: 0.085664 | Grad Norm: 0.09 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:42:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.079114 | Val Loss: 0.084708 | Grad Norm: 0.06 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:42:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 12:42:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 12:43:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.117782 | Val Loss: 0.120028 | Grad Norm: 0.20 | LR: 1.54e-05 | Time: 26.2s\n",
      "\u001b[32m[2025-10-28 12:43:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.110138 | Val Loss: 0.118457 | Grad Norm: 0.12 | LR: 1.50e-06 | Time: 26.3s\n",
      "\u001b[32m[2025-10-28 12:44:14 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_combined1/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 12:44:14 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_combined/model1/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 12:44:14 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2415.16s (40.25min)\n",
      "\u001b[32m[2025-10-28 12:44:14 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 12:44:21 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_combined/model1\n",
      "\u001b[32m[2025-10-28 12:44:21 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:44<00:00,  1.01s/it]\n",
      "wikitext2:12.515852928161621\n",
      "\u001b[32m[2025-10-28 12:47:19 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 12.52\n",
      "2025-10-28 12:47:19.552647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761655639.573726   42410 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761655639.580144   42410 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761655639.601513   42410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761655639.601541   42410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761655639.601544   42410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761655639.601547   42410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 12:47:24 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 12:47:25 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 12:47:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/PR/main_research.py\", line 344, in <module>\n",
      "    main()\n",
      "  File \"/content/PR/main_research.py\", line 318, in main\n",
      "    results = evaluate(model, tokenizer, args, logger)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/PR/main_research.py\", line 81, in evaluate\n",
      "    eval_results = lm_eval.simple_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 288, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/evaluator.py\", line 192, in simple_evaluate\n",
      "    task_dict = get_task_dict(tasks, task_manager)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 420, in get_task_dict\n",
      "    task_name_from_string_dict = task_manager.load_task_or_group(\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 270, in load_task_or_group\n",
      "    collections.ChainMap(*map(self._load_individual_task_or_group, task_list))\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 161, in _load_individual_task_or_group\n",
      "    return load_task(task_config, task=name_or_config, group=parent_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 150, in load_task\n",
      "    task_object = ConfigurableTask(config=config)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 782, in __init__\n",
      "    self.download(self.config.dataset_kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 871, in download\n",
      "    self.dataset = datasets.load_dataset(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 2556, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 2263, in load_dataset_builder\n",
      "    builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 247, in get_dataset_builder_class\n",
      "    builder_cls = import_main_class(dataset_module.module_path)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 165, in import_main_class\n",
      "    module = importlib.import_module(module_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1532, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1506, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1609, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1660, in _fill_cache\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "  --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "  --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "  --use_mixed_precision \\\n",
    "  --mpq_strategy adaptive \\\n",
    "  --target_avg_bits 2.4\\\n",
    "  --train_size 128 \\\n",
    "  --val_size 16 \\\n",
    "  --quant_lr 3e-5 \\\n",
    "  --weight_lr 2e-6 \\\n",
    "  --real_quant \\\n",
    "  --output_dir ./output/mistral_combined1 \\\n",
    "  --save_quant_dir ./output/mistral_combined/model1 \\\n",
    "  --eval_ppl \\\n",
    "  --eval_tasks piqa,arc_easy,hellaswag,winogrande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a125dac",
   "metadata": {},
   "source": [
    "# combined Methord 2.5 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5T594RbhkpU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5180870,
     "status": "ok",
     "timestamp": 1761661339006,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "h5T594RbhkpU",
    "outputId": "4c9e163d-c76c-4995-c5a4-72061ae48d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_mixed_precision', '--mpq_strategy', 'aggressive', '--target_avg_bits', '2.5', '--real_quant', '--use_adaptive_training', '--train_size', '128', '--val_size', '16', '--quant_lr', '1e-4', '--weight_lr', '2e-5', '--output_dir', './output/mistral_mpq_2.5bit', '--save_quant_dir', './output/mistral_mpq_2.5bit/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: True\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 213)\u001b[0m: INFO     Strategy: aggressive\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 214)\u001b[0m: INFO     Target avg bits: 2.5\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_mpq_2.5bit', save_quant_dir='./output/mistral_mpq_2.5bit/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=2e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=True, mpq_strategy='aggressive', target_avg_bits=2.5, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 12:56:02 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.28s/it]\n",
      "\u001b[32m[2025-10-28 12:56:09 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 12:56:09 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 12:56:09 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 12:56:09 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 12:56:10 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 537)\u001b[0m: INFO [MPQ] Mixed-Precision Configuration:\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 538)\u001b[0m: INFO   Strategy: aggressive, Target Avg: 2.5 bits\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 539)\u001b[0m: INFO   Bit-widths: [6, 6, 4, 5, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 540)\u001b[0m: INFO   Actual Avg: 2.47 bits\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 541)\u001b[0m: INFO   Compression: 6.48x vs FP16\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 6.67e-05-1.00e-04\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 6, Group: 128\n",
      "\u001b[32m[2025-10-28 12:56:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 7.77e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 12:56:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 5.87e-05 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 12:57:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 2.18e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 12:57:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000000 | Val Loss: 0.000000 | Grad Norm: 0.00 | LR: 3.88e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 12:57:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 12:57:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 6, Group: 64\n",
      "\u001b[32m[2025-10-28 12:57:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 6.67e-05, Patience: 5\n",
      "\u001b[32m[2025-10-28 12:58:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000097 | Val Loss: 0.000356 | Grad Norm: 0.09 | LR: 5.71e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 12:58:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000009 | Val Loss: 0.000344 | Grad Norm: 0.02 | LR: 3.46e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 12:59:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000006 | Val Loss: 0.000339 | Grad Norm: 0.02 | LR: 1.23e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 12:59:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000004 | Val Loss: 0.000336 | Grad Norm: 0.01 | LR: 3.33e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 12:59:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 12:59:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 4, Group: 256\n",
      "\u001b[32m[2025-10-28 12:59:58 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.51e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 13:00:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000006 | Val Loss: 0.000339 | Grad Norm: 0.00 | LR: 6.43e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:00:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000006 | Val Loss: 0.000339 | Grad Norm: 0.00 | LR: 2.39e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:01:22 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000005 | Val Loss: 0.000339 | Grad Norm: 0.00 | LR: 4.25e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:01:38 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 13:01:38 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 5, Group: 256\n",
      "\u001b[32m[2025-10-28 13:01:38 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.14e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 13:02:10 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000007 | Val Loss: 0.000340 | Grad Norm: 0.00 | LR: 6.15e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:02:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000006 | Val Loss: 0.000340 | Grad Norm: 0.00 | LR: 2.29e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:03:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000006 | Val Loss: 0.000339 | Grad Norm: 0.00 | LR: 4.07e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:03:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 13:03:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 13:03:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:03:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000013 | Val Loss: 0.000346 | Grad Norm: 0.00 | LR: 6.60e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:04:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000011 | Val Loss: 0.000345 | Grad Norm: 0.00 | LR: 2.45e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:04:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000011 | Val Loss: 0.000345 | Grad Norm: 0.00 | LR: 4.37e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:05:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 13:05:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 3, Group: 256\n",
      "\u001b[32m[2025-10-28 13:05:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 8.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:05:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000023 | Val Loss: 0.000356 | Grad Norm: 0.00 | LR: 6.70e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:06:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000021 | Val Loss: 0.000355 | Grad Norm: 0.00 | LR: 2.49e-05 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:06:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000020 | Val Loss: 0.000354 | Grad Norm: 0.00 | LR: 4.43e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:06:46 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 13:06:46 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:06:46 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.01e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:07:18 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000089 | Val Loss: 0.000405 | Grad Norm: 0.00 | LR: 4.63e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:07:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000068 | Val Loss: 0.000402 | Grad Norm: 0.00 | LR: 4.51e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:08:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 13:08:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:08:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.13e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:08:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000173 | Val Loss: 0.000479 | Grad Norm: 0.00 | LR: 4.69e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:09:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000136 | Val Loss: 0.000472 | Grad Norm: 0.00 | LR: 4.57e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 13:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:09:19 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.24e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:09:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000252 | Val Loss: 0.000554 | Grad Norm: 0.00 | LR: 4.75e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:10:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000206 | Val Loss: 0.000546 | Grad Norm: 0.00 | LR: 4.62e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:10:35 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 13:10:35 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:10:35 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.20e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:11:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000355 | Val Loss: 0.000654 | Grad Norm: 0.00 | LR: 4.72e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:11:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000297 | Val Loss: 0.000644 | Grad Norm: 0.00 | LR: 4.60e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 13:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:11:51 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.15e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:12:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.000474 | Val Loss: 0.000771 | Grad Norm: 0.00 | LR: 4.70e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:12:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000404 | Val Loss: 0.000758 | Grad Norm: 0.00 | LR: 4.58e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:13:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 13:13:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:13:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.23e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:13:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.000627 | Val Loss: 0.000923 | Grad Norm: 0.00 | LR: 4.74e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:14:06 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.000541 | Val Loss: 0.000906 | Grad Norm: 0.00 | LR: 4.62e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 13:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:14:24 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:14:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.000813 | Val Loss: 0.001110 | Grad Norm: 0.00 | LR: 4.95e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:15:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.000707 | Val Loss: 0.001089 | Grad Norm: 0.00 | LR: 4.82e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 13:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:15:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.32e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:16:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.001038 | Val Loss: 0.001338 | Grad Norm: 0.00 | LR: 4.79e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:16:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.000915 | Val Loss: 0.001316 | Grad Norm: 0.00 | LR: 4.66e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:16:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 13:16:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:16:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:17:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.001294 | Val Loss: 0.001579 | Grad Norm: 0.00 | LR: 4.77e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:17:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.001132 | Val Loss: 0.001550 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:18:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 13:18:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:18:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.51e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:18:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.001706 | Val Loss: 0.001992 | Grad Norm: 0.00 | LR: 4.88e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:19:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.001495 | Val Loss: 0.001952 | Grad Norm: 0.00 | LR: 4.75e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:19:30 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 13:19:30 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:19:30 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.69e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:20:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.002201 | Val Loss: 0.002489 | Grad Norm: 0.00 | LR: 4.98e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:20:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.001929 | Val Loss: 0.002438 | Grad Norm: 0.00 | LR: 4.85e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 13:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:20:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.51e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:21:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.002799 | Val Loss: 0.003102 | Grad Norm: 0.00 | LR: 4.89e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 13:21:45 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.002470 | Val Loss: 0.003040 | Grad Norm: 0.00 | LR: 4.76e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:22:03 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 13:22:03 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:22:03 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.28e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:22:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.003691 | Val Loss: 0.004018 | Grad Norm: 0.01 | LR: 4.77e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:23:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.003249 | Val Loss: 0.003931 | Grad Norm: 0.00 | LR: 4.64e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:23:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 13:23:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:23:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.39e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:23:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.005096 | Val Loss: 0.005528 | Grad Norm: 0.01 | LR: 4.82e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:24:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.004494 | Val Loss: 0.005408 | Grad Norm: 0.01 | LR: 4.70e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:24:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 13:24:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:24:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.73e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:25:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.006352 | Val Loss: 0.006986 | Grad Norm: 0.01 | LR: 5.00e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:25:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.005703 | Val Loss: 0.006870 | Grad Norm: 0.01 | LR: 4.86e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:25:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 13:25:53 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:25:53 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.52e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:26:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.007755 | Val Loss: 0.008609 | Grad Norm: 0.01 | LR: 4.89e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:26:51 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.007035 | Val Loss: 0.008482 | Grad Norm: 0.01 | LR: 4.76e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:27:09 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 13:27:09 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:27:09 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:27:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.009289 | Val Loss: 0.010481 | Grad Norm: 0.01 | LR: 4.95e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:28:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.008519 | Val Loss: 0.010354 | Grad Norm: 0.01 | LR: 4.82e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:28:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 13:28:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:28:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.73e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:28:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.011245 | Val Loss: 0.012725 | Grad Norm: 0.01 | LR: 5.00e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:29:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.010352 | Val Loss: 0.012585 | Grad Norm: 0.01 | LR: 4.87e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:29:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 13:29:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:29:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.60e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:30:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.013494 | Val Loss: 0.015410 | Grad Norm: 0.01 | LR: 4.93e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:30:39 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.012477 | Val Loss: 0.015246 | Grad Norm: 0.01 | LR: 4.80e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:30:57 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 13:30:57 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:30:57 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 1.00e-04, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:31:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.016079 | Val Loss: 0.018455 | Grad Norm: 0.01 | LR: 5.14e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:31:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.014938 | Val Loss: 0.018271 | Grad Norm: 0.01 | LR: 5.00e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:32:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 13:32:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:32:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:32:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.019236 | Val Loss: 0.022158 | Grad Norm: 0.01 | LR: 5.02e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:33:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.017916 | Val Loss: 0.021931 | Grad Norm: 0.01 | LR: 4.89e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:33:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 13:33:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:33:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.67e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:34:03 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.023084 | Val Loss: 0.026721 | Grad Norm: 0.02 | LR: 4.96e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:34:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.021590 | Val Loss: 0.026462 | Grad Norm: 0.01 | LR: 4.83e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:34:48 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 13:34:48 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:34:48 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.53e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:35:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.029176 | Val Loss: 0.033595 | Grad Norm: 0.02 | LR: 4.90e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:35:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.027138 | Val Loss: 0.033191 | Grad Norm: 0.01 | LR: 4.77e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:36:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 13:36:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:36:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.64e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:36:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.038577 | Val Loss: 0.044293 | Grad Norm: 0.04 | LR: 4.95e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:37:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.035715 | Val Loss: 0.043690 | Grad Norm: 0.02 | LR: 4.82e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:37:21 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 13:37:21 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:37:21 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.44e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:37:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.049931 | Val Loss: 0.056814 | Grad Norm: 0.05 | LR: 4.85e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:38:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.045990 | Val Loss: 0.055817 | Grad Norm: 0.03 | LR: 4.72e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:38:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 13:38:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 2, Group: 256\n",
      "\u001b[32m[2025-10-28 13:38:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 9.52e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 13:39:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.070995 | Val Loss: 0.080483 | Grad Norm: 0.18 | LR: 4.89e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 13:39:36 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.065223 | Val Loss: 0.079043 | Grad Norm: 0.12 | LR: 4.76e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 13:39:54 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_mpq_2.5bit/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 13:39:54 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_mpq_2.5bit/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 13:39:54 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2625.75s (43.76min)\n",
      "\u001b[32m[2025-10-28 13:39:54 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 13:40:04 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_mpq_2.5bit/model\n",
      "\u001b[32m[2025-10-28 13:40:04 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:46<00:00,  1.02s/it]\n",
      "wikitext2:9.497232437133789\n",
      "\u001b[32m[2025-10-28 13:43:04 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 9.50\n",
      "2025-10-28 13:43:05.005618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761658985.026527   55668 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761658985.032964   55668 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761658985.054241   55668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761658985.054267   55668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761658985.054270   55668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761658985.054278   55668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 13:43:09 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 13:43:10 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 13:43:14 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 126377.72it/s]\n",
      "\u001b[32m[2025-10-28 13:43:40 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2407.08it/s]\n",
      "\u001b[32m[2025-10-28 13:43:46 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1042.22it/s]\n",
      "\u001b[32m[2025-10-28 13:43:48 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1014.43it/s]\n",
      "\u001b[32m[2025-10-28 13:43:50 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [37:44<00:00, 24.68it/s]\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.6306|±  |0.0136|\n",
      "|hellaswag |      1|none  |     0|acc     |0.5394|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.7078|±  |0.0045|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.7306|±  |0.0091|\n",
      "|          |       |none  |     0|acc_norm|0.6780|±  |0.0096|\n",
      "|piqa      |      1|none  |     0|acc     |0.7699|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.7840|±  |0.0096|\n",
      "\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 66.76%\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_mpq_2.5bit/results.json\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 14:22:16 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python main_research.py \\\n",
    "    --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "    --sensitivity_file \"./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\" \\\n",
    "    --use_mixed_precision \\\n",
    "    --mpq_strategy aggressive \\\n",
    "    --target_avg_bits 2.5 \\\n",
    "    --real_quant \\\n",
    "    --use_adaptive_training \\\n",
    "    --train_size 128 \\\n",
    "    --val_size 16 \\\n",
    "    --quant_lr 1e-4 \\\n",
    "    --weight_lr 2e-5 \\\n",
    "    --output_dir \"./output/mistral_mpq_2.5bit\" \\\n",
    "    --save_quant_dir \"./output/mistral_mpq_2.5bit/model\" \\\n",
    "    --eval_ppl \\\n",
    "    --eval_tasks \"piqa,arc_easy,hellaswag,winogrande\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3bfd",
   "metadata": {},
   "source": [
    "# SRGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eF4S_vPHgmaz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5127297,
     "status": "ok",
     "timestamp": 1761678287607,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "eF4S_vPHgmaz",
    "outputId": "7e6d8e9e-a64f-4b2b-b8ed-73195a4fbfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_research.py', '--model', 'mistralai/Mistral-7B-Instruct-v0.2', '--sensitivity_file', './sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', '--use_adaptive_training', '--wbits', '2', '--train_size', '128', '--val_size', '16', '--quant_lr', '3e-5', '--weight_lr', '2e-6', '--real_quant', '--output_dir', './output/mistral_sgra', '--save_quant_dir', './output/mistral_sgra/model', '--eval_ppl', '--eval_tasks', 'piqa,arc_easy,hellaswag,winogrande']\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 205)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 206)\u001b[0m: INFO RESEARCH EXPERIMENT: Sensitivity-Guided Mixed-Precision Quantization\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 207)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 208)\u001b[0m: INFO Configuration:\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 209)\u001b[0m: INFO   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 210)\u001b[0m: INFO   Sensitivity file: ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 211)\u001b[0m: INFO   MPQ enabled: False\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 215)\u001b[0m: INFO   SGRA enabled: True\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 216)\u001b[0m: INFO   QBO enabled: False\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 219)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 220)\u001b[0m: INFO Namespace(model='mistralai/Mistral-7B-Instruct-v0.2', cache_dir='./cache', output_dir='./output/mistral_sgra', save_quant_dir='./output/mistral_sgra/model', real_quant=True, resume_quant=None, calib_dataset='wikitext2', train_size=128, val_size=16, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='piqa,arc_easy,hellaswag,winogrande', eval_batch_size=16, wbits=2, group_size=128, quant_lr=3e-05, weight_lr=2e-06, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False, sensitivity_file='./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json', use_mixed_precision=False, mpq_strategy='adaptive', target_avg_bits=4.0, use_adaptive_training=True, target_size_mb=None, ablation=None)\n",
      "\u001b[32m[2025-10-28 17:39:24 root]\u001b[0m\u001b[33m(main_research.py 224)\u001b[0m: INFO Setting net as Mistral-7B-Instruct-v0.2\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.16s/it]\n",
      "\u001b[32m[2025-10-28 17:39:29 root]\u001b[0m\u001b[33m(main_research.py 272)\u001b[0m: INFO === START RESEARCH QUANTIZATION ===\n",
      "\u001b[32m[2025-10-28 17:39:29 root]\u001b[0m\u001b[33m(main_research.py 280)\u001b[0m: INFO load trainloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_train.cache\n",
      "\u001b[32m[2025-10-28 17:39:29 root]\u001b[0m\u001b[33m(main_research.py 282)\u001b[0m: INFO load valloader from ./cache/dataloader_Mistral-7B-Instruct-v0.2_wikitext2_128_16_2048_val.cache\n",
      "\u001b[32m[2025-10-28 17:39:29 root]\u001b[0m\u001b[33m(block_ap_research.py 402)\u001b[0m: INFO Starting Research Implementation: Sensitivity-Guided Mixed-Precision QAT\n",
      "\u001b[32m[2025-10-28 17:39:30 root]\u001b[0m\u001b[33m(block_ap_research.py 487)\u001b[0m: INFO No attention mask caught from the first layer.\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 523)\u001b[0m: INFO [RESEARCH] Loading sensitivity scores from ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 527)\u001b[0m: INFO [RESEARCH] Loaded sensitivity scores for 32 layers\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 528)\u001b[0m: INFO [RESEARCH] Sensitivity range: 0.0000 to 1.0000\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 548)\u001b[0m: INFO [SGRA] Adaptive Training Configuration:\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 549)\u001b[0m: INFO   Epoch range: 2-4\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 550)\u001b[0m: INFO   LR range: 2.00e-05-3.00e-05\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 0 ===\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.5753, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:39:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.33e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 17:40:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 0/3 | Train Loss: 0.000003 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.76e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:40:28 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 1/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 6.55e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:40:55 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 0 Epoch 2/3 | Train Loss: 0.000002 | Val Loss: 0.000002 | Grad Norm: 0.00 | LR: 1.16e-06 | Time: 26.5s\n",
      "\u001b[32m[2025-10-28 17:41:11 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 1 ===\n",
      "\u001b[32m[2025-10-28 17:41:11 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 1.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:41:11 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 4, LR: 2.00e-05, Patience: 5\n",
      "\u001b[32m[2025-10-28 17:41:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 0/4 | Train Loss: 0.000598 | Val Loss: 0.000463 | Grad Norm: 0.04 | LR: 1.71e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:42:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 1/4 | Train Loss: 0.000369 | Val Loss: 0.000390 | Grad Norm: 0.02 | LR: 1.04e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:42:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 2/4 | Train Loss: 0.000294 | Val Loss: 0.000365 | Grad Norm: 0.02 | LR: 3.70e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:43:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 1 Epoch 3/4 | Train Loss: 0.000269 | Val Loss: 0.000359 | Grad Norm: 0.02 | LR: 1.00e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:43:18 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 2 ===\n",
      "\u001b[32m[2025-10-28 17:43:18 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.3507, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:43:18 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.55e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 17:43:49 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 0/3 | Train Loss: 0.000296 | Val Loss: 0.000387 | Grad Norm: 0.00 | LR: 1.93e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:44:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 1/3 | Train Loss: 0.000290 | Val Loss: 0.000385 | Grad Norm: 0.00 | LR: 7.17e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:44:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 2 Epoch 2/3 | Train Loss: 0.000289 | Val Loss: 0.000384 | Grad Norm: 0.00 | LR: 1.28e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:44:59 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 3 ===\n",
      "\u001b[32m[2025-10-28 17:44:59 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.4579, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:44:59 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.44e-05, Patience: 3\n",
      "\u001b[32m[2025-10-28 17:45:31 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 0/3 | Train Loss: 0.000319 | Val Loss: 0.000411 | Grad Norm: 0.00 | LR: 1.85e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:45:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 1/3 | Train Loss: 0.000313 | Val Loss: 0.000408 | Grad Norm: 0.00 | LR: 6.86e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:46:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 3 Epoch 2/3 | Train Loss: 0.000311 | Val Loss: 0.000407 | Grad Norm: 0.00 | LR: 1.22e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 4 ===\n",
      "\u001b[32m[2025-10-28 17:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2895, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:46:42 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.62e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:47:14 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 0/3 | Train Loss: 0.000356 | Val Loss: 0.000445 | Grad Norm: 0.00 | LR: 1.98e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:47:41 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 1/3 | Train Loss: 0.000345 | Val Loss: 0.000441 | Grad Norm: 0.00 | LR: 7.36e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 17:48:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 4 Epoch 2/3 | Train Loss: 0.000342 | Val Loss: 0.000440 | Grad Norm: 0.00 | LR: 1.31e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:48:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 5 ===\n",
      "\u001b[32m[2025-10-28 17:48:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2567, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:48:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 3, LR: 2.66e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:48:57 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 0/3 | Train Loss: 0.000413 | Val Loss: 0.000500 | Grad Norm: 0.00 | LR: 2.01e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:49:24 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 1/3 | Train Loss: 0.000396 | Val Loss: 0.000493 | Grad Norm: 0.00 | LR: 7.47e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 17:49:50 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 5 Epoch 2/3 | Train Loss: 0.000392 | Val Loss: 0.000491 | Grad Norm: 0.00 | LR: 1.33e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:50:08 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 6 ===\n",
      "\u001b[32m[2025-10-28 17:50:08 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.2186, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:50:08 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.70e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:50:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 0/2 | Train Loss: 0.000491 | Val Loss: 0.000577 | Grad Norm: 0.00 | LR: 1.39e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:51:07 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 6 Epoch 1/2 | Train Loss: 0.000469 | Val Loss: 0.000571 | Grad Norm: 0.00 | LR: 1.35e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 17:51:25 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 7 ===\n",
      "\u001b[32m[2025-10-28 17:51:25 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1903, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:51:25 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.74e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:51:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 0/2 | Train Loss: 0.000609 | Val Loss: 0.000690 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:52:23 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 7 Epoch 1/2 | Train Loss: 0.000575 | Val Loss: 0.000682 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:52:41 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 8 ===\n",
      "\u001b[32m[2025-10-28 17:52:41 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1642, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:52:41 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:53:13 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 0/2 | Train Loss: 0.000726 | Val Loss: 0.000808 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:53:40 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 8 Epoch 1/2 | Train Loss: 0.000683 | Val Loss: 0.000797 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:53:58 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 9 ===\n",
      "\u001b[32m[2025-10-28 17:53:58 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1750, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:53:58 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.76e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:54:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 0/2 | Train Loss: 0.000879 | Val Loss: 0.000964 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:54:56 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 9 Epoch 1/2 | Train Loss: 0.000827 | Val Loss: 0.000951 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:55:14 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 10 ===\n",
      "\u001b[32m[2025-10-28 17:55:14 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1846, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:55:14 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.75e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:55:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 0/2 | Train Loss: 0.001062 | Val Loss: 0.001143 | Grad Norm: 0.00 | LR: 1.41e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:56:12 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 10 Epoch 1/2 | Train Loss: 0.000995 | Val Loss: 0.001126 | Grad Norm: 0.00 | LR: 1.37e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 17:56:31 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 11 ===\n",
      "\u001b[32m[2025-10-28 17:56:31 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1667, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:56:31 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.77e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:57:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 0/2 | Train Loss: 0.001279 | Val Loss: 0.001366 | Grad Norm: 0.00 | LR: 1.42e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:57:29 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 11 Epoch 1/2 | Train Loss: 0.001204 | Val Loss: 0.001348 | Grad Norm: 0.00 | LR: 1.38e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 17:57:47 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 12 ===\n",
      "\u001b[32m[2025-10-28 17:57:47 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0745, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:57:47 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:58:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 0/2 | Train Loss: 0.001554 | Val Loss: 0.001643 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:58:46 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 12 Epoch 1/2 | Train Loss: 0.001460 | Val Loss: 0.001619 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 17:59:04 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 13 ===\n",
      "\u001b[32m[2025-10-28 17:59:04 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1451, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 17:59:04 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.80e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 17:59:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 0/2 | Train Loss: 0.001893 | Val Loss: 0.001983 | Grad Norm: 0.00 | LR: 1.44e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:00:02 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 13 Epoch 1/2 | Train Loss: 0.001783 | Val Loss: 0.001955 | Grad Norm: 0.00 | LR: 1.40e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 14 ===\n",
      "\u001b[32m[2025-10-28 18:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1541, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:00:20 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.79e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:00:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 0/2 | Train Loss: 0.002243 | Val Loss: 0.002334 | Grad Norm: 0.00 | LR: 1.43e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:01:19 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 14 Epoch 1/2 | Train Loss: 0.002101 | Val Loss: 0.002301 | Grad Norm: 0.00 | LR: 1.39e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:01:37 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 15 ===\n",
      "\u001b[32m[2025-10-28 18:01:37 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1038, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:01:37 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:02:09 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 0/2 | Train Loss: 0.002837 | Val Loss: 0.002944 | Grad Norm: 0.00 | LR: 1.46e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:02:35 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 15 Epoch 1/2 | Train Loss: 0.002661 | Val Loss: 0.002901 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:02:53 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 16 ===\n",
      "\u001b[32m[2025-10-28 18:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0635, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:02:54 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.91e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:03:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 0/2 | Train Loss: 0.003547 | Val Loss: 0.003662 | Grad Norm: 0.00 | LR: 1.49e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:03:52 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 16 Epoch 1/2 | Train Loss: 0.003320 | Val Loss: 0.003608 | Grad Norm: 0.00 | LR: 1.45e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 17 ===\n",
      "\u001b[32m[2025-10-28 18:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1020, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:04:10 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:04:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 0/2 | Train Loss: 0.004450 | Val Loss: 0.004570 | Grad Norm: 0.01 | LR: 1.47e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:05:08 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 17 Epoch 1/2 | Train Loss: 0.004161 | Val Loss: 0.004500 | Grad Norm: 0.00 | LR: 1.43e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:05:26 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 18 ===\n",
      "\u001b[32m[2025-10-28 18:05:26 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1549, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:05:26 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.78e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:05:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 0/2 | Train Loss: 0.005743 | Val Loss: 0.005911 | Grad Norm: 0.01 | LR: 1.43e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:06:25 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 18 Epoch 1/2 | Train Loss: 0.005367 | Val Loss: 0.005821 | Grad Norm: 0.01 | LR: 1.39e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 19 ===\n",
      "\u001b[32m[2025-10-28 18:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1295, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:06:43 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.82e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:07:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 0/2 | Train Loss: 0.007814 | Val Loss: 0.008130 | Grad Norm: 0.01 | LR: 1.45e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:07:42 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 19 Epoch 1/2 | Train Loss: 0.007314 | Val Loss: 0.008015 | Grad Norm: 0.01 | LR: 1.41e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:08:00 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 20 ===\n",
      "\u001b[32m[2025-10-28 18:08:00 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0565, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:08:00 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:08:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 0/2 | Train Loss: 0.009798 | Val Loss: 0.010333 | Grad Norm: 0.01 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:08:58 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 20 Epoch 1/2 | Train Loss: 0.009254 | Val Loss: 0.010210 | Grad Norm: 0.01 | LR: 1.46e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:09:17 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 21 ===\n",
      "\u001b[32m[2025-10-28 18:09:17 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1004, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:09:17 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:09:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 0/2 | Train Loss: 0.011980 | Val Loss: 0.012766 | Grad Norm: 0.01 | LR: 1.47e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:10:15 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 21 Epoch 1/2 | Train Loss: 0.011398 | Val Loss: 0.012638 | Grad Norm: 0.01 | LR: 1.43e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:10:33 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 22 ===\n",
      "\u001b[32m[2025-10-28 18:10:33 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0751, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:10:33 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:11:05 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 0/2 | Train Loss: 0.014438 | Val Loss: 0.015564 | Grad Norm: 0.01 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:11:32 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 22 Epoch 1/2 | Train Loss: 0.013808 | Val Loss: 0.015429 | Grad Norm: 0.01 | LR: 1.45e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 18:11:50 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 23 ===\n",
      "\u001b[32m[2025-10-28 18:11:50 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0550, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:11:50 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.92e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:12:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 0/2 | Train Loss: 0.017506 | Val Loss: 0.018915 | Grad Norm: 0.01 | LR: 1.50e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:12:48 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 23 Epoch 1/2 | Train Loss: 0.016765 | Val Loss: 0.018758 | Grad Norm: 0.01 | LR: 1.46e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:13:06 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 24 ===\n",
      "\u001b[32m[2025-10-28 18:13:06 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0834, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:13:06 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.88e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:13:38 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 0/2 | Train Loss: 0.020989 | Val Loss: 0.022840 | Grad Norm: 0.01 | LR: 1.48e-05 | Time: 26.6s\n",
      "\u001b[32m[2025-10-28 18:14:04 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 24 Epoch 1/2 | Train Loss: 0.020171 | Val Loss: 0.022672 | Grad Norm: 0.01 | LR: 1.44e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 25 ===\n",
      "\u001b[32m[2025-10-28 18:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0000, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:14:22 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 3.00e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:14:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 0/2 | Train Loss: 0.025035 | Val Loss: 0.027281 | Grad Norm: 0.02 | LR: 1.54e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:15:21 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 25 Epoch 1/2 | Train Loss: 0.024091 | Val Loss: 0.027086 | Grad Norm: 0.01 | LR: 1.50e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 26 ===\n",
      "\u001b[32m[2025-10-28 18:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0462, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:15:39 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.93e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:16:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 0/2 | Train Loss: 0.029934 | Val Loss: 0.032679 | Grad Norm: 0.02 | LR: 1.51e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:16:37 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 26 Epoch 1/2 | Train Loss: 0.028843 | Val Loss: 0.032447 | Grad Norm: 0.01 | LR: 1.47e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:16:55 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 27 ===\n",
      "\u001b[32m[2025-10-28 18:16:55 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0692, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:16:55 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.90e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:17:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 0/2 | Train Loss: 0.035841 | Val Loss: 0.039271 | Grad Norm: 0.02 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:17:54 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 27 Epoch 1/2 | Train Loss: 0.034632 | Val Loss: 0.039015 | Grad Norm: 0.02 | LR: 1.45e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:18:12 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 28 ===\n",
      "\u001b[32m[2025-10-28 18:18:12 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0978, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:18:12 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.86e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:18:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 0/2 | Train Loss: 0.044868 | Val Loss: 0.048968 | Grad Norm: 0.03 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:19:11 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 28 Epoch 1/2 | Train Loss: 0.043199 | Val Loss: 0.048613 | Grad Norm: 0.02 | LR: 1.43e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:19:29 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 29 ===\n",
      "\u001b[32m[2025-10-28 18:19:29 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.0737, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:19:29 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.89e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:20:00 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 0/2 | Train Loss: 0.058596 | Val Loss: 0.063906 | Grad Norm: 0.05 | LR: 1.49e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:20:27 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 29 Epoch 1/2 | Train Loss: 0.056282 | Val Loss: 0.063410 | Grad Norm: 0.03 | LR: 1.45e-06 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:20:45 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 30 ===\n",
      "\u001b[32m[2025-10-28 18:20:45 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1186, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:20:45 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.83e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:21:17 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 0/2 | Train Loss: 0.075014 | Val Loss: 0.081333 | Grad Norm: 0.08 | LR: 1.45e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:21:44 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 30 Epoch 1/2 | Train Loss: 0.071688 | Val Loss: 0.080559 | Grad Norm: 0.05 | LR: 1.42e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 18:22:02 root]\u001b[0m\u001b[33m(block_ap_research.py 577)\u001b[0m: INFO === Start quantize block 31 ===\n",
      "\u001b[32m[2025-10-28 18:22:02 root]\u001b[0m\u001b[33m(block_ap_research.py 578)\u001b[0m: INFO [CONFIG] Sensitivity: 0.1016, Bits: 2, Group: 128\n",
      "\u001b[32m[2025-10-28 18:22:02 root]\u001b[0m\u001b[33m(block_ap_research.py 581)\u001b[0m: INFO [SGRA] Epochs: 2, LR: 2.85e-05, Patience: 2\n",
      "\u001b[32m[2025-10-28 18:22:34 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 0/2 | Train Loss: 0.105584 | Val Loss: 0.112995 | Grad Norm: 0.19 | LR: 1.47e-05 | Time: 26.7s\n",
      "\u001b[32m[2025-10-28 18:23:01 root]\u001b[0m\u001b[33m(block_ap_research.py 715)\u001b[0m: INFO [TRAINING] ✅ Block 31 Epoch 1/2 | Train Loss: 0.099648 | Val Loss: 0.111678 | Grad Norm: 0.14 | LR: 1.43e-06 | Time: 26.8s\n",
      "\u001b[32m[2025-10-28 18:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 786)\u001b[0m: INFO [RESEARCH] Statistics saved to ./output/mistral_sgra/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 18:23:19 root]\u001b[0m\u001b[33m(block_ap_research.py 794)\u001b[0m: INFO [RESEARCH] Statistics also saved to ./output/mistral_sgra/model/layer_statistics.json\n",
      "\u001b[32m[2025-10-28 18:23:19 root]\u001b[0m\u001b[33m(main_research.py 305)\u001b[0m: INFO [RESULTS] Total training time: 2630.09s (43.83min)\n",
      "\u001b[32m[2025-10-28 18:23:19 root]\u001b[0m\u001b[33m(main_research.py 311)\u001b[0m: INFO Saving quantized model...\n",
      "\u001b[32m[2025-10-28 18:23:25 root]\u001b[0m\u001b[33m(main_research.py 314)\u001b[0m: INFO Model saved to ./output/mistral_sgra/model\n",
      "\u001b[32m[2025-10-28 18:23:25 root]\u001b[0m\u001b[33m(main_research.py 317)\u001b[0m: INFO === EVALUATION ===\n",
      "get_wikitext2\n",
      "100% 163/163 [02:41<00:00,  1.01it/s]\n",
      "wikitext2:12.533713340759277\n",
      "\u001b[32m[2025-10-28 18:26:19 root]\u001b[0m\u001b[33m(main_research.py 71)\u001b[0m: INFO [RESULTS] wikitext2 perplexity: 12.53\n",
      "2025-10-28 18:26:20.052823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761675980.073814  127029 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761675980.080219  127029 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761675980.101993  127029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761675980.102019  127029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761675980.102022  127029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761675980.102025  127029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[32m[2025-10-28 18:26:24 lm-eval]\u001b[0m\u001b[33m(huggingface.py 118)\u001b[0m: WARNING `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 18:26:25 lm-eval]\u001b[0m\u001b[33m(huggingface.py 337)\u001b[0m: WARNING Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "\u001b[32m[2025-10-28 18:26:29 lm-eval]\u001b[0m\u001b[33m(evaluator.py 131)\u001b[0m: INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of winogrande from None to 0\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of hellaswag from None to 0\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of arc_easy from None to 0\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(evaluator.py 222)\u001b[0m: WARNING Overwriting default num_fewshot of piqa from None to 0\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 130204.91it/s]\n",
      "\u001b[32m[2025-10-28 18:26:56 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for hellaswag on rank 0...\n",
      "100% 10042/10042 [00:04<00:00, 2404.45it/s]\n",
      "\u001b[32m[2025-10-28 18:27:02 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for arc_easy on rank 0...\n",
      "100% 2376/2376 [00:02<00:00, 1018.17it/s]\n",
      "\u001b[32m[2025-10-28 18:27:04 lm-eval]\u001b[0m\u001b[33m(task.py 395)\u001b[0m: INFO Building contexts for piqa on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1008.21it/s]\n",
      "\u001b[32m[2025-10-28 18:27:06 lm-eval]\u001b[0m\u001b[33m(evaluator.py 362)\u001b[0m: INFO Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 55879/55879 [36:57<00:00, 25.20it/s]\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 87)\u001b[0m: INFO |  Tasks   |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|----------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc     |0.5706|±  |0.0139|\n",
      "|hellaswag |      1|none  |     0|acc     |0.4815|±  |0.0050|\n",
      "|          |       |none  |     0|acc_norm|0.6231|±  |0.0048|\n",
      "|arc_easy  |      1|none  |     0|acc     |0.6494|±  |0.0098|\n",
      "|          |       |none  |     0|acc_norm|0.5842|±  |0.0101|\n",
      "|piqa      |      1|none  |     0|acc     |0.6964|±  |0.0107|\n",
      "|          |       |none  |     0|acc_norm|0.7122|±  |0.0106|\n",
      "\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 93)\u001b[0m: INFO [RESULTS] Average Acc: 59.95%\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 335)\u001b[0m: INFO Results saved to ./output/mistral_sgra/results.json\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 337)\u001b[0m: INFO ======================================================================\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 338)\u001b[0m: INFO EXPERIMENT COMPLETED\n",
      "\u001b[32m[2025-10-28 19:04:44 root]\u001b[0m\u001b[33m(main_research.py 339)\u001b[0m: INFO ======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python main_research.py \\\n",
    "       --model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
    "       --sensitivity_file ./sensitivity_results_mistral_7b_instruct_v0.2_corrected.json \\\n",
    "       --use_adaptive_training \\\n",
    "       --wbits 2 \\\n",
    "       --train_size 128 \\\n",
    "       --val_size 16 \\\n",
    "       --quant_lr 3e-5 \\\n",
    "       --weight_lr 2e-6 \\\n",
    "       --real_quant \\\n",
    "       --output_dir ./output/mistral_sgra \\\n",
    "       --save_quant_dir ./output/mistral_sgra/model \\\n",
    "       --eval_ppl \\\n",
    "       --eval_tasks piqa,arc_easy,hellaswag,winogrande\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhZllVJVHjaS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761643356515,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "yhZllVJVHjaS",
    "outputId": "758aaf5b-fe7e-47c0-c648-c037d5bbcd6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.88 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_combined/model\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-0BfPtVGIGCN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761643405181,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "-0BfPtVGIGCN",
    "outputId": "3af8cc8b-85c4-4348-d285-4cd576daede6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.88 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_combined/model\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sjfOGiwsINdL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761643400995,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "sjfOGiwsINdL",
    "outputId": "493546d3-1b88-455b-bf08-c9551cf9d2a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 3.22 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_baseline/model\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7pLUzee_IQ4L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761650035498,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "7pLUzee_IQ4L",
    "outputId": "8befe6e0-7a92-4841-bc2d-7b3de2335a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.86 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2.5\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_combined/model\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mkBUJbj43H64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761655696343,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "mkBUJbj43H64",
    "outputId": "abf6768b-8468-4938-86df-abd12c9213a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.58 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2.5\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_combined/model1\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R9dSARg3gA0k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761666491923,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "R9dSARg3gA0k",
    "outputId": "e06eb00c-4335-48a3-9d52-a02b0bc86fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.58 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_combined/model1\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1CxB9EUafu2B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761666388661,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "1CxB9EUafu2B",
    "outputId": "0abdc7a2-9197-4180-ea6a-7586b2242785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.23 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_baseline\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SPNgL2XhNh5-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1761678420839,
     "user": {
      "displayName": "gautam krishna.m",
      "userId": "16129779080195890370"
     },
     "user_tz": -330
    },
    "id": "SPNgL2XhNh5-",
    "outputId": "66766a06-a451-4872-dac5-437767db2acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 2.23 GB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#2bit\n",
    "# Path to your quantized model output directory\n",
    "model_path = Path(\"./output/mistral_sgra\")\n",
    "\n",
    "# Sum all file sizes recursively\n",
    "total_bytes = sum(f.stat().st_size for f in model_path.glob(\"**/*\") if f.is_file())\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Quantized model size: {total_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utx0OxBPN9vw",
   "metadata": {
    "id": "utx0OxBPN9vw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
